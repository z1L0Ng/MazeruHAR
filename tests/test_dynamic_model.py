#!/usr/bin/env python3
"""
ä¿®å¤å¯¼å…¥è·¯å¾„çš„DynamicHarModelæµ‹è¯•æ–‡ä»¶
ç›´æ¥åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼špython test_dynamic_model.py
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Any, Optional, Union
from abc import ABC, abstractmethod
import importlib
import math


# å†…è”å®ç°æ‰€æœ‰å¿…è¦çš„ç±»ï¼Œé¿å…å¯¼å…¥é—®é¢˜
class ExpertModel(nn.Module, ABC):
    """ä¸“å®¶æ¨¡å‹çš„æŠ½è±¡åŸºç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        self.config = config
        self.input_dim = config.get('input_dim', 6)
        self.hidden_dim = config.get('hidden_dim', 128)
        self.output_dim = config.get('output_dim', 128)
        self.dropout = config.get('dropout', 0.1)
        
    @abstractmethod
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        pass
    
    def get_output_dim(self) -> int:
        return self.output_dim


class TransformerExpert(ExpertModel):
    """åŸºäºTransformerçš„ä¸“å®¶æ¨¡å‹"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        
        self.num_heads = config.get('num_heads', 8)
        self.num_layers = config.get('num_layers', 4)
        self.ff_dim = config.get('ff_dim', self.hidden_dim * 4)
        
        # è¾“å…¥æŠ•å½±
        self.input_projection = nn.Linear(self.input_dim, self.hidden_dim)
        
        # ä½ç½®ç¼–ç 
        self.pos_embedding = nn.Parameter(
            torch.randn(1, 1000, self.hidden_dim) * 0.02
        )
        
        # Transformerå±‚
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.hidden_dim,
            nhead=self.num_heads,
            dim_feedforward=self.ff_dim,
            dropout=self.dropout,
            activation='relu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer, 
            num_layers=self.num_layers
        )
        
        # è¾“å‡ºæŠ•å½±
        self.output_projection = nn.Linear(self.hidden_dim, self.output_dim)
        self.layer_norm = nn.LayerNorm(self.hidden_dim)
        self.dropout_layer = nn.Dropout(self.dropout)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size, seq_len, _ = x.shape
        
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)  # [batch, seq_len, hidden_dim]
        
        # ä½ç½®ç¼–ç 
        pos_embed = self.pos_embedding[:, :seq_len, :]
        x = x + pos_embed
        x = self.layer_norm(x)
        
        # Transformerç¼–ç 
        x = self.transformer(x)  # [batch, seq_len, hidden_dim]
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = torch.mean(x, dim=1)  # [batch, hidden_dim]
        
        # è¾“å‡ºæŠ•å½±
        x = self.dropout_layer(x)
        x = self.output_projection(x)  # [batch, output_dim]
        
        return x


class RNNExpert(ExpertModel):
    """åŸºäºRNNçš„ä¸“å®¶æ¨¡å‹"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        
        self.rnn_type = config.get('rnn_type', 'LSTM')
        self.num_layers = config.get('num_layers', 2)
        self.bidirectional = config.get('bidirectional', True)
        
        # RNNå±‚
        if self.rnn_type == 'LSTM':
            self.rnn = nn.LSTM(
                input_size=self.input_dim,
                hidden_size=self.hidden_dim,
                num_layers=self.num_layers,
                dropout=self.dropout if self.num_layers > 1 else 0,
                bidirectional=self.bidirectional,
                batch_first=True
            )
        elif self.rnn_type == 'GRU':
            self.rnn = nn.GRU(
                input_size=self.input_dim,
                hidden_size=self.hidden_dim,
                num_layers=self.num_layers,
                dropout=self.dropout if self.num_layers > 1 else 0,
                bidirectional=self.bidirectional,
                batch_first=True
            )
        else:
            raise ValueError(f"Unsupported RNN type: {self.rnn_type}")
        
        # è®¡ç®—RNNè¾“å‡ºç»´åº¦
        rnn_output_dim = self.hidden_dim * (2 if self.bidirectional else 1)
        
        # è¾“å‡ºæŠ•å½±
        self.output_projection = nn.Sequential(
            nn.Linear(rnn_output_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(self.dropout),
            nn.Linear(self.hidden_dim, self.output_dim)
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # RNNå¤„ç†
        rnn_output, _ = self.rnn(x)  # [batch, seq_len, hidden_dim * directions]
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = rnn_output[:, -1, :]  # [batch, hidden_dim * directions]
        
        # è¾“å‡ºæŠ•å½±
        output = self.output_projection(last_output)  # [batch, output_dim]
        
        return output


class CNNExpert(ExpertModel):
    """åŸºäºCNNçš„ä¸“å®¶æ¨¡å‹"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        
        self.num_filters = config.get('num_filters', [64, 128, 256])
        self.kernel_sizes = config.get('kernel_sizes', [3, 3, 3])
        self.pool_sizes = config.get('pool_sizes', [2, 2, 2])
        
        # æ„å»ºå·ç§¯å±‚
        layers = []
        in_channels = self.input_dim
        
        for i, (filters, kernel_size, pool_size) in enumerate(
            zip(self.num_filters, self.kernel_sizes, self.pool_sizes)
        ):
            layers.extend([
                nn.Conv1d(in_channels, filters, kernel_size, padding=kernel_size//2),
                nn.BatchNorm1d(filters),
                nn.ReLU(),
                nn.MaxPool1d(pool_size),
                nn.Dropout(self.dropout)
            ])
            in_channels = filters
        
        self.conv_layers = nn.Sequential(*layers)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        # è¾“å‡ºæŠ•å½±
        self.output_projection = nn.Sequential(
            nn.Linear(self.num_filters[-1], self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(self.dropout),
            nn.Linear(self.hidden_dim, self.output_dim)
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # è½¬æ¢ç»´åº¦ä¸ºCNNæ ¼å¼
        x = x.transpose(1, 2)  # [batch, input_dim, seq_len]
        
        # å·ç§¯å¤„ç†
        x = self.conv_layers(x)  # [batch, num_filters[-1], reduced_seq_len]
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = self.global_pool(x)  # [batch, num_filters[-1], 1]
        x = x.squeeze(-1)  # [batch, num_filters[-1]]
        
        # è¾“å‡ºæŠ•å½±
        output = self.output_projection(x)  # [batch, output_dim]
        
        return output


class FusionLayer(nn.Module):
    """èåˆå±‚"""
    
    def __init__(self, strategy: str, config: Dict[str, Any] = None):
        super().__init__()
        self.strategy = strategy
        self.config = config or {}
        
    def forward(self, expert_outputs: Dict[str, torch.Tensor]) -> torch.Tensor:
        if self.strategy == 'concatenate':
            return self._concatenate_fusion(expert_outputs)
        elif self.strategy == 'average':
            return self._average_fusion(expert_outputs)
        elif self.strategy == 'attention':
            return self._attention_fusion(expert_outputs)
        elif self.strategy == 'weighted_sum':
            return self._weighted_sum_fusion(expert_outputs)
        else:
            raise ValueError(f"Unsupported fusion strategy: {self.strategy}")
    
    def _concatenate_fusion(self, expert_outputs: Dict[str, torch.Tensor]) -> torch.Tensor:
        """æ‹¼æ¥èåˆ - ä»»åŠ¡2.2æ ¸å¿ƒå®ç°"""
        if not expert_outputs:
            raise ValueError("Empty expert outputs")
        
        # æŒ‰é”®åæ’åºç¡®ä¿ä¸€è‡´æ€§
        sorted_keys = sorted(expert_outputs.keys())
        sorted_outputs = [expert_outputs[key] for key in sorted_keys]
        
        # æ‹¼æ¥æ‰€æœ‰ä¸“å®¶è¾“å‡º
        fused_features = torch.cat(sorted_outputs, dim=-1)
        
        return fused_features
    
    def _average_fusion(self, expert_outputs: Dict[str, torch.Tensor]) -> torch.Tensor:
        """å¹³å‡èåˆ"""
        outputs = list(expert_outputs.values())
        stacked = torch.stack(outputs, dim=0)
        return torch.mean(stacked, dim=0)
    
    def _weighted_sum_fusion(self, expert_outputs: Dict[str, torch.Tensor]) -> torch.Tensor:
        """åŠ æƒæ±‚å’Œèåˆ"""
        if not hasattr(self, 'fusion_weights'):
            num_experts = len(expert_outputs)
            self.fusion_weights = nn.Parameter(torch.ones(num_experts) / num_experts)
        
        outputs = list(expert_outputs.values())
        stacked = torch.stack(outputs, dim=0)  # [num_experts, batch_size, feature_dim]
        
        # åº”ç”¨softmaxç¡®ä¿æƒé‡å’Œä¸º1
        weights = F.softmax(self.fusion_weights, dim=0)
        weights = weights.view(-1, 1, 1)  # [num_experts, 1, 1]
        
        # åŠ æƒæ±‚å’Œ
        fused = torch.sum(stacked * weights, dim=0)
        return fused
    
    def _attention_fusion(self, expert_outputs: Dict[str, torch.Tensor]) -> torch.Tensor:
        """æ³¨æ„åŠ›èåˆ"""
        outputs = list(expert_outputs.values())
        stacked = torch.stack(outputs, dim=1)  # [batch_size, num_experts, feature_dim]
        
        # ç®€å•çš„æ³¨æ„åŠ›æœºåˆ¶
        if not hasattr(self, 'attention_layer'):
            feature_dim = stacked.shape[-1]
            self.attention_layer = nn.Linear(feature_dim, 1)
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_scores = self.attention_layer(stacked)  # [batch_size, num_experts, 1]
        attention_weights = F.softmax(attention_scores, dim=1)
        
        # åŠ æƒæ±‚å’Œ
        fused = torch.sum(stacked * attention_weights, dim=1)
        return fused


class DynamicHarModel(nn.Module):
    """åŠ¨æ€HARæ¨¡å‹å®¹å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__()
        self.config = config
        
        # è·å–æ¶æ„é…ç½®
        self.architecture_config = config.get('architecture', {})
        self.experts_config = self.architecture_config.get('experts', {})
        self.fusion_config = self.architecture_config.get('fusion', {})
        self.classifier_config = self.architecture_config.get('classifier', {})
        
        # è·å–æ•°æ®é›†ä¿¡æ¯
        self.num_classes = config.get('labels', {}).get('num_classes', 8)
        
        # ä¸“å®¶æ¨¡å‹æ³¨å†Œè¡¨
        self.expert_registry = {
            'TransformerExpert': TransformerExpert,
            'RNNExpert': RNNExpert,
            'CNNExpert': CNNExpert,
        }
        
        # åŠ¨æ€åˆ›å»ºä¸“å®¶æ¨¡å‹
        self.experts = nn.ModuleDict()
        self._create_experts()
        
        # åˆ›å»ºèåˆå±‚
        self.fusion_layer = self._create_fusion_layer()
        
        # åˆ›å»ºåˆ†ç±»å™¨
        self.classifier = self._create_classifier()
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
        
    def _create_experts(self):
        """æ ¹æ®é…ç½®åŠ¨æ€åˆ›å»ºä¸“å®¶æ¨¡å‹"""
        for expert_name, expert_config in self.experts_config.items():
            expert_type = expert_config.get('type', 'TransformerExpert')
            expert_params = expert_config.get('params', {})
            
            # è·å–ä¸“å®¶ç±»
            if expert_type in self.expert_registry:
                expert_class = self.expert_registry[expert_type]
            else:
                raise ValueError(f"Unsupported expert type: {expert_type}")
            
            # åˆ›å»ºä¸“å®¶å®ä¾‹
            expert_instance = expert_class(expert_params)
            self.experts[expert_name] = expert_instance
            
            print(f"âœ… åˆ›å»ºä¸“å®¶: {expert_name} ({expert_type}) è¾“å‡ºç»´åº¦={expert_instance.get_output_dim()}")
    
    def _create_fusion_layer(self):
        """åˆ›å»ºèåˆå±‚"""
        fusion_strategy = self.fusion_config.get('strategy', 'concatenate')
        fusion_params = self.fusion_config.get('params', {})
        
        fusion_layer = FusionLayer(fusion_strategy, fusion_params)
        
        # å¦‚æœæ˜¯åŠ æƒæ±‚å’Œæˆ–æ³¨æ„åŠ›èåˆï¼Œéœ€è¦åˆå§‹åŒ–ç›¸å…³å‚æ•°
        if fusion_strategy == 'weighted_sum':
            num_experts = len(self.experts)
            fusion_layer.fusion_weights = nn.Parameter(torch.ones(num_experts) / num_experts)
            
        print(f"âœ… åˆ›å»ºèåˆå±‚: {fusion_strategy}")
        return fusion_layer
    
    def _create_classifier(self):
        """åˆ›å»ºåˆ†ç±»å™¨"""
        # è®¡ç®—èåˆåçš„ç‰¹å¾ç»´åº¦
        fusion_output_dim = self._calculate_fusion_output_dim()
        
        # è·å–åˆ†ç±»å™¨é…ç½®
        classifier_type = self.classifier_config.get('type', 'MLP')
        
        if classifier_type == 'MLP':
            layers = self.classifier_config.get('layers', [fusion_output_dim, self.num_classes])
            activation = self.classifier_config.get('activation', 'relu')
            dropout = self.classifier_config.get('dropout', 0.2)
            
            # ç¡®ä¿ç¬¬ä¸€å±‚è¾“å…¥ç»´åº¦æ­£ç¡®
            if layers[0] != fusion_output_dim:
                layers[0] = fusion_output_dim
            
            # ç¡®ä¿æœ€åä¸€å±‚è¾“å‡ºç»´åº¦æ­£ç¡®
            if layers[-1] != self.num_classes:
                layers[-1] = self.num_classes
            
            classifier_layers = []
            for i in range(len(layers) - 1):
                classifier_layers.append(nn.Linear(layers[i], layers[i + 1]))
                
                # é™¤äº†æœ€åä¸€å±‚ï¼Œéƒ½æ·»åŠ æ¿€æ´»å‡½æ•°å’Œdropout
                if i < len(layers) - 2:
                    if activation.lower() == 'relu':
                        classifier_layers.append(nn.ReLU())
                    elif activation.lower() == 'gelu':
                        classifier_layers.append(nn.GELU())
                    elif activation.lower() == 'tanh':
                        classifier_layers.append(nn.Tanh())
                    
                    classifier_layers.append(nn.Dropout(dropout))
            
            classifier = nn.Sequential(*classifier_layers)
        else:
            raise ValueError(f"Unsupported classifier type: {classifier_type}")
        
        print(f"âœ… åˆ›å»ºåˆ†ç±»å™¨: è¾“å…¥ç»´åº¦={fusion_output_dim}, è¾“å‡ºç»´åº¦={self.num_classes}")
        return classifier
    
    def _calculate_fusion_output_dim(self):
        """è®¡ç®—èåˆåçš„è¾“å‡ºç»´åº¦"""
        strategy = self.fusion_config.get('strategy', 'concatenate')
        
        if strategy == 'concatenate':
            # æ‹¼æ¥ï¼šæ‰€æœ‰ä¸“å®¶è¾“å‡ºç»´åº¦ä¹‹å’Œ
            total_dim = 0
            for expert in self.experts.values():
                total_dim += expert.get_output_dim()
            return total_dim
        elif strategy in ['average', 'attention', 'weighted_sum']:
            # å…¶ä»–ç­–ç•¥ï¼šå‡è®¾æ‰€æœ‰ä¸“å®¶è¾“å‡ºç»´åº¦ç›¸åŒï¼Œå–ç¬¬ä¸€ä¸ªä¸“å®¶çš„è¾“å‡ºç»´åº¦
            first_expert = next(iter(self.experts.values()))
            return first_expert.get_output_dim()
        else:
            raise ValueError(f"Unsupported fusion strategy: {strategy}")
    
    def _init_weights(self, module):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Conv1d):
            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, (nn.BatchNorm1d, nn.LayerNorm)):
            nn.init.ones_(module.weight)
            nn.init.zeros_(module.bias)
    
    def forward(self, data_dict: Dict[str, torch.Tensor]) -> torch.Tensor:
        """åŠ¨æ€å‰å‘ä¼ æ’­"""
        # 1. ä¸“å®¶ç‰¹å¾æå–
        expert_outputs = {}
        
        for expert_name, expert_model in self.experts.items():
            # è·å–ä¸“å®¶å¯¹åº”çš„æ¨¡æ€æ•°æ®
            modality = self._get_expert_modality(expert_name)
            
            if modality in data_dict:
                modality_data = data_dict[modality]
                expert_output = expert_model(modality_data)
                expert_outputs[expert_name] = expert_output
            else:
                print(f"âš ï¸  è­¦å‘Š: æ¨¡æ€ {modality} åœ¨æ•°æ®ä¸­æœªæ‰¾åˆ° (ä¸“å®¶ {expert_name})")
        
        # 2. èåˆç‰¹å¾
        if not expert_outputs:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„ä¸“å®¶è¾“å‡ºè¿›è¡Œèåˆ")
        
        fused_features = self.fusion_layer(expert_outputs)
        
        # 3. åˆ†ç±»
        logits = self.classifier(fused_features)
        
        return logits
    
    def _get_expert_modality(self, expert_name: str) -> str:
        """è·å–ä¸“å®¶å¯¹åº”çš„æ¨¡æ€åç§°"""
        expert_config = self.experts_config.get(expert_name, {})
        modality = expert_config.get('modality')
        
        if modality is None:
            # å¦‚æœæ²¡æœ‰æ˜ç¡®æŒ‡å®šæ¨¡æ€ï¼Œå°è¯•ä»ä¸“å®¶åç§°æ¨æ–­
            # ä¾‹å¦‚ï¼š'imu_expert' -> 'imu'
            modality = expert_name.split('_')[0]
        
        return modality
    
    def get_expert_info(self) -> Dict[str, Any]:
        """è·å–ä¸“å®¶ä¿¡æ¯"""
        info = {}
        for expert_name, expert in self.experts.items():
            info[expert_name] = {
                'type': type(expert).__name__,
                'output_dim': expert.get_output_dim(),
                'modality': self._get_expert_modality(expert_name),
                'parameters': sum(p.numel() for p in expert.parameters())
            }
        return info
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'num_experts': len(self.experts),
            'fusion_strategy': self.fusion_config.get('strategy', 'concatenate'),
            'num_classes': self.num_classes,
            'experts': self.get_expert_info()
        }


def create_dynamic_har_model(config: Dict[str, Any]) -> DynamicHarModel:
    """åˆ›å»ºåŠ¨æ€HARæ¨¡å‹çš„å·¥å‚å‡½æ•°"""
    return DynamicHarModel(config)


def get_example_config():
    """è·å–ç¤ºä¾‹é…ç½®"""
    return {
        'labels': {
            'num_classes': 8
        },
        'architecture': {
            'experts': {
                'imu_expert': {
                    'type': 'TransformerExpert',
                    'modality': 'imu',
                    'params': {
                        'input_dim': 6,
                        'hidden_dim': 128,
                        'output_dim': 128,
                        'num_heads': 8,
                        'num_layers': 4,
                        'dropout': 0.1
                    }
                },
                'pressure_expert': {
                    'type': 'RNNExpert',
                    'modality': 'pressure',
                    'params': {
                        'input_dim': 1,
                        'hidden_dim': 64,
                        'output_dim': 64,
                        'rnn_type': 'LSTM',
                        'num_layers': 2,
                        'bidirectional': True,
                        'dropout': 0.1
                    }
                }
            },
            'fusion': {
                'strategy': 'concatenate',  # ä»»åŠ¡2.2æ ¸å¿ƒ
                'params': {}
            },
            'classifier': {
                'type': 'MLP',
                'layers': [192, 128, 64, 8],  # 128 + 64 = 192 (æ‹¼æ¥åçš„ç»´åº¦)
                'activation': 'relu',
                'dropout': 0.2
            }
        }
    }


def run_comprehensive_test():
    """è¿è¡Œå…¨é¢æµ‹è¯•"""
    print("ğŸš€ DynamicHarModel å…¨é¢æµ‹è¯•")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    
    try:
        # 1. åˆ›å»ºé…ç½®å’Œæ¨¡å‹
        print("1ï¸âƒ£ åˆ›å»ºæ¨¡å‹...")
        config = get_example_config()
        model = create_dynamic_har_model(config)
        
        # 2. æ‰“å°æ¨¡å‹ä¿¡æ¯
        print("\n2ï¸âƒ£ æ¨¡å‹ä¿¡æ¯:")
        model_info = model.get_model_info()
        print(f"   æ€»å‚æ•°æ•°: {model_info['total_parameters']:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {model_info['trainable_parameters']:,}")
        print(f"   ä¸“å®¶æ•°é‡: {model_info['num_experts']}")
        print(f"   èåˆç­–ç•¥: {model_info['fusion_strategy']}")
        print(f"   åˆ†ç±»æ•°é‡: {model_info['num_classes']}")
        
        print("\n   ä¸“å®¶è¯¦æƒ…:")
        for expert_name, expert_info in model_info['experts'].items():
            print(f"     {expert_name}:")
            print(f"       ç±»å‹: {expert_info['type']}")
            print(f"       æ¨¡æ€: {expert_info['modality']}")
            print(f"       è¾“å‡ºç»´åº¦: {expert_info['output_dim']}")
            print(f"       å‚æ•°æ•°é‡: {expert_info['parameters']:,}")
        
        # 3. åˆ›å»ºæµ‹è¯•æ•°æ®
        print("\n3ï¸âƒ£ åˆ›å»ºæµ‹è¯•æ•°æ®...")
        batch_size = 4
        seq_len = 128
        test_data = {
            'imu': torch.randn(batch_size, seq_len, 6),
            'pressure': torch.randn(batch_size, seq_len, 1)
        }
        
        print(f"   æµ‹è¯•æ•°æ®å½¢çŠ¶:")
        for modality, data in test_data.items():
            print(f"     {modality}: {data.shape}")
        
        # 4. å‰å‘ä¼ æ’­æµ‹è¯•
        print("\n4ï¸âƒ£ å‰å‘ä¼ æ’­æµ‹è¯•...")
        with torch.no_grad():
            output = model(test_data)
        
        print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"   è¾“å‡ºèŒƒå›´: [{output.min():.3f}, {output.max():.3f}]")
        
        # æµ‹è¯•æ¦‚ç‡åˆ†å¸ƒ
        probs = torch.softmax(output, dim=-1)
        print(f"   æ¦‚ç‡åˆ†å¸ƒç¤ºä¾‹ (ç¬¬ä¸€ä¸ªæ ·æœ¬): {probs[0].numpy()}")
        print(f"   æ¦‚ç‡å’Œ: {probs.sum(dim=-1).mean():.6f} (åº”è¯¥â‰ˆ1.0)")
        
        # 5. æ¢¯åº¦æµæµ‹è¯•
        print("\n5ï¸âƒ£ æ¢¯åº¦æµæµ‹è¯•...")
        model.train()
        labels = torch.randint(0, 8, (batch_size,))
        criterion = nn.CrossEntropyLoss()
        
        # å‰å‘ä¼ æ’­
        output = model(test_data)
        loss = criterion(output, labels)
        
        print(f"   æŸå¤±å€¼: {loss.item():.4f}")
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        grad_count = 0
        total_params = 0
        for name, param in model.named_parameters():
            total_params += 1
            if param.grad is not None and param.grad.norm() > 0:
                grad_count += 1
        
        print(f"   æœ‰æ¢¯åº¦çš„å‚æ•°: {grad_count}/{total_params}")
        print(f"   æ¢¯åº¦è¦†ç›–ç‡: {grad_count/total_params*100:.1f}%")
        
        # 6. ä¸åŒèåˆç­–ç•¥æµ‹è¯•
        print("\n6ï¸âƒ£ ä¸åŒèåˆç­–ç•¥æµ‹è¯•...")
        fusion_strategies = ['concatenate', 'average', 'attention', 'weighted_sum']
        
        for strategy in fusion_strategies:
            try:
                # åˆ›å»ºæ–°é…ç½®
                test_config = get_example_config()
                test_config['architecture']['fusion']['strategy'] = strategy
                
                # å¯¹äºéæ‹¼æ¥ç­–ç•¥ï¼Œéœ€è¦ç»Ÿä¸€ä¸“å®¶è¾“å‡ºç»´åº¦
                if strategy != 'concatenate':
                    test_config['architecture']['experts']['pressure_expert']['params']['output_dim'] = 128
                    test_config['architecture']['classifier']['layers'][0] = 128
                
                test_model = create_dynamic_har_model(test_config)
                
                with torch.no_grad():
                    test_output = test_model(test_data)
                
                print(f"   âœ… {strategy}: è¾“å‡ºå½¢çŠ¶ {test_output.shape}")
                
            except Exception as e:
                print(f"   âŒ {strategy}: å¤±è´¥ - {e}")
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        print("=" * 60)
        print("âœ… DynamicHarModel å¯ä»¥æ­£å¸¸å·¥ä½œ")
        print("âœ… ä»»åŠ¡2.2 æ‹¼æ¥èåˆç­–ç•¥å®ç°æ­£ç¡®")
        print("âœ… æ¨¡å‹æ”¯æŒå¤šç§èåˆç­–ç•¥")
        print("âœ… æ¢¯åº¦æµæ­£å¸¸ï¼Œå¯ä»¥è¿›è¡Œè®­ç»ƒ")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = run_comprehensive_test()
    
    if success:
        print("\nğŸš€ ä¸‹ä¸€æ­¥:")
        print("1. å°†ä»£ç é›†æˆåˆ°å®Œæ•´é¡¹ç›®ä¸­")
        print("2. ä½¿ç”¨çœŸå®æ•°æ®è¿›è¡Œç«¯åˆ°ç«¯æµ‹è¯•")
        print("3. è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜")
        print("4. å®ç°æ›´é«˜çº§çš„èåˆç­–ç•¥")
    else:
        print("\nâš ï¸  è¯·ä¿®å¤æµ‹è¯•å¤±è´¥çš„é—®é¢˜åå†ç»§ç»­")