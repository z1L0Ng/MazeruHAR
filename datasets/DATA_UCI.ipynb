{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if running on googlecolab \n",
    "# !pip install hickle\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "# %cd drive/MyDrive/PerCom2021-FL-master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from subprocess import call\n",
    "import requests \n",
    "np.random.seed(0)\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import hickle as hkl \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for loading and downloading the dataset\n",
    "\n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "\tdataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
    "\treturn dataframe.values\n",
    " \n",
    "# load a list of files, such as x, y, z data for a given variable\n",
    "def load_group(filenames, prefix=''):\n",
    "\tloaded = list()\n",
    "\tfor name in filenames:\n",
    "\t\tdata = load_file(prefix + name)\n",
    "\t\tloaded.append(data)\n",
    "\t# stack group so that features are the 3rd dimension\n",
    "\tloaded = np.dstack(loaded)\n",
    "\treturn loaded\n",
    " \n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset(group, prefix=''):\n",
    "\tfilepath = prefix + group + '/Inertial Signals/'\n",
    "\tfilenames = list()\n",
    "\t# body acceleration\n",
    "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "\t# body gyroscope\n",
    "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "\t# load input data\n",
    "\tX = load_group(filenames, filepath)\n",
    "\t# load class output\n",
    "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "\treturn X, y\n",
    "\n",
    "# Framing data by windows\n",
    "def segmentData(accData,time_step,step):\n",
    "    segmentAccData = list()\n",
    "    for i in range(0, accData.shape[0] - time_step,step):\n",
    "        segmentAccData.append(accData[i:i+time_step,:])\n",
    "    return segmentAccData\n",
    "\n",
    "# download function for datasets\n",
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and unzipping dataset\n",
    "os.makedirs('dataset',exist_ok=True)\n",
    "print(\"downloading...\")            \n",
    "data_directory = os.path.abspath(\"dataset/UCI HAR Dataset.zip\")\n",
    "if not os.path.exists(data_directory):\n",
    "    download_url(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip\",data_directory)\n",
    "    print(\"download done\")\n",
    "else:\n",
    "    print(\"dataset already downloaded\")\n",
    "    \n",
    "data_directory2 = os.path.abspath(\"dataset/UCI HAR Dataset\")\n",
    "if not os.path.exists(data_directory2): \n",
    "    print(\"extracting data\")\n",
    "    with zipfile.ZipFile(data_directory, 'r') as zip_ref:\n",
    "        zip_ref.extractall(os.path.abspath(\"dataset/\"))\n",
    "    print(\"data extracted in \" + data_directory2)\n",
    "else:\n",
    "    print(\"Data already extracted in \" + data_directory2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSubjectList = pd.read_csv('dataset/UCI HAR Dataset/train/subject_train.txt', header=None, delim_whitespace=True).values\n",
    "testSubjectList = pd.read_csv('dataset/UCI HAR Dataset/test/subject_test.txt', header=None, delim_whitespace=True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all train\n",
    "trainX, trainy = load_dataset('train', 'dataset/UCI HAR Dataset/')\n",
    "trainy = np.asarray([x - 1 for x in trainy])\n",
    "\n",
    "# load all test\n",
    "testX, testy = load_dataset('test', 'dataset/UCI HAR Dataset/')\n",
    "testy = np.asarray([x - 1 for x in testy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedData = np.vstack((trainX,testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectList = np.vstack((trainSubjectList,testSubjectList)).squeeze()\n",
    "labels = np.vstack((trainy,testy)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbOfSubjects = len(np.unique(subjectList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectDataDict = {new_list: [] for new_list in range(nbOfSubjects)}\n",
    "subjectLabelDict = {new_list: [] for new_list in range(nbOfSubjects)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanAcc = np.mean(combinedData[:,:,:3])\n",
    "stdAcc = np.std(combinedData[:,:,:3])\n",
    "varAcc = np.var(combinedData[:,:,:3])\n",
    "\n",
    "meanGyro = np.mean(combinedData[:,:,3:])\n",
    "stdGyro = np.std(combinedData[:,:,3:])\n",
    "varGyro = np.var(combinedData[:,:,3:])\n",
    "\n",
    "normalizedAllAcc = (combinedData[:,:,:3] - meanAcc) / stdAcc\n",
    "normalizedAllGyro = (combinedData[:,:,3:] - meanGyro) / stdGyro\n",
    "normalizedAll = np.dstack((normalizedAllAcc,normalizedAllGyro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainy = np.squeeze(trainy)\n",
    "testy = np.squeeze(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataName = 'UCI'\n",
    "os.makedirs('datasetStandardized/'+dataName, exist_ok=True)\n",
    "hkl.dump(normalizedAll[:trainX.shape[0]],'datasetStandardized/'+dataName+ '/trainX.hkl' )\n",
    "hkl.dump(normalizedAll[trainX.shape[0]:],'datasetStandardized/'+dataName+ '/testX.hkl' )\n",
    "hkl.dump(trainy,'datasetStandardized/'+dataName+ '/trainY.hkl' )\n",
    "hkl.dump(testy,'datasetStandardized/'+dataName+ '/testY.hkl' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
