{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if running on googlecolab \n",
    "# !pip install hickle\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "# %cd drive/MyDrive/PerCom2021-FL-master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl \n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from subprocess import call\n",
    "import requests \n",
    "np.random.seed(0)\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processLabel(labels):\n",
    "    \"\"\"\n",
    "    处理标签，返回窗口中最常见的标签。\n",
    "    \"\"\"\n",
    "    # np.unique返回唯一值和它们的计数\n",
    "    unique_values, counts = np.unique(labels, return_counts=True)\n",
    "    if len(unique_values) > 1:\n",
    "        # 如果有多个标签，返回出现次数最多的那个\n",
    "        return unique_values[np.argmax(counts)]\n",
    "    else:\n",
    "        # 如果只有一个标签，直接返回它\n",
    "        return unique_values[0]\n",
    "\n",
    "def segmentData(accData, time_step, step):\n",
    "    \"\"\"\n",
    "    将传感器数据分割成窗口。\n",
    "    \"\"\"\n",
    "    segmentAccData = []\n",
    "    for i in range(0, accData.shape[0] - time_step, step):\n",
    "        segmentAccData.append(accData[i:i+time_step, :])\n",
    "    return np.asarray(segmentAccData)\n",
    "\n",
    "def segmentLabel(accData, time_step, step):\n",
    "    \"\"\"\n",
    "    将标签数据分割成窗口，并为每个窗口确定一个单一标签。\n",
    "    \"\"\"\n",
    "    segmentAccData = []\n",
    "    for i in range(0, accData.shape[0] - time_step, step):\n",
    "        segmentAccData.append(processLabel(accData[i:i+time_step]))\n",
    "    return np.asarray(segmentAccData)\n",
    "\n",
    "# 确保在运行其他代码前，先运行这个单元格\n",
    "print(\"辅助函数已定义。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for loading and downloading the dataset\n",
    "\n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "\tdataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
    "\treturn dataframe.values\n",
    " \n",
    "# load a list of files, such as x, y, z data for a given variable\n",
    "def load_group(filenames, prefix=''):\n",
    "\tloaded = list()\n",
    "\tfor name in filenames:\n",
    "\t\tdata = load_file(prefix + name)\n",
    "\t\tloaded.append(data)\n",
    "\t# stack group so that features are the 3rd dimension\n",
    "\tloaded = np.dstack(loaded)\n",
    "\treturn loaded\n",
    " \n",
    "# load a dataset group, such as train or test\n",
    "# Make sure the cell defining processLabel is executed before running this function\n",
    "def load_dataset(group, prefix='',position=''):\n",
    "\tfilepath = prefix + '/' + group + '/' + position\n",
    "\tfilenames = list()\n",
    "\t# body acceleration\n",
    "\tfilenames += ['Acc_x.txt', 'Acc_y.txt', 'Acc_z.txt']\n",
    "\t# body gyroscope\n",
    "\tfilenames += ['Gyr_x.txt', 'Gyr_y.txt', 'Gyr_z.txt']\n",
    "\t# load input data\n",
    "\tx = np.asarray(load_group(filenames, filepath))\n",
    "\t# load class output\n",
    "\ty =  processLabel(load_file(filepath+'/Label.txt'))\n",
    "\treturn x, y\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 新的、更健壮的下载函数\n",
    "def download_url(url, save_path, chunk_size=8192, max_retries=5):\n",
    "    \"\"\"\n",
    "    一个更稳健的下载函数，支持重试和进度条。\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"正在下载 {url} (尝试 {attempt + 1}/{max_retries})...\")\n",
    "            with requests.get(url, stream=True) as r:\n",
    "                r.raise_for_status()  # 如果发生HTTP错误，则会抛出异常\n",
    "                total_size = int(r.headers.get('content-length', 0))\n",
    "                \n",
    "                with open(save_path, 'wb') as fd, tqdm(\n",
    "                    total=total_size, unit='iB', unit_scale=True, desc=save_path.split('/')[-1]\n",
    "                ) as pbar:\n",
    "                    for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                        if chunk:\n",
    "                            fd.write(chunk)\n",
    "                            pbar.update(len(chunk))\n",
    "            print(f\"\\n文件成功下载到: {save_path}\")\n",
    "            return True # 下载成功\n",
    "        except (requests.exceptions.RequestException, requests.exceptions.ChunkedEncodingError) as e:\n",
    "            print(f\"\\n下载失败: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 5 * (attempt + 1) # 等待时间随重试次数增加\n",
    "                print(f\"将在 {wait_time} 秒后重试...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(\"已达到最大重试次数，下载失败。\")\n",
    "                return False # 下载失败\n",
    "\n",
    "# 记得要取消注释并运行下载数据的代码块！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = [\n",
    "        \"SHLDataset_preview_v1_part1.zip\",\n",
    "        \"SHLDataset_preview_v1_part2.zip\",\n",
    "        \"SHLDataset_preview_v1_part3.zip\"\n",
    "           ]\n",
    "links = [\n",
    "    \"http://www.shl-dataset.org/wp-content/uploads/SHLDataset_preview_v1_part1.zip\",\n",
    "        \"http://www.shl-dataset.org/wp-content/uploads/SHLDataset_preview_v1_part2.zip\",\n",
    "        \"http://www.shl-dataset.org/wp-content/uploads/SHLDataset_preview_v1_part3.zip\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and unzipping dataset/download\n",
    "os.makedirs('dataset/download',exist_ok=True)\n",
    "os.makedirs('dataset/extracted',exist_ok=True)\n",
    "\n",
    "for i in range(len(fileName)):\n",
    "    data_directory = os.path.abspath(\"dataset/download/\"+str(fileName[i]))\n",
    "    if not os.path.exists(data_directory):\n",
    "        print(\"downloading \"+str(fileName[i]))\n",
    "        download_url(links[i],data_directory)\n",
    "        print(\"download done\")\n",
    "        data_directory2 =  os.path.abspath(\"dataset/extracted/\"+str(fileName[i]))\n",
    "        print(\"extracting data...\")\n",
    "        with zipfile.ZipFile(data_directory, 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.path.abspath(\"dataset/extracted/\"))\n",
    "        print(\"data extracted\")\n",
    "    else:\n",
    "        print(str(fileName[i]) + \" already downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findRanges(nums):\n",
    "    nums = sorted(set(nums))\n",
    "    gaps = [[s, e] for s, e in zip(nums, nums[1:]) if s+1 < e]\n",
    "    edges = iter(nums[:1] + sum(gaps, []) + nums[-1:])\n",
    "    return list(zip(edges, edges))\n",
    "\n",
    "def unionRange(a):\n",
    "    b = []\n",
    "    for begin,end in sorted(a):\n",
    "        if b and b[-1][1] >= begin - 1:\n",
    "            b[-1][1] = max(b[-1][1], end + 1)\n",
    "        else:\n",
    "            b.append([begin, end + 1])\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodyLocations = [\"Bag\",\"Hand\",\"Hips\",\"Torso\"] \n",
    "rootDirectory = 'dataset/extracted/SHLDataset_preview_v1'\n",
    "\n",
    "if not os.path.exists(rootDirectory):\n",
    "\traise FileNotFoundError(f\"Directory '{rootDirectory}' does not exist. Please ensure the dataset is downloaded and extracted.\")\n",
    "\n",
    "dirs = [d for d in os.listdir(rootDirectory) if os.path.isdir(os.path.join(rootDirectory, d))]\n",
    "if \"scripts\" in dirs:\n",
    "\tdirs.remove(\"scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processLabel(labels):\n",
    "    uniqueCount = np.unique(labels,return_counts=True)\n",
    "    if(len(uniqueCount[0]) > 1):\n",
    "        return uniqueCount[0][np.argmax(uniqueCount[1])]\n",
    "    else:\n",
    "        return uniqueCount[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentData(accData,time_step,step):\n",
    "    segmentAccData = list()\n",
    "    for i in range(0, accData.shape[0] - time_step,step):\n",
    "        segmentAccData.append(accData[i:i+time_step,:])\n",
    "\n",
    "\n",
    "    return np.asarray(segmentAccData)\n",
    "def segmentLabel(accData,time_step,step):\n",
    "    segmentAccData = list()\n",
    "    for i in range(0, accData.shape[0] - time_step,step):\n",
    "        segmentAccData.append(processLabel(accData[i:i+time_step]))\n",
    "        \n",
    "    return np.asarray(segmentAccData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downSampleLowPass(motionData):\n",
    "    accX = signal.decimate(motionData[:,:,0],2)\n",
    "    accY = signal.decimate(motionData[:,:,1],2)\n",
    "    accZ = signal.decimate(motionData[:,:,2],2)\n",
    "    gyroX = signal.decimate(motionData[:,:,3],2)\n",
    "    gyroY = signal.decimate(motionData[:,:,4],2)\n",
    "    gyroZ = signal.decimate(motionData[:,:,5],2)\n",
    "    return np.dstack((accX,accY,accZ,gyroX,gyroY,gyroZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"processing data...\")\n",
    "userData = []\n",
    "userLabel = []\n",
    "for userSubFolder in dirs:\n",
    "    print(\"procesing \"+userSubFolder)\n",
    "    subDir =  rootDirectory + \"/\"+userSubFolder\n",
    "    timeSubFolder = [d for d in os.listdir(subDir) if os.path.isdir(os.path.join(subDir, d))]\n",
    "\n",
    "    start = True\n",
    "    for timeDir in timeSubFolder:\n",
    "        dataPosition = []\n",
    "        dataDir = subDir + \"/\" + timeDir\n",
    "        labelPosition = load_file(dataDir+\"/Label.txt\")[:,1]\n",
    "        nanSortedLocation = []\n",
    "        for locations in bodyLocations:\n",
    "            dataPosition.append(load_file(dataDir+\"/\"+locations +\"_Motion.txt\")[:,1:7])\n",
    "            nanlocation = findRanges(np.unique(np.where(np.isnan(dataPosition[-1]))))\n",
    "            for eachRange in nanlocation:\n",
    "                nanSortedLocation.append(eachRange)\n",
    "        deleteRange = unionRange(nanSortedLocation)\n",
    "        for i in reversed(range(len(deleteRange))):\n",
    "            labelPosition = np.delete(labelPosition,np.s_[deleteRange[i][0]:deleteRange[i][1]],axis=0)\n",
    "            for bodyCount in range(len(bodyLocations)):\n",
    "                dataPosition[bodyCount]  = np.delete(dataPosition[bodyCount],np.s_[deleteRange[i][0]:deleteRange[i][1]],axis=0)\n",
    "                \n",
    "#         segmenting data and removing null class frames       \n",
    "        labelPosition = segmentLabel(labelPosition,256,128) \n",
    "        nullFrameToDelete = np.where(labelPosition == 0 )\n",
    "        labelPosition = np.delete(labelPosition,nullFrameToDelete)\n",
    "        labelPosition = labelPosition - 1\n",
    "        labelPosition = np.swapaxes(np.repeat(labelPosition[:,  np.newaxis], len(bodyLocations), axis=1),0,1)\n",
    "        for bodyCount in range(len(bodyLocations)):\n",
    "            dataPosition[bodyCount] = segmentData(dataPosition[bodyCount],256,128)\n",
    "            dataPosition[bodyCount] = np.delete(dataPosition[bodyCount],nullFrameToDelete,axis=0)\n",
    "            dataPosition[bodyCount] = downSampleLowPass(dataPosition[bodyCount])\n",
    "        userData.append(dataPosition)\n",
    "        userLabel.append(labelPosition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedUserData = np.hstack((userData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedAccData = combinedUserData[:,:,:,:3]\n",
    "combinedGyroData = combinedUserData[:,:,:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accMean =  np.mean(combinedAccData)\n",
    "accStd =  np.std(combinedAccData)\n",
    "                   \n",
    "gyroMean =  np.mean(combinedGyroData)\n",
    "gyroStd =  np.std(combinedGyroData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userData = np.asarray(userData, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for clientIndex in range(userData.shape[0]):\n",
    "    for bodyIndex in range(userData.shape[1]):\n",
    "        userData[clientIndex][bodyIndex][:,:,:3] = (userData[clientIndex][bodyIndex][:,:,:3] - accMean)/accStd\n",
    "        userData[clientIndex][bodyIndex][:,:,3:] = (userData[clientIndex][bodyIndex][:,:,3:] - gyroMean)/gyroStd\n",
    "        labels.append(userLabel[clientIndex][bodyIndex])\n",
    "labels = np.asarray(labels, dtype=object)\n",
    "userData = np.hstack((userData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startIndex = 0\n",
    "endIndex = 0 \n",
    "dataName = 'SHL'\n",
    "os.makedirs('datasetStandardized/'+dataName, exist_ok=True)\n",
    "hkl.dump(userData,'datasetStandardized/'+dataName+'/clientsData.hkl' )\n",
    "hkl.dump(labels,'datasetStandardized/'+dataName+'/clientsLabel.hkl' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data processing finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HAR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
