{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHL Multimodal Data Processing Notebook\n",
    "\n",
    "This notebook is an enhanced version of the original `DATA_SHL.ipynb`. It's designed to extract and process **all available motion sensor data** from the SHL preview dataset, including:\n",
    "\n",
    "- Accelerometer (Acc)\n",
    "- Gyroscope (Gyr)\n",
    "- Magnetometer (Mag)\n",
    "- Linear Acceleration (LAcc)\n",
    "- Gravity (Gra)\n",
    "- Orientation (Ori)\n",
    "\n",
    "This prepares the data for use in a multimodal Mixture-of-Experts (MoE) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle as hkl\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import zipfile\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Helper Functions (Enhanced)\n",
    "\n",
    "These functions are updated to handle multiple sensor modalities and ensure data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url, save_path, chunk_size=8192, max_retries=5):\n",
    "    \"\"\"A robust download function with retries and a progress bar.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Downloading {url} (Attempt {attempt + 1}/{max_retries})...\")\n",
    "            with requests.get(url, stream=True, timeout=30) as r:\n",
    "                r.raise_for_status()\n",
    "                total_size = int(r.headers.get('content-length', 0))\n",
    "                with open(save_path, 'wb') as fd, tqdm(\n",
    "                    total=total_size, unit='iB', unit_scale=True, desc=os.path.basename(save_path)\n",
    "                ) as pbar:\n",
    "                    for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                        if chunk:\n",
    "                            fd.write(chunk)\n",
    "                            pbar.update(len(chunk))\n",
    "            print(f\"\\nFile successfully downloaded to: {save_path}\")\n",
    "            return True\n",
    "        except (requests.exceptions.RequestException, requests.exceptions.ChunkedEncodingError) as e:\n",
    "            print(f\"\\nDownload failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 5 * (attempt + 1)\n",
    "                print(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(\"Max retries reached. Download failed.\")\n",
    "                return False\n",
    "\n",
    "def process_label_for_window(labels):\n",
    "    \"\"\"Determines the most common label in a window.\"\"\"\n",
    "    unique_values, counts = np.unique(labels, return_counts=True)\n",
    "    return unique_values[np.argmax(counts)]\n",
    "\n",
    "def segment_data(data, window_size, step_size):\n",
    "    \"\"\"Segments time-series data into windows.\"\"\"\n",
    "    segments = []\n",
    "    for i in range(0, data.shape[0] - window_size, step_size):\n",
    "        segments.append(data[i:i + window_size, :])\n",
    "    return np.asarray(segments)\n",
    "\n",
    "def segment_labels(labels, window_size, step_size):\n",
    "    \"\"\"Segments labels and assigns one label per window.\"\"\"\n",
    "    segmented_labels = []\n",
    "    for i in range(0, labels.shape[0] - window_size, step_size):\n",
    "        segmented_labels.append(process_label_for_window(labels[i:i + window_size]))\n",
    "    return np.asarray(segmented_labels)\n",
    "\n",
    "def downsample_data_block(data_block, factor=2):\n",
    "    \"\"\"Downsamples a block of windowed data using a low-pass filter.\"\"\"\n",
    "    if factor <= 1:\n",
    "        return data_block\n",
    "    return signal.decimate(data_block, factor, axis=1) # decimate along the time axis (axis=1)\n",
    "\n",
    "def find_and_union_nan_ranges(data_list):\n",
    "    \"\"\"Finds NaN ranges across multiple dataframes and unions them.\"\"\"\n",
    "    nan_ranges = []\n",
    "    for data in data_list:\n",
    "        nan_indices = np.unique(np.where(np.isnan(data))[0])\n",
    "        if len(nan_indices) > 0:\n",
    "            # Convert indices to ranges\n",
    "            nums = sorted(set(nan_indices))\n",
    "            gaps = [[s, e] for s, e in zip(nums, nums[1:]) if s+1 < e]\n",
    "            edges = iter(nums[:1] + sum(gaps, []) + nums[-1:])\n",
    "            nan_ranges.extend(list(zip(edges, edges)))\n",
    "    \n",
    "    # Union all ranges\n",
    "    if not nan_ranges:\n",
    "        return []\n",
    "    b = []\n",
    "    for begin, end in sorted(nan_ranges):\n",
    "        if b and b[-1][1] >= begin - 1:\n",
    "            b[-1][1] = max(b[-1][1], end)\n",
    "        else:\n",
    "            b.append([begin, end])\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Download and Extraction\n",
    "\n",
    "This section downloads and extracts the SHL Preview dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://www.shl-dataset.org/wp-content/uploads/SHLDataset_preview_v1_part1.zip (Attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SHLDataset_preview_v1_part1.zip: 100%|██████████| 2.90G/2.90G [04:28<00:00, 10.8MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File successfully downloaded to: /files1/Zilong/MazeruHAR/datasets/dataset/download/SHLDataset_preview_v1_part1.zip\n",
      "Downloading http://www.shl-dataset.org/wp-content/uploads/SHLDataset_preview_v1_part2.zip (Attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SHLDataset_preview_v1_part2.zip: 100%|██████████| 2.50G/2.50G [03:54<00:00, 10.6MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File successfully downloaded to: /files1/Zilong/MazeruHAR/datasets/dataset/download/SHLDataset_preview_v1_part2.zip\n",
      "Downloading http://www.shl-dataset.org/wp-content/uploads/SHLDataset_preview_v1_part3.zip (Attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SHLDataset_preview_v1_part3.zip: 100%|██████████| 2.28G/2.28G [03:22<00:00, 11.2MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File successfully downloaded to: /files1/Zilong/MazeruHAR/datasets/dataset/download/SHLDataset_preview_v1_part3.zip\n",
      "\n",
      "--- Starting Extraction Check ---\n",
      "Extracting SHLDataset_preview_v1_part1.zip...\n",
      "Successfully extracted SHLDataset_preview_v1_part1.zip.\n",
      "Extracting SHLDataset_preview_v1_part2.zip...\n",
      "Successfully extracted SHLDataset_preview_v1_part2.zip.\n",
      "Extracting SHLDataset_preview_v1_part3.zip...\n",
      "Successfully extracted SHLDataset_preview_v1_part3.zip.\n",
      "--- Extraction process complete ---\n"
     ]
    }
   ],
   "source": [
    "# 替换 \"2. Data Download and Extraction\" 下的代码单元格\n",
    "\n",
    "file_names = [\n",
    "    \"SHLDataset_preview_v1_part1.zip\",\n",
    "    \"SHLDataset_preview_v1_part2.zip\",\n",
    "    \"SHLDataset_preview_v1_part3.zip\"\n",
    "]\n",
    "links = [\n",
    "    \"http://www.shl-dataset.org/wp-content/uploads/SHLDataset_preview_v1_part1.zip\",\n",
    "    \"http://www.shl-dataset.org/wp-content/uploads/SHLDataset_preview_v1_part2.zip\",\n",
    "    \"http://www.shl-dataset.org/wp-content/uploads/SHLDataset_preview_v1_part3.zip\"\n",
    "]\n",
    "\n",
    "download_dir = os.path.abspath(\"dataset/download\")\n",
    "extract_dir = os.path.abspath(\"dataset/extracted/\") # 修正路径以匹配项目结构\n",
    "\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# --- 修正后的逻辑 ---\n",
    "\n",
    "# 阶段 1: 确保所有文件都已下载\n",
    "for file_name, link in zip(file_names, links):\n",
    "    file_path = os.path.join(download_dir, file_name)\n",
    "    if not os.path.exists(file_path):\n",
    "        download_url(link, file_path)\n",
    "    else:\n",
    "        print(f\"{file_name} already downloaded.\")\n",
    "\n",
    "# 阶段 2: 确保所有已下载的文件都被解压\n",
    "print(\"\\n--- Starting Extraction Check ---\")\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(download_dir, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Extracting {file_name}...\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_dir)\n",
    "            print(f\"Successfully extracted {file_name}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting {file_name}: {e}\")\n",
    "\n",
    "print(\"--- Extraction process complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multimodal Data Processing\n",
    "\n",
    "Here we define the sensors and columns we want to extract. The `_Motion.txt` file contains multiple sensors. We will extract them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 user directories: ['User1', 'User2', 'User3']\n"
     ]
    }
   ],
   "source": [
    "BODY_LOCATIONS = [\"Bag\", \"Hand\", \"Hips\", \"Torso\"]\n",
    "ROOT_DIRECTORY = 'dataset/extracted/SHLDataset_preview_v1'\n",
    "\n",
    "# Define all sensors and their columns in _Motion.txt\n",
    "# Format: { 'name': [col1, col2, ...], ... }\n",
    "SENSOR_CONFIG = {\n",
    "    'acc': [1, 2, 3],      # Accelerometer\n",
    "    'gyro': [4, 5, 6],     # Gyroscope\n",
    "    'mag': [7, 8, 9],      # Magnetometer\n",
    "    'ori': [10, 11, 12, 13], # Orientation (Quaternion)\n",
    "    'gra': [14, 15, 16],   # Gravity\n",
    "    'lacc': [17, 18, 19]   # Linear Acceleration\n",
    "}\n",
    "\n",
    "if not os.path.exists(ROOT_DIRECTORY):\n",
    "    raise FileNotFoundError(f\"Directory '{ROOT_DIRECTORY}' does not exist. Please ensure the dataset is downloaded and extracted.\")\n",
    "\n",
    "user_dirs = [d for d in os.listdir(ROOT_DIRECTORY) if os.path.isdir(os.path.join(ROOT_DIRECTORY, d)) and 'User' in d]\n",
    "print(f\"Found {len(user_dirs)} user directories: {user_dirs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting multimodal data processing...\n",
      "\n",
      "Processing User1...\n",
      "  - Processing session: 220617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2255721/699170039.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  labels_raw = pd.read_csv(os.path.join(session_path, 'Label.txt'), header=None, delim_whitespace=True).values[:, 1]\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing session: 260617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2255721/699170039.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  labels_raw = pd.read_csv(os.path.join(session_path, 'Label.txt'), header=None, delim_whitespace=True).values[:, 1]\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing session: 270617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2255721/699170039.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  labels_raw = pd.read_csv(os.path.join(session_path, 'Label.txt'), header=None, delim_whitespace=True).values[:, 1]\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing User2...\n",
      "  - Processing session: 140617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2255721/699170039.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  labels_raw = pd.read_csv(os.path.join(session_path, 'Label.txt'), header=None, delim_whitespace=True).values[:, 1]\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing session: 140717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2255721/699170039.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  labels_raw = pd.read_csv(os.path.join(session_path, 'Label.txt'), header=None, delim_whitespace=True).values[:, 1]\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing session: 180717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2255721/699170039.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  labels_raw = pd.read_csv(os.path.join(session_path, 'Label.txt'), header=None, delim_whitespace=True).values[:, 1]\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing User3...\n",
      "  - Processing session: 030717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2255721/699170039.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  labels_raw = pd.read_csv(os.path.join(session_path, 'Label.txt'), header=None, delim_whitespace=True).values[:, 1]\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing session: 070717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2255721/699170039.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  labels_raw = pd.read_csv(os.path.join(session_path, 'Label.txt'), header=None, delim_whitespace=True).values[:, 1]\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing session: 140617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2255721/699170039.py:18: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  labels_raw = pd.read_csv(os.path.join(session_path, 'Label.txt'), header=None, delim_whitespace=True).values[:, 1]\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
      "/tmp/ipykernel_2255721/699170039.py:24: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data processing finished for all users.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting multimodal data processing...\")\n",
    "all_users_data = []\n",
    "all_users_labels = []\n",
    "\n",
    "for user_folder in sorted(user_dirs):\n",
    "    print(f\"\\nProcessing {user_folder}...\")\n",
    "    user_path = os.path.join(ROOT_DIRECTORY, user_folder)\n",
    "    time_folders = [d for d in os.listdir(user_path) if os.path.isdir(os.path.join(user_path, d))]\n",
    "    \n",
    "    user_sessions_data = []\n",
    "    user_sessions_labels = []\n",
    "\n",
    "    for time_folder in sorted(time_folders):\n",
    "        session_path = os.path.join(user_path, time_folder)\n",
    "        print(f\"  - Processing session: {time_folder}\")\n",
    "\n",
    "        # Load label data first\n",
    "        labels_raw = pd.read_csv(os.path.join(session_path, 'Label.txt'), header=None, delim_whitespace=True).values[:, 1]\n",
    "\n",
    "        # Load all motion data for all locations\n",
    "        motion_data_per_location = {}\n",
    "        for location in BODY_LOCATIONS:\n",
    "            motion_file = os.path.join(session_path, f\"{location}_Motion.txt\")\n",
    "            motion_data_per_location[location] = pd.read_csv(motion_file, header=None, delim_whitespace=True).values\n",
    "\n",
    "        # Clean NaN values across all locations simultaneously\n",
    "        all_motion_dfs = [pd.DataFrame(data) for data in motion_data_per_location.values()]\n",
    "        combined_df = pd.concat(all_motion_dfs, axis=1)\n",
    "        nan_rows = combined_df.isnull().any(axis=1)\n",
    "        \n",
    "        labels_clean = labels_raw[~nan_rows]\n",
    "        for location in BODY_LOCATIONS:\n",
    "            motion_data_per_location[location] = motion_data_per_location[location][~nan_rows]\n",
    "\n",
    "        # Process each body location as a separate client/stream\n",
    "        for location in BODY_LOCATIONS:\n",
    "            # Extract all configured sensors and concatenate them\n",
    "            sensor_columns = []\n",
    "            for sensor, cols in SENSOR_CONFIG.items():\n",
    "                sensor_columns.append(motion_data_per_location[location][:, cols])\n",
    "            multimodal_data = np.concatenate(sensor_columns, axis=1)\n",
    "            \n",
    "            # Segment data and labels\n",
    "            data_segmented = segment_data(multimodal_data, window_size=256, step_size=128)\n",
    "            labels_segmented = segment_labels(labels_clean, window_size=256, step_size=128)\n",
    "            \n",
    "            # Filter out 'null' class (label 0)\n",
    "            non_null_indices = np.where(labels_segmented != 0)\n",
    "            data_filtered = data_segmented[non_null_indices]\n",
    "            labels_filtered = labels_segmented[non_null_indices]\n",
    "            \n",
    "            # Adjust labels to be 0-indexed\n",
    "            labels_adjusted = labels_filtered - 1\n",
    "            \n",
    "            # Downsample the data\n",
    "            data_downsampled = downsample_data_block(data_filtered, factor=2)\n",
    "\n",
    "            if data_downsampled.shape[0] > 0:\n",
    "                user_sessions_data.append(data_downsampled)\n",
    "                user_sessions_labels.append(labels_adjusted)\n",
    "    \n",
    "    # Combine all sessions for the current user/location stream\n",
    "    if user_sessions_data:\n",
    "        all_users_data.extend(user_sessions_data)\n",
    "        all_users_labels.extend(user_sessions_labels)\n",
    "\n",
    "print(\"\\nData processing finished for all users.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalization and Saving\n",
    "\n",
    "The final step is to perform Z-score normalization across the entire dataset and save the processed data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stacked data shape for normalization: (640152, 128, 19)\n",
      "\n",
      "Normalization complete.\n",
      "Saved 36 clients/streams.\n",
      "Data and labels saved in: datasetStandardized/SHL_Multimodal\n"
     ]
    }
   ],
   "source": [
    "# Stack all client data for global normalization\n",
    "if all_users_data:\n",
    "    stacked_data = np.vstack(all_users_data)\n",
    "    print(f\"Total stacked data shape for normalization: {stacked_data.shape}\")\n",
    "\n",
    "    # Z-score normalization\n",
    "    mean = np.mean(stacked_data, axis=(0, 1))\n",
    "    std = np.std(stacked_data, axis=(0, 1))\n",
    "    std[std == 0] = 1 # Avoid division by zero\n",
    "\n",
    "    # Normalize each client's data and save\n",
    "    data_name = 'SHL_Multimodal'\n",
    "    output_dir = os.path.join('datasetStandardized', data_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    normalized_clients_data = []\n",
    "    for client_data_block in all_users_data:\n",
    "        normalized_block = (client_data_block - mean) / std\n",
    "        normalized_clients_data.append(normalized_block)\n",
    "\n",
    "    # Save as a list of numpy arrays, which is what the original scripts expect\n",
    "    hkl.dump(normalized_clients_data, os.path.join(output_dir, 'clientsData.hkl'))\n",
    "    hkl.dump(all_users_labels, os.path.join(output_dir, 'clientsLabel.hkl'))\n",
    "    \n",
    "    print(f\"\\nNormalization complete.\")\n",
    "    print(f\"Saved {len(normalized_clients_data)} clients/streams.\")\n",
    "    print(f\"Data and labels saved in: {output_dir}\")\n",
    "else:\n",
    "    print(\"No data was processed. Please check the source directories and file structures.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "har",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
