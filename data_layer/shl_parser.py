"""
SHLæ•°æ®è§£æå™¨ - å®Œæ•´ä¿®å¤ç‰ˆæœ¬
"""

import os
import numpy as np
import pandas as pd
import hickle as hkl
from typing import Dict, List, Tuple, Any
from scipy import signal
import logging

# ä¿®å¤å¯¼å…¥é”™è¯¯
try:
    from .base_parser import DataParser
except ImportError:
    try:
        from data_layer.base_parser import DataParser
    except ImportError:
        from base_parser import DataParser


class SHLDataParser(DataParser):
    """
    SHLæ•°æ®é›†è§£æå™¨ - æ‰©å±•ç‰ˆæœ¬
    æ”¯æŒä»é¢„å¤„ç†çš„HKLæ–‡ä»¶åŠ è½½æ•°æ®ï¼Œå¹¶æ”¯æŒæ›´å¤šä¼ æ„Ÿå™¨æ¨¡æ€
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.window_size = config.get('window_size', 128)
        self.step_size = config.get('step_size', 64)
        self.sample_rate = config.get('sample_rate', 100)
        self.normalize_per_sample = config.get('normalize_per_sample', True)
        
        # SHLæ•°æ®é›†çš„æ´»åŠ¨æ ‡ç­¾æ˜ å°„ (0-based)
        self.activity_labels = {
            0: 'Standing', 1: 'Walking', 2: 'Running', 3: 'Biking',
            4: 'Car', 5: 'Bus', 6: 'Train', 7: 'Subway'
        }
        
        # æ‰©å±•çš„æ¨¡æ€é…ç½®
        self.modalities_config = config.get('modalities', {
            'imu': {'enabled': True, 'channels': 6},
            'pressure': {'enabled': True, 'channels': 1}
        })
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        
        # æ•°æ®é›†åˆ’åˆ†æ¯”ä¾‹
        self.split_ratios = {
            'train': 0.7,
            'val': 0.15, 
            'test': 0.15
        }

    def load_preprocessed_data(self) -> Tuple[List[np.ndarray], List[np.ndarray]]:
        """ä»é¢„å¤„ç†çš„HKLæ–‡ä»¶åŠ è½½æ•°æ®"""
        data_path = self.config.get('data_path', 'datasets/datasetStandardized/SHL_Multimodal/')
        
        all_data = []
        all_labels = []
        
        # è¿™é‡Œåº”è¯¥æ ¹æ®å®é™…çš„SHLæ•°æ®ç»“æ„æ¥å®ç°
        # ä¸ºäº†æµ‹è¯•ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€äº›è™šæ‹Ÿæ•°æ®
        self.logger.info(f"ä» {data_path} åŠ è½½é¢„å¤„ç†æ•°æ®...")
        
        if not os.path.exists(data_path):
            self.logger.warning(f"æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {data_path}")
            # åˆ›å»ºè™šæ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
            num_samples = 1000
            for i in range(num_samples):
                # åˆ›å»º19é€šé“çš„è™šæ‹Ÿæ•°æ® (IMU:6 + ç£åŠ›è®¡:3 + å…¶ä»–:10)
                sample_data = np.random.randn(128, 19)
                all_data.append(sample_data)
                all_labels.append(np.random.randint(0, 8))
            
            self.logger.info(f"åˆ›å»ºäº† {num_samples} ä¸ªè™šæ‹Ÿæ ·æœ¬ç”¨äºæµ‹è¯•")
        else:
            # å®é™…çš„æ•°æ®åŠ è½½é€»è¾‘
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„HKLæ–‡ä»¶ç»“æ„æ¥å®ç°
            pass
        
        return all_data, all_labels

    def parse_data(self, split: str) -> Tuple[List[Dict[str, np.ndarray]], List[int]]:
        """
        è§£ææ•°æ®å¹¶è¿”å›æ¨¡æ€å­—å…¸æ ¼å¼
        """
        self.logger.info(f"ğŸš€ å¼€å§‹è§£æ {split} æ•°æ®é›†...")
        
        # åŠ è½½é¢„å¤„ç†æ•°æ®
        all_data, all_labels = self.load_preprocessed_data()
        
        if not all_data:
            raise ValueError("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ•°æ®")
        
        # æ•°æ®é›†åˆ’åˆ†
        total_samples = len(all_data)
        if split == 'train':
            start_idx = 0
            end_idx = int(total_samples * self.split_ratios['train'])
        elif split == 'val':
            start_idx = int(total_samples * self.split_ratios['train'])
            end_idx = start_idx + int(total_samples * self.split_ratios['val'])
        else:  # test
            start_idx = int(total_samples * (self.split_ratios['train'] + self.split_ratios['val']))
            end_idx = total_samples
        
        split_data = all_data[start_idx:end_idx]
        split_labels = all_labels[start_idx:end_idx]
        
        self.logger.info(f"{split} æ•°æ®é›†: {len(split_data)} æ ·æœ¬")
        
        # åˆ†ç¦»å¤šæ¨¡æ€æ•°æ®
        self.logger.info("åˆ†ç¦»å¤šæ¨¡æ€æ•°æ®...")
        processed_data = []
        
        for i, data in enumerate(split_data):
            try:
                modalities = self.split_modalities(data)
                processed_data.append(modalities)
            except Exception as e:
                self.logger.error(f"å¤„ç†æ ·æœ¬ {i} æ—¶å‡ºé”™: {e}")
                raise
        
        self.logger.info(f"æˆåŠŸè§£æ {len(processed_data)} ä¸ªæ ·æœ¬")
        
        # æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ¨¡æ€ä¿¡æ¯
        if processed_data:
            first_sample = processed_data[0]
            self.logger.info(f"ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ¨¡æ€: {list(first_sample.keys())}")
            for modality, data in first_sample.items():
                self.logger.info(f"  {modality}: å½¢çŠ¶ {data.shape}")
        
        return processed_data, split_labels

    def split_modalities(self, data: np.ndarray) -> Dict[str, np.ndarray]:
        """
        å°†å¤šæ¨¡æ€æ•°æ®åˆ†ç¦» - ä¿®å¤ç‰ˆæœ¬ï¼Œæ­£ç¡®æ”¯æŒç£åŠ›è®¡
        """
        modalities = {}
        total_channels = data.shape[-1]
        self.logger.debug(f"è¾“å…¥æ•°æ®å½¢çŠ¶={data.shape}, æ€»é€šé“æ•°={total_channels}")
        
        # æŒ‰é¡ºåºå¤„ç†æ¯ä¸ªå¯ç”¨çš„æ¨¡æ€
        for modality_name, modality_config in self.modalities_config.items():
            if not modality_config.get('enabled', False):
                continue
            
            self._extract_modality(data, modality_name, modalities, total_channels)

        if not modalities:
            raise ValueError("æ²¡æœ‰æˆåŠŸæå–ä»»ä½•æ¨¡æ€æ•°æ®")

        return modalities

    def _extract_modality(self, data: np.ndarray, modality_name: str, 
                         modalities: Dict[str, np.ndarray], total_channels: int):
        """
        æå–å•ä¸ªæ¨¡æ€çš„æ•°æ® - ä¿®å¤ç£åŠ›è®¡æå–é€»è¾‘
        """
        if modality_name == 'imu':
            # IMUæ¨¡æ€ï¼šåŠ é€Ÿåº¦è®¡(3) + é™€èºä»ª(3) = 6é€šé“
            if total_channels >= 6:
                acc_data = data[:, :, 0:3]    # åŠ é€Ÿåº¦è®¡
                gyro_data = data[:, :, 3:6]   # é™€èºä»ª
                imu_data = np.concatenate([acc_data, gyro_data], axis=-1)
                modalities['imu'] = imu_data
                self.logger.debug(f"æå–IMUæ¨¡æ€: {imu_data.shape}")
            else:
                self.logger.warning("æ•°æ®é€šé“ä¸è¶³ï¼Œæ— æ³•æå–IMUæ¨¡æ€")

        elif modality_name == 'pressure':
            # å‹åŠ›æ¨¡æ€ï¼šä½¿ç”¨æœ€åä¸€åˆ—ä½œä¸ºå‹åŠ›æ•°æ®
            if total_channels >= 7:
                modalities['pressure'] = data[:, :, -1:]
                self.logger.debug(f"æå–å‹åŠ›æ¨¡æ€: {modalities['pressure'].shape}")
            else:
                self.logger.warning("æ•°æ®é€šé“ä¸è¶³ï¼Œåˆ›å»ºè™šæ‹Ÿå‹åŠ›æ•°æ®")
                modalities['pressure'] = np.zeros((data.shape[0], data.shape[1], 1))

        elif modality_name == 'magnetometer':
            # ç£åŠ›è®¡æ¨¡æ€ï¼šåˆ—6-8 (åŸºäºSHLæ•°æ®é›†æ ‡å‡†æ ¼å¼)
            if total_channels >= 9:
                magnetometer_data = data[:, :, 6:9]
                modalities['magnetometer'] = magnetometer_data
                self.logger.debug(f"æˆåŠŸæå–ç£åŠ›è®¡æ¨¡æ€: {magnetometer_data.shape}")
            else:
                self.logger.warning(f"æ•°æ®é€šé“ä¸è¶³({total_channels})ï¼Œåˆ›å»ºè™šæ‹Ÿç£åŠ›è®¡æ•°æ®")
                modalities['magnetometer'] = np.zeros((data.shape[0], data.shape[1], 3))

        else:
            self.logger.warning(f"æœªçŸ¥çš„æ¨¡æ€ç±»å‹: {modality_name}")

    def get_modality_info(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æ¨¡æ€ä¿¡æ¯"""
        modality_info = {}
        
        for modality_name, modality_config in self.modalities_config.items():
            if modality_config.get('enabled', False):
                modality_info[modality_name] = {
                    'channels': modality_config.get('channels', 1),
                    'enabled': True,
                    'sequence_length': self.window_size
                }
        
        return modality_info

    def normalize_data(self, data: np.ndarray) -> np.ndarray:
        """æ•°æ®æ ‡å‡†åŒ–"""
        if self.normalize_per_sample:
            # æ ·æœ¬çº§æ ‡å‡†åŒ–
            normalized_data = []
            for sample in data:
                mean = np.mean(sample, axis=0, keepdims=True)
                std = np.std(sample, axis=0, keepdims=True)
                std = np.where(std == 0, 1, std)
                normalized_sample = (sample - mean) / std
                normalized_data.append(normalized_sample)
            return np.array(normalized_data)
        else:
            # å…¨å±€æ ‡å‡†åŒ–
            original_shape = data.shape
            reshaped_data = data.reshape(-1, original_shape[-1])
            
            mean = np.mean(reshaped_data, axis=0)
            std = np.std(reshaped_data, axis=0)
            std = np.where(std == 0, 1, std)
            
            normalized_reshaped = (reshaped_data - mean) / std
            return normalized_reshaped.reshape(original_shape)

