# ===================================================================
# SHL数据集多模态配置文件 - 任务3.1
# 包含IMU和气压传感器模态的完整配置
# ===================================================================

# 实验基本信息
name: "SHL_MultiModal_Experiment"
description: "SHL数据集多模态融合实验 - IMU + 气压传感器"
version: "1.0"
author: "MazeruHAR Team"
created_date: "2024-12-01"

# 环境配置
environment:
  seed: 42
  device: "auto"
  num_workers: 4
  pin_memory: true

# 数据集配置
dataset:
  name: "SHL"
  path: "/Users/zilongzeng/Research/MazeruHAR/datasets/dataset/extracted/"
  preprocessing:
    window_size: 128
    step_size: 64
    sample_rate: 100
  activity_labels:
    1: "Standing"
    2: "Walking" 
    3: "Running"
    4: "Biking"
    5: "Car"
    6: "Bus"
    7: "Train"
    8: "Subway"
  data_split:
    train: 0.7
    validation: 0.15
    test: 0.15
    stratify: true
  modalities:
    imu:
      enabled: true
      channels: 6
    pressure:
      enabled: true
      channels: 1

# 模型架构配置
architecture:
  type: "DynamicHarModel"
  num_classes: 8
  
  # 专家模型配置
  experts:
    imu_expert:
      # 已修正: "TransformerExpert" -> "transformer"
      type: "transformer"
      modality: "imu"
      params:
        input_shape: [128, 6]
        output_dim: 128
        input_dim: 6
        hidden_dim: 256
        num_heads: 8
        num_layers: 4
        dropout: 0.1
        
    pressure_expert:
      # 已修正: "RNNExpert" -> "rnn"
      type: "rnn"
      modality: "pressure"
      params:
        input_shape: [128, 1]
        output_dim: 32
        input_dim: 1
        hidden_dim: 64
        num_layers: 2
        rnn_type: "LSTM"
        dropout: 0.2
        bidirectional: true
  
  # 特征融合配置
  fusion:
    strategy: "concatenate"
    params: {}
      
  # 分类器配置
  classifier:
    type: "MLP"
    layers: [160, 64, 8]
    activation: "relu"
    dropout: 0.4

# 训练配置
training:
  epochs: 100
  batch_size: 32
  learning_rate: 0.001
  weight_decay: 0.0001
  optimizer: "AdamW"
  scheduler: "CosineAnnealingLR"
  loss: "CrossEntropyLoss"
  early_stopping_patience: 15

# ... (文件的其余部分保持不变) ...

# ... (文件的其余部分保持不变) ...
  # 学习率调度器
  scheduler: "cosine"  # cosine, step, plateau, null
  scheduler_params:
    T_max: 100
    eta_min: 1e-6
  
  # 正则化
  label_smoothing: 0.1
  gradient_clip_norm: 1.0
  
  # 早停策略
  early_stopping:
    enabled: true
    patience: 15
    monitor: "val_f1"
    mode: "max"
    min_delta: 0.001

# 验证和测试配置
evaluation:
  metrics: ["accuracy", "f1_macro", "f1_weighted", "precision", "recall"]
  test_during_training: false
  validation_frequency: 1

# 可视化配置
visualization:
  enabled: true
  plots:
    learning_curves: true
    confusion_matrix: true
    class_distribution: true
    feature_distribution: true
    attention_maps: true
    tsne_embeddings: true
  figure_size: [10, 8]
  dpi: 300
  save_format: "png"

# 实验跟踪配置
experiment_tracking:
  enabled: false
  platform: "wandb"
  project_name: "MazeruHAR-SHL"
  tags: ["SHL", "multimodal", "IMU", "pressure"]

# ===================================================================
# 以下是所有可用专家模型的完整配置示例
# 您可以根据需要选择和组合不同的专家
# ===================================================================

# 可选专家配置示例
alternative_experts:
  
  # 1. CNN专家配置
  cnn_expert_example:
    type: "cnn"
    params:
      num_layers: 3
      base_channels: 64
      kernel_sizes: [3, 5, 7]
      use_multiscale: true
      use_residual: true
      dropout: 0.2
      output_dim: 128
  
  # 2. 混合专家配置 (RNN + Transformer)
  hybrid_expert_example:
    type: "hybrid"
    params:
      projection_dim: 128
      rnn_hidden_dim: 64
      rnn_num_layers: 2
      num_heads: 4
      num_transformer_layers: 2
      fusion_method: "attention"  # concat, add, attention
      dropout: 0.1
      output_dim: 128
  
  # 3. 深度RNN专家配置
  deep_rnn_expert_example:
    type: "rnn"
    params:
      hidden_dim: 128
      num_layers: 4
      rnn_type: "lstm"  # lstm, gru, rnn
      bidirectional: true
      use_channel_specific: true
      dropout: 0.3
      output_dim: 128
  
  # 4. 轻量级CNN专家配置
  lightweight_cnn_example:
    type: "cnn"
    params:
      num_layers: 2
      base_channels: 32
      kernel_sizes: [3, 5]
      use_multiscale: false
      use_residual: false
      dropout: 0.1
      output_dim: 64

# ===================================================================
# 专家预设配置 - 可以直接使用预定义的专家
# ===================================================================

# 预设专家类型 (可以在experts配置中直接使用)
expert_presets:
  # Transformer预设
  - name: "transformer_small"
    description: "小型Transformer专家，适用于轻量级任务"
  - name: "transformer_medium" 
    description: "中型Transformer专家，平衡性能和资源"
  - name: "transformer_large"
    description: "大型Transformer专家，适用于复杂任务"
  
  # RNN预设
  - name: "rnn_lstm"
    description: "基于LSTM的RNN专家"
  - name: "rnn_gru"
    description: "基于GRU的RNN专家"
  - name: "rnn_channel_specific"
    description: "通道特定的RNN专家，适用于多传感器数据"
  - name: "rnn_deep"
    description: "深度RNN专家，多层结构"
  
  # CNN预设
  - name: "cnn_simple"
    description: "简单CNN专家，基础特征提取"
  - name: "cnn_multiscale"
    description: "多尺度CNN专家，捕获不同时间模式"
  - name: "cnn_deep"
    description: "深度CNN专家，带残差连接"
  - name: "cnn_lightweight"
    description: "轻量级CNN专家，资源约束环境"
  
  # 混合专家预设
  - name: "hybrid_small"
    description: "小型混合专家，结合RNN和Transformer"
  - name: "hybrid_medium"
    description: "中型混合专家，平衡架构"
  - name: "hybrid_large"
    description: "大型混合专家，复杂时序建模"
  - name: "hybrid_attention_fusion"
    description: "基于注意力融合的混合专家"

# 使用预设的示例配置
example_preset_usage:
  experts:
    imu_expert:
      preset: "transformer_medium"  # 使用预设
      modality: "imu"
    
    pressure_expert:
      preset: "rnn_gru"  # 使用预设
      modality: "pressure"