{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MazeruHAR 动态训练引擎\n",
    "\n",
    "欢迎使用 MazeruHAR 的配置驱动训练引擎。此 Notebook 旨在提供一个灵活、可重现且易于使用的训练流程。\n",
    "\n",
    "**核心理念:** **代码只写一次，实验配置万千。**\n",
    "\n",
    "您只需要执行以下两个简单步骤即可开始训练：\n",
    "\n",
    "1.  **配置实验**: 修改位于 `config/` 目录下的 `.yaml` 配置文件。您可以复制 `config/default_configs/shl_config.yaml` 并根据您的需求进行调整，比如更换数据集、模型架构或超参数。\n",
    "2.  **运行 Notebook**: 在下面的 **“实验配置”**单元格中设置好配置文件的路径，然后从头到尾运行此 Notebook 即可。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 1: 环境设置与库导入\n",
    "\n",
    "此单元格负责导入所有必需的库并设置初始环境。它整合了项目所需的所有依赖项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准库导入\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import random\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "# 第三方库导入\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR, ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# --- 项目内部模块导入 ---\n",
    "# 为了确保 Notebook 的可移植性，我们将关键的模块代码直接包含进来，\n",
    "# 同时保留从文件导入的逻辑作为备用方案。\n",
    "try:\n",
    "    # 优先从文件系统导入\n",
    "    from config.config_loader import ConfigLoader\n",
    "    from model_layer.dynamic_har_model import create_dynamic_har_model\n",
    "    import utils_torch as utils\n",
    "    print(\"成功从项目文件导入模块。\")\n",
    "except ImportError as e:\n",
    "    print(f\"从文件导入模块失败: {e}。将使用 Notebook 内联定义。\")\n",
    "    \n",
    "    # ========================== Fallback Logic: Start ==========================\n",
    "    # 如果导入失败，定义完整的后备（Fallback）实现，以保证 Notebook 能独立运行。\n",
    "\n",
    "    # 1. Fallback for 'HARDataset'\n",
    "    class HARDataset(Dataset):\n",
    "        \"\"\"一个标准的 PyTorch 数据集类。\"\"\"\n",
    "        def __init__(self, data: torch.Tensor, labels: torch.Tensor):\n",
    "            self.data = data\n",
    "            self.labels = labels\n",
    "        def __len__(self) -> int:\n",
    "            return len(self.data)\n",
    "        def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "            return self.data[idx], self.labels[idx]\n",
    "\n",
    "    # 2. Fallback for 'create_dynamic_har_model'\n",
    "    def create_dynamic_har_model(config: Dict[str, Any]) -> nn.Module:\n",
    "        \"\"\"\n",
    "        创建一个简单的占位符CNN模型用于HAR。\n",
    "        此函数作为实际动态模型工厂的替代品。\n",
    "        \"\"\"\n",
    "        dataset_params = config.get('dataset', {})\n",
    "        # 从配置中获取模型参数，或使用默认值\n",
    "        n_features = dataset_params.get('n_features', 9)      # e.g., 9-axis sensor data\n",
    "        num_classes = dataset_params.get('num_classes', 8)    # e.g., 8 activity classes\n",
    "        seq_len = dataset_params.get('window_size', 128)      # Sequence length\n",
    "\n",
    "        class SimpleHAR_CNN(nn.Module):\n",
    "            def __init__(self, in_channels: int, num_classes: int, sequence_length: int):\n",
    "                super().__init__()\n",
    "                # 输入形状: (batch, features, sequence_len)，例如 (B, 9, 128)\n",
    "                self.conv1 = nn.Conv1d(in_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "                self.relu1 = nn.ReLU()\n",
    "                self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "                \n",
    "                self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "                self.relu2 = nn.ReLU()\n",
    "                self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "                \n",
    "                # 经过两次步长为2的最大池化后，序列长度减半两次：sequence_length / 4\n",
    "                flattened_size = 64 * (sequence_length // 4)\n",
    "                self.flatten = nn.Flatten()\n",
    "                self.fc1 = nn.Linear(flattened_size, 128)\n",
    "                self.relu3 = nn.ReLU()\n",
    "                self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "            def forward(self, data_dict: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "                # 假设所有输入数据都在 'imu' 这个键下\n",
    "                x = data_dict['imu']  # 预期形状: (Batch, Length, Features), e.g., (B, 128, 9)\n",
    "                x = x.permute(0, 2, 1) # 转换为: (Batch, Features, Length), e.g., (B, 9, 128)\n",
    "                \n",
    "                x = self.pool1(self.relu1(self.conv1(x)))\n",
    "                x = self.pool2(self.relu2(self.conv2(x)))\n",
    "                x = self.flatten(x)\n",
    "                x = self.relu3(self.fc1(x))\n",
    "                x = self.fc2(x)\n",
    "                return x\n",
    "        \n",
    "        print(f\"创建了一个简单的CNN占位模型 (输入特征: {n_features}, 类别数: {num_classes})\")\n",
    "        return SimpleHAR_CNN(n_features, num_classes, seq_len)\n",
    "\n",
    "    # 3. Fallback for 'utils.load_dataset_pytorch'\n",
    "    class MockDatasetLoader:\n",
    "        \"\"\"一个模拟的数据加载器，用于生成合成数据。\"\"\"\n",
    "        def __init__(self, config: Dict[str, Any], seed: int):\n",
    "            print(\"正在生成模拟HAR数据...\")\n",
    "            # 从配置中获取数据参数\n",
    "            n_features = config.get('n_features', 9)\n",
    "            seq_len = config.get('window_size', 128)\n",
    "            num_classes = config.get('num_classes', 8)\n",
    "            \n",
    "            # 设置样本数量\n",
    "            n_train_samples = 2000\n",
    "            n_test_samples = 500\n",
    "            \n",
    "            # 使用torch生成随机数据\n",
    "            self.central_train_data = torch.randn(n_train_samples, seq_len, n_features)\n",
    "            self.central_train_label = torch.randint(0, num_classes, (n_train_samples,))\n",
    "            self.central_test_data = torch.randn(n_test_samples, seq_len, n_features)\n",
    "            self.central_test_label = torch.randint(0, num_classes, (n_test_samples,))\n",
    "            print(\"模拟数据生成完毕。\")\n",
    "\n",
    "    def load_dataset_pytorch(dataset_name: str, batch_size: int, type: str, seed: int, data_path: str, config: Dict[str, Any]):\n",
    "        \"\"\"加载数据集的后备函数，返回一个包含合成数据的加载器对象。\"\"\"\n",
    "        # 注意：这里的参数是为了与原始调用签名保持一致\n",
    "        return MockDatasetLoader(config.get('dataset', {}), seed)\n",
    "\n",
    "    # 创建一个模拟的 'utils' 模块，并将后备函数/类附加到它上面\n",
    "    utils = type('utils', (), {\n",
    "        'HARDataset': HARDataset,\n",
    "        'load_dataset_pytorch': load_dataset_pytorch\n",
    "    })\n",
    "    \n",
    "    # =========================== Fallback Logic: End ===========================\n",
    "\n",
    "print(\"所有模块已准备就绪。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 2: 实验配置\n",
    "\n",
    "**这是您唯一需要修改的单元格。**\n",
    "\n",
    "请将 `CONFIG_PATH`变量设置为您想要使用的配置文件的路径。所有实验参数都将从此文件加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 核心配置 ==================\n",
    "# 🔥 只需修改此处的配置文件路径即可开始新的实验\n",
    "CONFIG_PATH = 'config/default_configs/shl_config.yaml'  # 默认使用根目录下的config.yaml\n",
    "# ==============================================\n",
    "\n",
    "# 检查配置文件是否存在\n",
    "if not os.path.exists(CONFIG_PATH):\n",
    "    print(f\"❌ 错误: 配置文件 '{CONFIG_PATH}' 未找到!\")\n",
    "    print(\"请确保路径正确，或创建一个新的配置文件。\")\n",
    "else:\n",
    "    print(f\"✓ 将使用配置文件: '{CONFIG_PATH}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 3: 核心训练器类\n",
    "\n",
    "下面的 `ConfigurableTrainer` 类是整个训练流程的核心。它封装了从配置加载、环境设置、数据处理、模型创建、训练、评估到结果可视化的所有逻辑。您无需修改此类中的任何代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurableTrainer:\n",
    "    \"\"\"配置驱动的训练器类\"\"\"\n",
    "\n",
    "    def __init__(self, config_path: str):\n",
    "        self.config_path = config_path\n",
    "        self.config = self._load_config(config_path)\n",
    "        self.params = self._extract_parameters()\n",
    "        self.device = None\n",
    "        self.output_dir = None\n",
    "        self.logger = None\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.criterion = None\n",
    "        self.history = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': [], 'val_f1': []}\n",
    "\n",
    "    def _load_config(self, path: str) -> Dict[str, Any]:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return yaml.safe_load(f)\n",
    "\n",
    "    def _extract_parameters(self) -> Dict[str, Any]:\n",
    "        \"\"\"将配置文件中的所有层级展平到一个字典中以便于访问。\"\"\"\n",
    "        params = {}\n",
    "        for key, value in self.config.items():\n",
    "            if isinstance(value, dict):\n",
    "                params.update(value)\n",
    "            else:\n",
    "                params[key] = value\n",
    "        return params\n",
    "\n",
    "    def setup_environment(self):\n",
    "        \"\"\"设置随机种子、设备和日志记录\"\"\"\n",
    "        seed = self.params.get('seed', 42)\n",
    "        random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "        device_pref = self.params.get('device', 'auto')\n",
    "        if device_pref == 'auto':\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device_pref)\n",
    "        \n",
    "        self.output_dir = Path(self.params.get('output_dir', './results/default_experiment'))\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # 设置日志\n",
    "        logging.basicConfig(level=logging.INFO, \n",
    "                            format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "                            handlers=[logging.FileHandler(self.output_dir / 'training.log'), logging.StreamHandler(sys.stdout)])\n",
    "        self.logger = logging.getLogger()\n",
    "        self.logger.info(f\"环境设置完成。设备: {self.device}, 输出目录: {self.output_dir}\")\n",
    "\n",
    "    def load_data(self) -> Tuple[DataLoader, DataLoader, DataLoader, np.ndarray]:\n",
    "        \"\"\"根据配置加载并准备SHL多模态数据\"\"\"\n",
    "        self.logger.info(f\"加载SHL数据集: {self.params['name']}\")\n",
    "        \n",
    "        # *** 关键修改点 1: 使用新的数据层接口 ***\n",
    "        from data_layer import SHLDataParser, UniversalHARDataset\n",
    "        \n",
    "        # 创建数据解析器配置\n",
    "        parser_config = {\n",
    "            'name': 'SHL',\n",
    "            'data_path': self.params.get('path', './datasets/'),\n",
    "            'window_size': self.params.get('window_size', 128),\n",
    "            'step_size': self.params.get('step_size', 64),\n",
    "            'sample_rate': self.params.get('sample_rate', 100),\n",
    "            'modalities': {\n",
    "                'imu': {'enabled': True, 'channels': 6},\n",
    "                'pressure': {'enabled': True, 'channels': 1}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 创建数据解析器和数据集\n",
    "        parser = SHLDataParser(parser_config)\n",
    "        train_dataset = UniversalHARDataset(parser, split='train')\n",
    "        dev_dataset = UniversalHARDataset(parser, split='val')\n",
    "        test_dataset = UniversalHARDataset(parser, split='test')\n",
    "        \n",
    "        # 获取标签用于类权重计算\n",
    "        train_labels = []\n",
    "        for _, label in train_dataset:\n",
    "            train_labels.append(label)\n",
    "        train_labels = np.array(train_labels)\n",
    "        \n",
    "        # 创建DataLoaders\n",
    "        common_params = {\n",
    "            'batch_size': self.params['batch_size'], \n",
    "            'num_workers': self.params.get('num_workers', 0), \n",
    "            'pin_memory': self.params.get('pin_memory', True)\n",
    "        }\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, shuffle=True, **common_params)\n",
    "        dev_loader = DataLoader(dev_dataset, shuffle=False, **common_params)\n",
    "        test_loader = DataLoader(test_dataset, shuffle=False, **common_params)\n",
    "        \n",
    "        self.logger.info(\"SHL多模态数据加载完成。\")\n",
    "        self.logger.info(f\"训练集大小: {len(train_dataset)}\")\n",
    "        self.logger.info(f\"验证集大小: {len(dev_dataset)}\")\n",
    "        self.logger.info(f\"测试集大小: {len(test_dataset)}\")\n",
    "        \n",
    "        return train_loader, dev_loader, test_loader, train_labels\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"根据配置构建多模态模型\"\"\"\n",
    "        self.logger.info(\"构建SHL多模态模型...\")\n",
    "        \n",
    "        # *** 关键修改点 5: 使用新的动态模型创建接口 ***\n",
    "        from model_layer import create_dynamic_har_model\n",
    "        \n",
    "        # 确保配置包含必要的信息\n",
    "        if not hasattr(self.config, 'architecture'):\n",
    "            raise ValueError(\"配置文件缺少architecture部分\")\n",
    "        \n",
    "        # 创建动态模型\n",
    "        self.model = create_dynamic_har_model(self.config).to(self.device)\n",
    "        \n",
    "        # 打印模型信息\n",
    "        total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        self.logger.info(f\"模型构建完成。总参数量: {total_params:,}\")\n",
    "        \n",
    "        # 打印专家模型信息\n",
    "        if hasattr(self.model, 'experts'):\n",
    "            self.logger.info(\"专家模型信息:\")\n",
    "            for expert_name, expert in self.model.experts.items():\n",
    "                expert_params = sum(p.numel() for p in expert.parameters() if p.requires_grad)\n",
    "                self.logger.info(f\"  {expert_name}: {expert_params:,} 参数\")\n",
    "        \n",
    "    def setup_training_components(self, train_labels: np.ndarray):\n",
    "        \"\"\"设置优化器、损失函数和调度器\"\"\"\n",
    "        weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "        class_weights = torch.FloatTensor(weights).to(self.device)\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=self.params.get('label_smoothing', 0.0))\n",
    "\n",
    "        opt_name = self.params.get('optimizer', 'adamw').lower()\n",
    "        opt_map = {'adam': optim.Adam, 'adamw': optim.AdamW, 'sgd': optim.SGD}\n",
    "        self.optimizer = opt_map[opt_name](self.model.parameters(), lr=self.params['learning_rate'], weight_decay=self.params.get('weight_decay', 1e-4))\n",
    "\n",
    "        scheduler_name = self.params.get('scheduler', 'cosine')\n",
    "        if scheduler_name == 'cosine':\n",
    "            self.scheduler = CosineAnnealingLR(self.optimizer, T_max=self.params['epochs'])\n",
    "        elif scheduler_name == 'step':\n",
    "            self.scheduler = StepLR(self.optimizer, step_size=30, gamma=0.1)\n",
    "        else: \n",
    "            self.scheduler = None\n",
    "        self.logger.info(\"训练组件设置完成。\")\n",
    "\n",
    "    def train(self, train_loader, dev_loader):\n",
    "        \"\"\"训练方法 - 关键修改多模态数据处理\"\"\"\n",
    "        self.logger.info(\"--- 开始训练 ---\")\n",
    "        best_val_f1 = 0.0\n",
    "        patience_counter = 0\n",
    "        patience = self.params.get('early_stopping_patience', 10)\n",
    "\n",
    "        for epoch in range(self.params['epochs']):\n",
    "            self.model.train()\n",
    "            train_loss, train_correct, train_total = 0, 0, 0\n",
    "\n",
    "            for batch_idx, (data_dict, targets) in enumerate(train_loader):\n",
    "                # *** 关键修改点 2: 处理多模态数据字典 ***\n",
    "                # 不再假设所有数据都是'imu'模态\n",
    "                # data_dict已经是正确的格式: {'imu': tensor, 'pressure': tensor}\n",
    "\n",
    "                # 将数据移到设备上\n",
    "                data_dict = {k: v.to(self.device) for k, v in data_dict.items()}\n",
    "                targets = targets.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # *** 关键修改点 3: 模型接收数据字典 ***\n",
    "                outputs = self.model(data_dict)  # 模型现在接收多模态数据字典\n",
    "\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "\n",
    "                if self.params.get('gradient_clip_norm', 0) > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(),\n",
    "                        self.params['gradient_clip_norm']\n",
    "                    )\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                train_total += targets.size(0)\n",
    "                train_correct += (predicted == targets).sum().item()\n",
    "\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "            # 验证阶段\n",
    "            val_loss, val_f1, val_acc = self.evaluate(dev_loader, is_test=False)\n",
    "            train_acc = train_correct / train_total\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Epoch {epoch+1}/{self.params['epochs']} | \"\n",
    "                f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\"\n",
    "            )\n",
    "\n",
    "            # 保存历史记录和早停逻辑\n",
    "            self.history['train_loss'].append(train_loss / len(train_loader))\n",
    "            self.history['train_accuracy'].append(train_acc)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_accuracy'].append(val_acc)\n",
    "            self.history['val_f1'].append(val_f1)\n",
    "\n",
    "            # 早停与模型保存\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                patience_counter = 0\n",
    "                if self.params.get('save_checkpoints', True):\n",
    "                    torch.save(self.model.state_dict(), self.output_dir / 'best_model.pth')\n",
    "                    self.logger.info(f\"新最佳模型已保存，验证F1分数: {best_val_f1:.4f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                self.logger.info(f\"早停触发! 最佳验证F1: {best_val_f1:.4f}\")\n",
    "                break\n",
    "\n",
    "    def evaluate(self, data_loader, is_test=False):\n",
    "        \"\"\"评估方法 - 支持多模态数据\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data_dict, targets in data_loader:\n",
    "                # *** 关键修改点 4: 评估时的多模态数据处理 ***\n",
    "                data_dict = {k: v.to(self.device) for k, v in data_dict.items()}\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                outputs = self.model(data_dict)  # 使用数据字典\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        # 计算指标\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        accuracy = accuracy_score(all_targets, all_preds)\n",
    "        f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "        \n",
    "        return avg_loss, f1, accuracy\n",
    "\n",
    "    def plot_learning_curves(self):\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history['train_loss'], label='Train Loss')\n",
    "        plt.plot(self.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Loss vs. Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.history['train_accuracy'], label='Train Accuracy')\n",
    "        plt.plot(self.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Accuracy vs. Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_path = self.output_dir / 'learning_curves.png'\n",
    "        plt.savefig(save_path)\n",
    "        self.logger.info(f\"学习曲线已保存至 {save_path}\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred, labels):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        save_path = self.output_dir / 'confusion_matrix.png'\n",
    "        plt.savefig(save_path)\n",
    "        self.logger.info(f\"混淆矩阵已保存至 {save_path}\")\n",
    "        plt.close() # 关闭图像，防止在notebook中重复显示\n",
    "\n",
    "    def validate_shl_config(self):\n",
    "        \"\"\"验证SHL配置文件的完整性\"\"\"\n",
    "        required_sections = ['dataset', 'architecture', 'training']\n",
    "        \n",
    "        for section in required_sections:\n",
    "            if not hasattr(self.config, section):\n",
    "                raise ValueError(f\"配置文件缺少必需的 '{section}' 部分\")\n",
    "        \n",
    "        # 验证数据集配置\n",
    "        dataset_config = self.config.dataset\n",
    "        if dataset_config.name != 'SHL':\n",
    "            raise ValueError(\"配置文件不是为SHL数据集设计的\")\n",
    "        \n",
    "        # 验证模态配置\n",
    "        modalities = dataset_config.modalities\n",
    "        if 'imu' not in modalities or 'pressure' not in modalities:\n",
    "            raise ValueError(\"SHL配置必须包含IMU和pressure模态\")\n",
    "        \n",
    "        # 验证架构配置\n",
    "        experts = self.config.architecture.experts\n",
    "        expected_experts = ['imu_expert', 'pressure_expert']\n",
    "        for expert in expected_experts:\n",
    "            if expert not in experts:\n",
    "                raise ValueError(f\"配置文件缺少必需的专家模型: {expert}\")\n",
    "    \n",
    "        self.logger.info(\"✓ SHL配置文件验证通过\")\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"完整的训练流程 - SHL多模态版本\"\"\"\n",
    "        try:\n",
    "            # 设置环境\n",
    "            self.setup_environment()\n",
    "            \n",
    "            # 验证配置 (新增)\n",
    "            if self.params.get('use_config', False):\n",
    "                self.validate_shl_config()\n",
    "            \n",
    "            # 设置输出目录和日志\n",
    "            self.output_dir = Path(self.params.get('output_dir', './results/shl_experiment'))\n",
    "            self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            self.logger.info(\"开始SHL多模态训练实验\")\n",
    "            self.logger.info(f\"使用配置: {self.config_path}\")\n",
    "            \n",
    "            # 加载数据\n",
    "            train_loader, dev_loader, test_loader, train_labels = self.load_data()\n",
    "            \n",
    "            # 构建模型\n",
    "            self.build_model()\n",
    "            \n",
    "            # 设置训练组件\n",
    "            self.setup_training_components(train_labels)\n",
    "            \n",
    "            # 训练\n",
    "            self.train(train_loader, dev_loader)\n",
    "            \n",
    "            # 最终测试\n",
    "            self.logger.info(\"进行最终测试...\")\n",
    "            test_loss, test_f1, test_acc = self.evaluate(test_loader, is_test=True)\n",
    "            self.logger.info(f\"最终测试结果 - Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, F1: {test_f1:.4f}\")\n",
    "            \n",
    "            self.logger.info(\"SHL多模态训练实验完成!\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"训练过程中发生错误: {str(e)}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 4: 执行训练流程\n",
    "\n",
    "最后，我们实例化 `ConfigurableTrainer` 类并调用其 `run` 方法来启动整个训练和评估流程。所有操作都将由之前加载的配置驱动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # 使用SHL配置文件\n",
    "        config_file = 'config/default_configs/shl_config.yaml'\n",
    "        \n",
    "        if not os.path.exists(config_file):\n",
    "            print(f\"❌ SHL配置文件 {config_file} 不存在!\")\n",
    "            print(\"请先创建SHL数据集的配置文件\")\n",
    "        else:\n",
    "            print(f\"✓ 使用SHL配置文件: {config_file}\")\n",
    "            trainer = ConfigurableTrainer(config_path=config_file)\n",
    "            trainer.run()\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n用户中断训练\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nSHL训练流程发生错误: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HAR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
