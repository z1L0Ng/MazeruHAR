{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准库导入\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "# 第三方库导入\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 项目内部导入\n",
    "try:\n",
    "    from config.config_loader import ConfigLoader\n",
    "    from config.config_bridge import ConfigBridge\n",
    "except ImportError:\n",
    "    print(\"警告: 配置模块未找到，将只使用传统模式\")\n",
    "    ConfigLoader = None\n",
    "    ConfigBridge = None\n",
    "\n",
    "import utils_torch as utils\n",
    "import model_cbranchformer as model\n",
    "\n",
    "print(\"所有模块导入完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 从 utils_torch.py 移入的关键代码 ===\n",
    "class HARDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# === 配置桥接器 (简化版) ===\n",
    "class DotDict(dict):\n",
    "    \"\"\"支持点号访问的字典\"\"\"\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self._dict = {}\n",
    "        for key, value in data.items():\n",
    "            self._dict[key] = self._to_dot_notation(value)\n",
    "            setattr(self, key, self._dict[key])\n",
    "    \n",
    "    def keys(self):\n",
    "        return self._dict.keys()\n",
    "    \n",
    "    def items(self):\n",
    "        return [(k, getattr(self, k)) for k in self._dict.keys()]\n",
    "    \n",
    "    def get(self, key, default=None):\n",
    "        return getattr(self, key, default)\n",
    "    \n",
    "    def _to_dot_notation(self, data):\n",
    "        if isinstance(data, dict):\n",
    "            return DotDict(data)\n",
    "        elif isinstance(data, list):\n",
    "            return [self._to_dot_notation(i) for i in data]\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "class SimplifiedConfigBridge:\n",
    "    \"\"\"简化的配置桥接器\"\"\"\n",
    "    def __init__(self, config_path: str = None):\n",
    "        self.config_path = config_path\n",
    "        self.raw_config = {}\n",
    "        \n",
    "        if config_path and os.path.exists(config_path):\n",
    "            with open(config_path, 'r', encoding='utf-8') as f:\n",
    "                self.raw_config = yaml.safe_load(f)\n",
    "        \n",
    "        self.config = self._to_dot_notation(self.raw_config)\n",
    "    \n",
    "    def _to_dot_notation(self, data):\n",
    "        if isinstance(data, dict):\n",
    "            return DotDict(data)\n",
    "        elif isinstance(data, list):\n",
    "            return [self._to_dot_notation(i) for i in data]\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "    def get_dataset_config(self) -> Dict[str, Any]:\n",
    "        return self.raw_config.get('dataset', {})\n",
    "\n",
    "    def get_training_config(self) -> Dict[str, Any]:\n",
    "        return self.raw_config.get('training', {})\n",
    "        \n",
    "    def get_visualization_config(self) -> Dict[str, Any]:\n",
    "        return self.raw_config.get('visualization', {})\n",
    "\n",
    "print(\"✓ 辅助工具和类定义完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurableTrainer:\n",
    "    \"\"\"配置驱动的训练器类\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: Optional[str] = None):\n",
    "        \"\"\"初始化训练器\"\"\"\n",
    "        self.config_path = config_path\n",
    "        self.use_config = config_path is not None and os.path.exists(config_path)\n",
    "        \n",
    "        # 初始化配置桥接器\n",
    "        if self.use_config:\n",
    "            try:\n",
    "                self.config_bridge = SimplifiedConfigBridge(config_path)\n",
    "                print(f\"✓ 配置文件加载成功: {config_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 配置文件加载失败: {e}\")\n",
    "                self.use_config = False\n",
    "        \n",
    "        # 初始化其他属性\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.criterion = None\n",
    "        self.device = None\n",
    "        self.logger = None\n",
    "        self.gradient_clip_norm = 0\n",
    "        self.output_dir = None\n",
    "    \n",
    "    def get_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取训练参数\"\"\"\n",
    "        if self.use_config:\n",
    "            return self._get_config_parameters()\n",
    "        else:\n",
    "            return self._get_hardcoded_parameters()\n",
    "    \n",
    "    def _get_config_parameters(self) -> Dict[str, Any]:\n",
    "        \"\"\"从配置文件获取参数\"\"\"\n",
    "        config = self.config_bridge.raw_config\n",
    "        \n",
    "        params = {\n",
    "            # 基本设置\n",
    "            'device': config.get('device', 'auto'),\n",
    "            'random_seed': config.get('seed', 42),\n",
    "            'output_dir': config.get('output_dir', './results/shl_multimodal'),\n",
    "            'save_checkpoints': config.get('save_checkpoints', True),\n",
    "            'verbose': config.get('verbose', True),\n",
    "            \n",
    "            # 数据集配置\n",
    "            'dataset_name': config.get('dataset', {}).get('name', 'SHL'),\n",
    "            'data_path': config.get('dataset', {}).get('path', './datasets/'),\n",
    "            \n",
    "            # 训练配置\n",
    "            'batch_size': config.get('training', {}).get('batch_size', 32),\n",
    "            'learning_rate': config.get('training', {}).get('learning_rate', 0.001),\n",
    "            'epochs': config.get('training', {}).get('epochs', 100),\n",
    "            'weight_decay': config.get('training', {}).get('weight_decay', 0.0001),\n",
    "            'gradient_clip_norm': config.get('training', {}).get('gradient_clip_norm', 1.0),\n",
    "            'early_stopping_patience': config.get('training', {}).get('early_stopping_patience', 10),\n",
    "        }\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def setup_environment(self, params: Dict[str, Any]) -> None:\n",
    "        \"\"\"设置训练环境\"\"\"\n",
    "        try:\n",
    "            # 设置随机种子\n",
    "            seed = params['random_seed']\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed(seed)\n",
    "                torch.cuda.manual_seed_all(seed)\n",
    "            \n",
    "            # 设置设备\n",
    "            device_config = params['device']\n",
    "            if device_config == 'auto':\n",
    "                if torch.cuda.is_available():\n",
    "                    self.device = torch.device('cuda')\n",
    "                    print(f\"使用 CUDA 设备: {torch.cuda.get_device_name()}\")\n",
    "                elif torch.backends.mps.is_available():\n",
    "                    self.device = torch.device('mps')\n",
    "                    print(\"使用 MPS 设备\")\n",
    "                else:\n",
    "                    self.device = torch.device('cpu')\n",
    "                    print(\"使用 CPU 设备\")\n",
    "            else:\n",
    "                self.device = torch.device(device_config)\n",
    "                print(f\"使用指定设备: {self.device}\")\n",
    "            \n",
    "            # 设置输出目录\n",
    "            self.output_dir = Path(params['output_dir'])\n",
    "            self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # 保存梯度裁剪参数\n",
    "            self.gradient_clip_norm = params.get('gradient_clip_norm', 0)\n",
    "            \n",
    "            # PyTorch 性能优化\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.deterministic = False\n",
    "            \n",
    "            print(\"✓ 环境设置完成\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 环境设置失败: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_data(self, params: Dict[str, Any]) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "        \"\"\"加载数据 - 占位符方法\"\"\"\n",
    "        # 这个方法在实际使用中会被外部数据加载逻辑替代\n",
    "        print(\"⚠️  使用占位符数据加载方法\")\n",
    "        return None, None, None\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"执行训练\"\"\"\n",
    "        print(\"开始配置驱动训练...\")\n",
    "        \n",
    "        # 获取参数并设置环境\n",
    "        params = self.get_params()\n",
    "        self.setup_environment(params)\n",
    "        \n",
    "        print(\"✓ 配置驱动训练器初始化完成\")\n",
    "        print(\"   实际训练将由后续cells执行\")\n",
    "\n",
    "print(\"✓ ConfigurableTrainer类定义完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 配置生成 - 支持SHL_Multimodal数据集 ==========\n",
    "# 🔥 每次训练只需要修改这个cell中的參數\n",
    "\n",
    "# === 基本配置参数 ===\n",
    "USE_CONFIG_MODE = False  # True=配置驱动模式, False=传统模式 (推荐False以确保稳定性)\n",
    "CONFIG_PATH = \"config/default_configs/shl_config.yaml\"  # SHL数据集配置文件路径\n",
    "\n",
    "# === SHL_Multimodal数据集配置 ===\n",
    "dataset_name = \"SHL\"  # 数据集名称\n",
    "# 🔥 修改为您的实际文件路径\n",
    "SHL_DATA_PATH = \"/Users/zilongzeng/Research/MazeruHAR/datasets/datasetStandardized/SHL_Multimodal\"\n",
    "\n",
    "data_config = {\n",
    "    'window_size': 128,\n",
    "    'step_size': 64,\n",
    "    'normalize': True,\n",
    "    'filter_freq': 20,\n",
    "    'modalities': ['imu', 'magnetometer']  # 启用的模态\n",
    "}\n",
    "\n",
    "# === 🔥 主要训练配置 (经常修改的参数) ===\n",
    "TRAINING_CONFIG = {\n",
    "    # 训练基本参数\n",
    "    'batch_size': 32,               # 🔥 批次大小: 16, 32, 64\n",
    "    'learning_rate': 0.001,         # 🔥 学习率: 0.0001, 0.001, 0.01\n",
    "    'epochs': 10,                  # 🔥 训练轮数: 50, 100, 150, 200\n",
    "    'weight_decay': 0.0001,         # 权重衰减\n",
    "    'dropout_rate': 0.1,            # Dropout率: 0.1, 0.2, 0.3\n",
    "    'label_smoothing': 0.1,         # 标签平滑\n",
    "    'gradient_clip_norm': 1.0,      # 梯度裁剪\n",
    "    'early_stopping_patience': 10, # 早停耐心值: 5, 10, 15\n",
    "    \n",
    "    # 模型架构参数\n",
    "    'projection_dim': 192,          # 🔥 投影维度: 128, 192, 256\n",
    "    'num_heads': 4,                 # 🔥 注意力头数: 4, 6, 8\n",
    "    'num_layers': 3,                # 🔥 层数: 2, 3, 4\n",
    "    'patch_size': 16,               # 补丁大小: 8, 16, 32\n",
    "    'time_step': 16,                # 时间步长: 8, 16, 32\n",
    "    'conv_kernel_size': 15,         # 卷积核大小: 7, 15, 31\n",
    "}\n",
    "\n",
    "# === 传统模式配置 ===\n",
    "traditional_config = {\n",
    "    # 数据集相关\n",
    "    'dataset_name': dataset_name,\n",
    "    'data_path': SHL_DATA_PATH,  # 使用您的实际路径\n",
    "    'window_size': 128,\n",
    "    'step_size': 64,\n",
    "    'sample_rate': 100,\n",
    "    \n",
    "    # 合并训练配置\n",
    "    **TRAINING_CONFIG,\n",
    "    \n",
    "    # 其他设置\n",
    "    'device': 'auto',               # 'auto', 'cpu', 'cuda', 'mps'\n",
    "    'random_seed': 42,              # 随机种子\n",
    "    'output_dir': './results/shl_multimodal',\n",
    "    'save_checkpoints': True,\n",
    "    'verbose': True,\n",
    "    'position_device': '',          # SHL数据集不使用位置设备分割\n",
    "    'main_dir': './'\n",
    "}\n",
    "\n",
    "# === SHL_Multimodal数据集特定配置 ===\n",
    "shl_specific_config = {\n",
    "    'activity_labels': [\n",
    "        'Still', 'Walking', 'Run', 'Bike', \n",
    "        'Car', 'Bus', 'Train', 'Subway'\n",
    "    ],\n",
    "    'modalities': {\n",
    "        'imu': {\n",
    "            'channels': 6,  # Accelerometer (3) + Gyroscope (3)\n",
    "            'enabled': True,  # 🔥 是否启用IMU模态\n",
    "            'preprocessing': {\n",
    "                'normalize': True,\n",
    "                'filter_freq': 20\n",
    "            }\n",
    "        },\n",
    "        'magnetometer': {\n",
    "            'channels': 3,  # Magnetometer\n",
    "            'enabled': True,  # 🔥 是否启用磁力计模态\n",
    "            'preprocessing': {\n",
    "                'normalize': True\n",
    "            }\n",
    "        },\n",
    "        'linear_acc': {\n",
    "            'channels': 3,  # Linear Acceleration\n",
    "            'enabled': False,  # 🔥 可以根据需要启用\n",
    "            'preprocessing': {\n",
    "                'normalize': True\n",
    "            }\n",
    "        },\n",
    "        'gravity': {\n",
    "            'channels': 3,  # Gravity\n",
    "            'enabled': False,  # 🔥 可以根据需要启用\n",
    "            'preprocessing': {\n",
    "                'normalize': True\n",
    "            }\n",
    "        },\n",
    "        'orientation': {\n",
    "            'channels': 3,  # Orientation\n",
    "            'enabled': False,  # 🔥 可以根据需要启用\n",
    "            'preprocessing': {\n",
    "                'normalize': True\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'train_split': 0.7,\n",
    "    'val_split': 0.15,\n",
    "    'test_split': 0.15\n",
    "}\n",
    "\n",
    "# === 🔥 快速配置预设 (取消注释使用) ===\n",
    "# 快速测试配置\n",
    "# TRAINING_CONFIG.update({\n",
    "#     'batch_size': 64,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'epochs': 20,\n",
    "#     'early_stopping_patience': 5\n",
    "# })\n",
    "\n",
    "# 精细训练配置\n",
    "# TRAINING_CONFIG.update({\n",
    "#     'batch_size': 16,\n",
    "#     'learning_rate': 0.0005,\n",
    "#     'epochs': 200,\n",
    "#     'early_stopping_patience': 20\n",
    "# })\n",
    "\n",
    "# 大模型配置\n",
    "# TRAINING_CONFIG.update({\n",
    "#     'projection_dim': 256,\n",
    "#     'num_heads': 8,\n",
    "#     'num_layers': 4,\n",
    "#     'batch_size': 16  # 减小批次大小\n",
    "# })\n",
    "\n",
    "# === 配置验证与初始化 ===\n",
    "if USE_CONFIG_MODE and ConfigBridge is not None:\n",
    "    print(\"✓ 使用配置驱动模式\")\n",
    "    print(f\"  配置文件路径: {CONFIG_PATH}\")\n",
    "    \n",
    "    # 检查配置文件是否存在\n",
    "    if not os.path.exists(CONFIG_PATH):\n",
    "        print(f\"⚠️  配置文件不存在: {CONFIG_PATH}\")\n",
    "        print(\"   创建默认SHL_Multimodal配置文件...\")\n",
    "        \n",
    "        # 创建默认SHL_Multimodal配置\n",
    "        default_shl_config = {\n",
    "            'name': 'SHL_MultiModal_Experiment',\n",
    "            'dataset': {\n",
    "                'name': 'SHL',\n",
    "                'path': SHL_DATA_PATH,\n",
    "                **shl_specific_config\n",
    "            },\n",
    "            'architecture': {\n",
    "                'experts': {\n",
    "                    'imu': {\n",
    "                        'type': 'cbranchformer',\n",
    "                        'params': {\n",
    "                            'projection_dim': TRAINING_CONFIG['projection_dim'],\n",
    "                            'num_heads': TRAINING_CONFIG['num_heads'],\n",
    "                            'num_layers': TRAINING_CONFIG['num_layers'],\n",
    "                            'patch_size': TRAINING_CONFIG['patch_size'],\n",
    "                            'time_step': TRAINING_CONFIG['time_step'],\n",
    "                            'conv_kernel_size': TRAINING_CONFIG['conv_kernel_size'],\n",
    "                            'dropout_rate': TRAINING_CONFIG['dropout_rate'],\n",
    "                            'output_dim': TRAINING_CONFIG['projection_dim']\n",
    "                        }\n",
    "                    },\n",
    "                    'magnetometer': {\n",
    "                        'type': 'lstm',\n",
    "                        'params': {\n",
    "                            'hidden_dim': 64,\n",
    "                            'num_layers': 2,\n",
    "                            'dropout_rate': TRAINING_CONFIG['dropout_rate'],\n",
    "                            'bidirectional': True,\n",
    "                            'output_dim': 64\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                'fusion': {\n",
    "                    'strategy': 'concatenate',\n",
    "                    'params': {}\n",
    "                },\n",
    "                'fusion_output_dim': TRAINING_CONFIG['projection_dim'] + 64,\n",
    "                'dropout_rate': TRAINING_CONFIG['dropout_rate']\n",
    "            },\n",
    "            'training': {\n",
    "                'batch_size': TRAINING_CONFIG['batch_size'],\n",
    "                'learning_rate': TRAINING_CONFIG['learning_rate'],\n",
    "                'epochs': TRAINING_CONFIG['epochs'],\n",
    "                'optimizer': 'adam',\n",
    "                'scheduler': 'cosine',\n",
    "                'weight_decay': TRAINING_CONFIG['weight_decay'],\n",
    "                'label_smoothing': TRAINING_CONFIG['label_smoothing'],\n",
    "                'gradient_clip_norm': TRAINING_CONFIG['gradient_clip_norm'],\n",
    "                'early_stopping_patience': TRAINING_CONFIG['early_stopping_patience']\n",
    "            },\n",
    "            'device': traditional_config['device'],\n",
    "            'seed': traditional_config['random_seed'],\n",
    "            'output_dir': traditional_config['output_dir'],\n",
    "            'save_checkpoints': True,\n",
    "            'verbose': True\n",
    "        }\n",
    "        \n",
    "        # 确保目录存在\n",
    "        os.makedirs(os.path.dirname(CONFIG_PATH), exist_ok=True)\n",
    "        \n",
    "        # 保存配置文件\n",
    "        with open(CONFIG_PATH, 'w', encoding='utf-8') as f:\n",
    "            yaml.dump(default_shl_config, f, default_flow_style=False, allow_unicode=True)\n",
    "        \n",
    "        print(f\"✓ 已创建默认配置文件: {CONFIG_PATH}\")\n",
    "    \n",
    "    # 加载并验证配置\n",
    "    try:\n",
    "        with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "            loaded_config = yaml.safe_load(f)\n",
    "        \n",
    "        # 验证必要的配置项\n",
    "        required_sections = ['dataset', 'training', 'device']\n",
    "        for section in required_sections:\n",
    "            if section not in loaded_config:\n",
    "                raise ValueError(f\"配置文件缺少必要部分: {section}\")\n",
    "        \n",
    "        print(\"✓ 配置文件验证通过\")\n",
    "        print(f\"  数据集: {loaded_config['dataset']['name']}\")\n",
    "        print(f\"  数据路径: {loaded_config['dataset']['path']}\")\n",
    "        print(f\"  批次大小: {loaded_config['training']['batch_size']}\")\n",
    "        print(f\"  学习率: {loaded_config['training']['learning_rate']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 配置文件加载失败: {e}\")\n",
    "        print(\"   切换到传统模式...\")\n",
    "        USE_CONFIG_MODE = False\n",
    "\n",
    "else:\n",
    "    print(\"✓ 使用传统模式\")\n",
    "    if not USE_CONFIG_MODE:\n",
    "        print(\"  原因: 手动设置为传统模式\")\n",
    "    elif ConfigBridge is None:\n",
    "        print(\"  原因: ConfigBridge模块未可用\")\n",
    "    \n",
    "    # 更新传统配置\n",
    "    traditional_config.update(TRAINING_CONFIG)\n",
    "\n",
    "# === 验证数据文件存在性 ===\n",
    "print(f\"\\n验证SHL_Multimodal数据文件...\")\n",
    "clients_data_path = os.path.join(SHL_DATA_PATH, 'clientsData.hkl')\n",
    "clients_label_path = os.path.join(SHL_DATA_PATH, 'clientsLabel.hkl')\n",
    "\n",
    "if os.path.exists(clients_data_path) and os.path.exists(clients_label_path):\n",
    "    print(f\"✓ 数据文件存在:\")\n",
    "    print(f\"  数据文件: {clients_data_path}\")\n",
    "    print(f\"  标签文件: {clients_label_path}\")\n",
    "else:\n",
    "    print(f\"❌ 数据文件不存在:\")\n",
    "    print(f\"  数据文件: {clients_data_path} ({'存在' if os.path.exists(clients_data_path) else '不存在'})\")\n",
    "    print(f\"  标签文件: {clients_label_path} ({'存在' if os.path.exists(clients_label_path) else '不存在'})\")\n",
    "    print(f\"  请确保SHL_Multimodal数据已正确处理并保存\")\n",
    "\n",
    "# === 设置全局变量 ===\n",
    "dataset_name = traditional_config['dataset_name']\n",
    "batch_size = traditional_config['batch_size']\n",
    "learning_rate = traditional_config['learning_rate']\n",
    "epochs = traditional_config['epochs']\n",
    "random_seed = traditional_config['random_seed']\n",
    "position_device = traditional_config['position_device']\n",
    "main_dir = traditional_config['main_dir']\n",
    "\n",
    "# === 最终配置摘要 ===\n",
    "print(f\"\"\"\n",
    "========== 🔥 配置摘要 ==========\n",
    "模式: {'配置驱动' if USE_CONFIG_MODE else '传统模式'}\n",
    "数据集: {dataset_name}\n",
    "数据路径: {SHL_DATA_PATH}\n",
    "训练参数:\n",
    "  - 批次大小: {traditional_config['batch_size']}\n",
    "  - 学习率: {traditional_config['learning_rate']}\n",
    "  - 训练轮数: {traditional_config['epochs']}\n",
    "  - 投影维度: {traditional_config['projection_dim']}\n",
    "  - 注意力头数: {traditional_config['num_heads']}\n",
    "  - 网络层数: {traditional_config['num_layers']}\n",
    "输出目录: {traditional_config['output_dir']}\n",
    "================================\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 数据加载 - SHL_Multimodal数据集支持 ==========\n",
    "\n",
    "print(\"开始加载SHL_Multimodal数据集...\")\n",
    "\n",
    "# === SHL_Multimodal数据集加载函数 ===\n",
    "def load_shl_multimodal_dataset(data_path: str, config: Dict[str, Any]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    加载SHL_Multimodal数据集\n",
    "    \n",
    "    Args:\n",
    "        data_path: 数据集路径 (应该是包含clientsData.hkl和clientsLabel.hkl的目录)\n",
    "        config: 数据配置\n",
    "    \n",
    "    Returns:\n",
    "        (combined_data, combined_labels)\n",
    "    \"\"\"\n",
    "    import hickle as hkl\n",
    "    \n",
    "    # SHL_Multimodal数据集文件路径\n",
    "    clients_data_path = os.path.join(data_path, 'clientsData.hkl')\n",
    "    clients_label_path = os.path.join(data_path, 'clientsLabel.hkl')\n",
    "    \n",
    "    # 检查数据文件是否存在\n",
    "    if not os.path.exists(clients_data_path) or not os.path.exists(clients_label_path):\n",
    "        print(f\"❌ SHL_Multimodal数据文件不存在:\")\n",
    "        print(f\"   数据文件: {clients_data_path} ({'存在' if os.path.exists(clients_data_path) else '不存在'})\")\n",
    "        print(f\"   标签文件: {clients_label_path} ({'存在' if os.path.exists(clients_label_path) else '不存在'})\")\n",
    "        print(f\"   请确保已正确运行DATA_SHL_NEW.ipynb生成数据\")\n",
    "        raise FileNotFoundError(\"SHL_Multimodal数据文件不存在\")\n",
    "    \n",
    "    try:\n",
    "        # 加载数据\n",
    "        print(f\"正在加载SHL_Multimodal数据文件...\")\n",
    "        print(f\"  从路径: {data_path}\")\n",
    "        \n",
    "        clients_data = hkl.load(clients_data_path)\n",
    "        clients_labels = hkl.load(clients_label_path)\n",
    "        \n",
    "        print(f\"✓ 成功加载SHL_Multimodal数据\")\n",
    "        print(f\"  客户端/流数量: {len(clients_data)}\")\n",
    "        print(f\"  标签数量: {len(clients_labels)}\")\n",
    "        \n",
    "        # 检查数据结构\n",
    "        if len(clients_data) != len(clients_labels):\n",
    "            raise ValueError(f\"数据和标签数量不匹配: {len(clients_data)} vs {len(clients_labels)}\")\n",
    "        \n",
    "        # 合并所有客户端/流数据\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for i, (client_data, client_label) in enumerate(zip(clients_data, clients_labels)):\n",
    "            if client_data is not None and len(client_data) > 0:\n",
    "                # 确保数据是numpy数组\n",
    "                if not isinstance(client_data, np.ndarray):\n",
    "                    client_data = np.array(client_data)\n",
    "                if not isinstance(client_label, np.ndarray):\n",
    "                    client_label = np.array(client_label)\n",
    "                \n",
    "                all_data.append(client_data)\n",
    "                all_labels.append(client_label)\n",
    "                print(f\"  客户端/流 {i}: {client_data.shape} (样本 x 特征)\")\n",
    "                print(f\"    标签范围: {np.min(client_label)} - {np.max(client_label)}\")\n",
    "            else:\n",
    "                print(f\"  客户端/流 {i}: 跳过 (数据为空)\")\n",
    "        \n",
    "        if not all_data:\n",
    "            raise ValueError(\"没有有效的数据可加载\")\n",
    "        \n",
    "        # 合并数据\n",
    "        print(f\"合并 {len(all_data)} 个客户端/流的数据...\")\n",
    "        combined_data = np.vstack(all_data)\n",
    "        combined_labels = np.hstack(all_labels)\n",
    "        \n",
    "        print(f\"✓ 数据合并完成\")\n",
    "        print(f\"  总样本数: {combined_data.shape[0]:,}\")\n",
    "        print(f\"  特征维度: {combined_data.shape[1]}\")\n",
    "        print(f\"  数据类型: {combined_data.dtype}\")\n",
    "        print(f\"  数据范围: [{np.min(combined_data):.3f}, {np.max(combined_data):.3f}]\")\n",
    "        print(f\"  标签范围: {np.min(combined_labels)} - {np.max(combined_labels)}\")\n",
    "        print(f\"  唯一标签: {sorted(np.unique(combined_labels))}\")\n",
    "        \n",
    "        # 检查数据质量\n",
    "        if np.any(np.isnan(combined_data)):\n",
    "            nan_count = np.sum(np.isnan(combined_data))\n",
    "            print(f\"⚠️  发现 {nan_count} 个NaN值，将进行处理\")\n",
    "            combined_data = np.nan_to_num(combined_data, nan=0.0)\n",
    "        \n",
    "        if np.any(np.isinf(combined_data)):\n",
    "            inf_count = np.sum(np.isinf(combined_data))\n",
    "            print(f\"⚠️  发现 {inf_count} 个无穷值，将进行处理\")\n",
    "            combined_data = np.nan_to_num(combined_data, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        return combined_data, combined_labels\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 加载SHL_Multimodal数据时出错: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def split_shl_data(data: np.ndarray, labels: np.ndarray, \n",
    "                   train_ratio: float = 0.7, val_ratio: float = 0.15,\n",
    "                   random_seed: int = 42) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    分割SHL_Multimodal数据集\n",
    "    \n",
    "    Args:\n",
    "        data: 输入数据\n",
    "        labels: 标签\n",
    "        train_ratio: 训练集比例\n",
    "        val_ratio: 验证集比例\n",
    "        random_seed: 随机种子\n",
    "    \n",
    "    Returns:\n",
    "        (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # 设置随机种子\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    print(f\"分割数据集 (train:{train_ratio:.1%}, val:{val_ratio:.1%}, test:{1-train_ratio-val_ratio:.1%})...\")\n",
    "    \n",
    "    # 检查是否有足够的每个类别的样本进行分层抽样\n",
    "    unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "    min_count = np.min(label_counts)\n",
    "    \n",
    "    print(f\"标签分布:\")\n",
    "    activity_labels = shl_specific_config.get('activity_labels', [f'Class_{i}' for i in unique_labels])\n",
    "    for label, count in zip(unique_labels, label_counts):\n",
    "        label_name = activity_labels[label] if label < len(activity_labels) else f'Class_{label}'\n",
    "        print(f\"  {label_name}: {count:,} 样本\")\n",
    "    \n",
    "    # 如果最少的类别样本数小于3，则不使用分层抽样\n",
    "    use_stratify = min_count >= 3\n",
    "    if not use_stratify:\n",
    "        print(f\"⚠️  最少类别样本数为 {min_count}，不使用分层抽样\")\n",
    "    \n",
    "    try:\n",
    "        # 第一次分割: 分离训练集和临时集(验证+测试)\n",
    "        stratify_param = labels if use_stratify else None\n",
    "        train_data, temp_data, train_labels, temp_labels = train_test_split(\n",
    "            data, labels, test_size=(1-train_ratio), \n",
    "            random_state=random_seed, stratify=stratify_param\n",
    "        )\n",
    "        \n",
    "        # 第二次分割: 从临时集中分离验证集和测试集\n",
    "        val_ratio_adjusted = val_ratio / (1 - train_ratio)  # 调整验证集在临时集中的比例\n",
    "        stratify_param_temp = temp_labels if use_stratify else None\n",
    "        val_data, test_data, val_labels, test_labels = train_test_split(\n",
    "            temp_data, temp_labels, test_size=(1-val_ratio_adjusted), \n",
    "            random_state=random_seed, stratify=stratify_param_temp\n",
    "        )\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"⚠️  分层抽样失败 ({e})，使用随机抽样\")\n",
    "        # 降级到随机抽样\n",
    "        train_data, temp_data, train_labels, temp_labels = train_test_split(\n",
    "            data, labels, test_size=(1-train_ratio), random_state=random_seed\n",
    "        )\n",
    "        val_ratio_adjusted = val_ratio / (1 - train_ratio)\n",
    "        val_data, test_data, val_labels, test_labels = train_test_split(\n",
    "            temp_data, temp_labels, test_size=(1-val_ratio_adjusted), random_state=random_seed\n",
    "        )\n",
    "    \n",
    "    print(f\"✓ 数据分割完成:\")\n",
    "    print(f\"  训练集: {train_data.shape[0]:,} 样本 ({train_data.shape[0]/data.shape[0]*100:.1f}%)\")\n",
    "    print(f\"  验证集: {val_data.shape[0]:,} 样本 ({val_data.shape[0]/data.shape[0]*100:.1f}%)\")\n",
    "    print(f\"  测试集: {test_data.shape[0]:,} 样本 ({test_data.shape[0]/data.shape[0]*100:.1f}%)\")\n",
    "    \n",
    "    return train_data, train_labels, val_data, val_labels, test_data, test_labels\n",
    "\n",
    "# === 根据模式加载数据 ===\n",
    "try:\n",
    "    if USE_CONFIG_MODE and ConfigBridge is not None:\n",
    "        # 配置驱动模式\n",
    "        print(\"使用配置驱动模式加载数据...\")\n",
    "        \n",
    "        # 创建训练器实例并加载数据\n",
    "        trainer = ConfigurableTrainer(CONFIG_PATH)\n",
    "        # 注意: 实际的数据加载逻辑需要在这里实现\n",
    "        print(\"⚠️  配置驱动模式数据加载需要进一步实现\")\n",
    "        USE_CONFIG_MODE = False  # 临时切换到传统模式\n",
    "        \n",
    "    if not USE_CONFIG_MODE:\n",
    "        # 传统模式\n",
    "        print(\"使用传统模式加载数据...\")\n",
    "        \n",
    "        # 加载SHL_Multimodal数据集\n",
    "        all_data, all_labels = load_shl_multimodal_dataset(SHL_DATA_PATH, traditional_config)\n",
    "        \n",
    "        # 分割数据集\n",
    "        train_split = shl_specific_config.get('train_split', 0.7)\n",
    "        val_split = shl_specific_config.get('val_split', 0.15)\n",
    "        random_seed = traditional_config.get('random_seed', 42)\n",
    "        \n",
    "        (central_train_data, central_train_label, \n",
    "         central_dev_data, central_dev_label, \n",
    "         central_test_data, central_test_label) = split_shl_data(\n",
    "            all_data, all_labels, train_split, val_split, random_seed\n",
    "        )\n",
    "        \n",
    "        print(\"✓ 传统模式数据加载完成\")\n",
    "    \n",
    "    # === 数据后处理和验证 ===\n",
    "    print(f\"\\n数据加载摘要:\")\n",
    "    print(f\"训练集形状: {central_train_data.shape}\")\n",
    "    print(f\"验证集形状: {central_dev_data.shape}\")  \n",
    "    print(f\"测试集形状: {central_test_data.shape}\")\n",
    "    print(f\"训练标签范围: {np.min(central_train_label)} - {np.max(central_train_label)}\")\n",
    "    print(f\"唯一标签数量: {len(np.unique(central_train_label))}\")\n",
    "    \n",
    "    # 活动标签映射 (SHL数据集)\n",
    "    ACTIVITY_LABEL = shl_specific_config['activity_labels']\n",
    "    activity_count = len(ACTIVITY_LABEL)\n",
    "    \n",
    "    print(f\"\\n活动类别 ({activity_count}个):\")\n",
    "    for i, activity in enumerate(ACTIVITY_LABEL):\n",
    "        train_count = np.sum(central_train_label == i)\n",
    "        val_count = np.sum(central_dev_label == i)\n",
    "        test_count = np.sum(central_test_label == i)\n",
    "        total_count = train_count + val_count + test_count\n",
    "        print(f\"  {i}: {activity}\")\n",
    "        print(f\"     训练: {train_count:,}, 验证: {val_count:,}, 测试: {test_count:,}, 总计: {total_count:,}\")\n",
    "    \n",
    "    # 验证数据完整性\n",
    "    print(f\"\\n验证数据完整性...\")\n",
    "    assert central_train_data.shape[0] == central_train_label.shape[0], \"训练数据和标签数量不匹配\"\n",
    "    assert central_dev_data.shape[0] == central_dev_label.shape[0], \"验证数据和标签数量不匹配\"\n",
    "    assert central_test_data.shape[0] == central_test_label.shape[0], \"测试数据和标签数量不匹配\"\n",
    "    \n",
    "    # 检查标签范围\n",
    "    all_labels_combined = np.concatenate([central_train_label, central_dev_label, central_test_label])\n",
    "    unique_labels = np.unique(all_labels_combined)\n",
    "    assert np.min(unique_labels) >= 0, \"标签包含负值\"\n",
    "    assert np.max(unique_labels) < activity_count, f\"标签超出范围 (最大: {np.max(unique_labels)}, 期望 < {activity_count})\"\n",
    "    \n",
    "    # 检查数据维度一致性\n",
    "    expected_features = central_train_data.shape[1]\n",
    "    assert central_dev_data.shape[1] == expected_features, f\"验证集特征维度不匹配: {central_dev_data.shape[1]} vs {expected_features}\"\n",
    "    assert central_test_data.shape[1] == expected_features, f\"测试集特征维度不匹配: {central_test_data.shape[1]} vs {expected_features}\"\n",
    "    \n",
    "    print(\"✓ 数据完整性验证通过\")\n",
    "    \n",
    "    # 数据统计信息\n",
    "    print(f\"\\n数据统计信息:\")\n",
    "    print(f\"  特征维度: {expected_features}\")\n",
    "    print(f\"  数据类型: {central_train_data.dtype}\")\n",
    "    print(f\"  训练数据范围: [{np.min(central_train_data):.3f}, {np.max(central_train_data):.3f}]\")\n",
    "    print(f\"  训练数据均值: {np.mean(central_train_data):.3f}\")\n",
    "    print(f\"  训练数据标准差: {np.std(central_train_data):.3f}\")\n",
    "    \n",
    "    # 设置兼容性变量 (与原始代码保持一致)\n",
    "    client_orientation_train = None\n",
    "    client_orientation_test = None  \n",
    "    orientations_names = None\n",
    "    position_device = traditional_config.get('position_device', '')\n",
    "    \n",
    "    print(\"✓ SHL_Multimodal数据集加载完成!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 数据加载失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # 如果数据加载失败，生成示例数据以便测试\n",
    "    print(f\"\\n⚠️  数据加载失败，生成示例数据以便测试...\")\n",
    "    print(f\"请检查以下问题:\")\n",
    "    print(f\"1. 数据路径是否正确: {SHL_DATA_PATH}\")\n",
    "    print(f\"2. 是否存在 clientsData.hkl 和 clientsLabel.hkl 文件\")\n",
    "    print(f\"3. 是否已运行 DATA_SHL_NEW.ipynb 生成数据\")\n",
    "    \n",
    "    # 生成示例SHL_Multimodal数据\n",
    "    n_train = 1000\n",
    "    n_val = 200\n",
    "    n_test = 300\n",
    "    \n",
    "    # 根据SHL_Multimodal的实际特征维度设置\n",
    "    # 假设包含多个传感器模态的特征\n",
    "    n_features = 128 * 18  # 128个时间步 * 18个传感器通道 (Acc(3) + Gyr(3) + Mag(3) + LAcc(3) + Gra(3) + Ori(3))\n",
    "    n_classes = len(shl_specific_config['activity_labels'])\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(f\"生成示例数据:\")\n",
    "    print(f\"  特征维度: {n_features} (假设 128时间步 × 18传感器通道)\")\n",
    "    print(f\"  类别数: {n_classes}\")\n",
    "    \n",
    "    central_train_data = np.random.randn(n_train, n_features).astype(np.float32)\n",
    "    central_train_label = np.random.randint(0, n_classes, n_train)\n",
    "    \n",
    "    central_dev_data = np.random.randn(n_val, n_features).astype(np.float32)\n",
    "    central_dev_label = np.random.randint(0, n_classes, n_val)\n",
    "    \n",
    "    central_test_data = np.random.randn(n_test, n_features).astype(np.float32)\n",
    "    central_test_label = np.random.randint(0, n_classes, n_test)\n",
    "    \n",
    "    # 设置活动标签和计数\n",
    "    ACTIVITY_LABEL = shl_specific_config['activity_labels']\n",
    "    activity_count = len(ACTIVITY_LABEL)\n",
    "    \n",
    "    # 兼容性变量\n",
    "    client_orientation_train = None\n",
    "    client_orientation_test = None\n",
    "    orientations_names = None\n",
    "    position_device = ''\n",
    "    \n",
    "    print(f\"✓ 示例数据生成完成:\")\n",
    "    print(f\"  训练集: {central_train_data.shape}\")\n",
    "    print(f\"  验证集: {central_dev_data.shape}\")\n",
    "    print(f\"  测试集: {central_test_data.shape}\")\n",
    "    print(f\"  类别数: {activity_count}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"数据加载阶段完成\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# === 数据加载后的额外检查 ===\n",
    "print(f\"\\n最终数据检查:\")\n",
    "print(f\"数据集类型: SHL_Multimodal\")\n",
    "print(f\"数据来源: {SHL_DATA_PATH}\")\n",
    "print(f\"训练样本: {central_train_data.shape[0]:,}\")\n",
    "print(f\"验证样本: {central_dev_data.shape[0]:,}\")\n",
    "print(f\"测试样本: {central_test_data.shape[0]:,}\")\n",
    "print(f\"特征维度: {central_train_data.shape[1]:,}\")\n",
    "print(f\"活动类别: {activity_count}个\")\n",
    "print(f\"数据类型: {central_train_data.dtype}\")\n",
    "\n",
    "# 检查内存使用\n",
    "train_size_mb = central_train_data.nbytes / (1024 * 1024)\n",
    "total_size_mb = (central_train_data.nbytes + central_dev_data.nbytes + central_test_data.nbytes) / (1024 * 1024)\n",
    "print(f\"内存使用: 训练集 {train_size_mb:.1f}MB, 总计 {total_size_mb:.1f}MB\")\n",
    "\n",
    "if total_size_mb > 1000:  # 超过1GB时警告\n",
    "    print(f\"⚠️  数据集较大 ({total_size_mb:.1f}MB)，建议使用较小的批次大小\")\n",
    "    if traditional_config['batch_size'] > 32:\n",
    "        traditional_config['batch_size'] = 32\n",
    "        print(f\"   自动调整批次大小为: {traditional_config['batch_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 数据预处理和数据加载器创建 ==========\n",
    "\n",
    "print(\"开始训练准备...\")\n",
    "\n",
    "try:\n",
    "    # 如果在RealWorld或HHAR上使用指定位置/设备，我们移除一个并将其用作测试集，并将其他用于训练\n",
    "    # SHL数据集不需要此步骤，但保留兼容性\n",
    "    if position_device != '' or dataset_name == 'UCI':\n",
    "        print(f\"注意: SHL数据集不支持位置设备分割 (position_device='{position_device}')\")\n",
    "        print(\"使用标准数据分割...\")\n",
    "    \n",
    "    # 计算类权重 (处理不平衡数据集)\n",
    "    print(\"计算类权重...\")\n",
    "    from sklearn.utils import class_weight\n",
    "    \n",
    "    temp_weights = class_weight.compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(central_train_label),\n",
    "        y=central_train_label.ravel()\n",
    "    )\n",
    "    class_weights = {j: temp_weights[j] for j in range(len(temp_weights))}\n",
    "    \n",
    "    print(\"✓ 类权重计算完成:\")\n",
    "    for i, weight in class_weights.items():\n",
    "        activity_name = ACTIVITY_LABEL[i] if i < len(ACTIVITY_LABEL) else f\"类别{i}\"\n",
    "        print(f\"  {activity_name}: {weight:.3f}\")\n",
    "\n",
    "    # 创建PyTorch数据集和数据加载器\n",
    "    print(\"创建PyTorch数据集...\")\n",
    "    \n",
    "    # 创建特征和标签的张量\n",
    "    train_features = torch.FloatTensor(central_train_data)\n",
    "    train_labels = torch.LongTensor(central_train_label)\n",
    "    dev_features = torch.FloatTensor(central_dev_data)\n",
    "    dev_labels = torch.LongTensor(central_dev_label)\n",
    "    test_features = torch.FloatTensor(central_test_data)\n",
    "    test_labels = torch.LongTensor(central_test_label)\n",
    "\n",
    "    # 创建数据集\n",
    "    train_dataset = HARDataset(train_features, train_labels)\n",
    "    dev_dataset = HARDataset(dev_features, dev_labels)\n",
    "    test_dataset = HARDataset(test_features, test_labels)\n",
    "\n",
    "    # 获取批次大小\n",
    "    batch_size = traditional_config.get('batch_size', 32)\n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, \n",
    "        num_workers=0, pin_memory=False, drop_last=False\n",
    "    )\n",
    "    dev_loader = torch.utils.data.DataLoader(\n",
    "        dev_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=0, pin_memory=False, drop_last=False\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=0, pin_memory=False, drop_last=False\n",
    "    )\n",
    "\n",
    "    print(f\"✓ 数据加载器创建完成:\")\n",
    "    print(f\"  训练批次数: {len(train_loader)}\")\n",
    "    print(f\"  验证批次数: {len(dev_loader)}\")\n",
    "    print(f\"  测试批次数: {len(test_loader)}\")\n",
    "    print(f\"  批次大小: {batch_size}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 数据预处理失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(\"✓ 数据预处理阶段完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 模型创建和训练设置 ==========\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"开始模型训练\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "try:\n",
    "    # === 传统模式训练 ===\n",
    "    print(\"使用传统训练模式...\")\n",
    "    \n",
    "    # 获取训练参数\n",
    "    learning_rate = traditional_config.get('learning_rate', 0.001)\n",
    "    epochs = traditional_config.get('epochs', 100)\n",
    "    weight_decay = traditional_config.get('weight_decay', 0.0001)\n",
    "    random_seed = traditional_config.get('random_seed', 42)\n",
    "    \n",
    "    # 设置设备\n",
    "    device_config = traditional_config.get('device', 'auto')\n",
    "    if device_config == 'auto':\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "            print(f\"使用 CUDA 设备: {torch.cuda.get_device_name()}\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = torch.device('mps')\n",
    "            print(\"使用 MPS 设备\")\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "            print(\"使用 CPU 设备\")\n",
    "    else:\n",
    "        device = torch.device(device_config)\n",
    "        print(f\"使用指定设备: {device}\")\n",
    "    \n",
    "    # 创建模型\n",
    "    input_dim = central_train_data.shape[1]  # 特征维度\n",
    "    num_classes = len(np.unique(central_train_label))  # 类别数量\n",
    "    \n",
    "    print(f\"创建模型:\")\n",
    "    print(f\"  输入维度: {input_dim}\")\n",
    "    print(f\"  输出类别数: {num_classes}\")\n",
    "    \n",
    "    # 创建模型实例\n",
    "    model_config = {\n",
    "        'input_dim': input_dim,\n",
    "        'num_classes': num_classes,\n",
    "        'projection_dim': traditional_config.get('projection_dim', 192),\n",
    "        'num_heads': traditional_config.get('num_heads', 4),\n",
    "        'num_layers': traditional_config.get('num_layers', 3),\n",
    "        'patch_size': traditional_config.get('patch_size', 16),\n",
    "        'time_step': traditional_config.get('time_step', 16),\n",
    "        'conv_kernel_size': traditional_config.get('conv_kernel_size', 15),\n",
    "        'dropout_rate': traditional_config.get('dropout_rate', 0.1)\n",
    "    }\n",
    "    \n",
    "    # 使用CBranchformer模型\n",
    "    har_model = model.CBranchformerHAR(\n",
    "        input_dim=model_config['input_dim'],\n",
    "        num_classes=model_config['num_classes'],\n",
    "        projection_dim=model_config['projection_dim'],\n",
    "        num_heads=model_config['num_heads'],\n",
    "        num_layers=model_config['num_layers'],\n",
    "        patch_size=model_config['patch_size'],\n",
    "        time_step=model_config['time_step'],\n",
    "        conv_kernel_size=model_config['conv_kernel_size'],\n",
    "        dropout_rate=model_config['dropout_rate']\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"✓ 模型创建完成\")\n",
    "    print(f\"  模型参数量: {sum(p.numel() for p in har_model.parameters()):,}\")\n",
    "    \n",
    "    # 创建优化器和损失函数\n",
    "    optimizer = torch.optim.Adam(\n",
    "        har_model.parameters(), \n",
    "        lr=learning_rate, \n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # 使用加权交叉熵损失处理类别不平衡\n",
    "    class_weights_tensor = torch.FloatTensor([class_weights[i] for i in range(num_classes)]).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    \n",
    "    # 学习率调度器\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    print(f\"✓ 优化器和损失函数设置完成\")\n",
    "    print(f\"  优化器: Adam (lr={learning_rate}, weight_decay={weight_decay})\")\n",
    "    print(f\"  损失函数: 加权交叉熵\")\n",
    "    print(f\"  学习率调度: Cosine Annealing\")\n",
    "    \n",
    "    # 设置输出目录\n",
    "    output_dir = Path(traditional_config.get('output_dir', './results/shl_multimodal'))\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"  输出目录: {output_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 模型创建失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(\"✓ 模型创建和训练设置完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 训练循环执行 ==========\n",
    "\n",
    "print(f\"\\n开始训练 (共{epochs}个epoch)...\")\n",
    "\n",
    "try:\n",
    "    # 训练参数\n",
    "    best_val_acc = 0.0\n",
    "    best_val_f1 = 0.0\n",
    "    patience = traditional_config.get('early_stopping_patience', 10)\n",
    "    patience_counter = 0\n",
    "    gradient_clip_norm = traditional_config.get('gradient_clip_norm', 1.0)\n",
    "    \n",
    "    # 记录训练历史\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    val_f1s = []\n",
    "    \n",
    "    print(f\"训练配置:\")\n",
    "    print(f\"  设备: {device}\")\n",
    "    print(f\"  批次大小: {batch_size}\")\n",
    "    print(f\"  学习率: {learning_rate}\")\n",
    "    print(f\"  训练轮数: {epochs}\")\n",
    "    print(f\"  早停耐心: {patience}\")\n",
    "    print(f\"  梯度裁剪: {gradient_clip_norm}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # === 训练阶段 ===\n",
    "        har_model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = har_model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            if gradient_clip_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(har_model.parameters(), gradient_clip_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            # 打印批次进度 (每50个批次)\n",
    "            if batch_idx % 50 == 0 and batch_idx > 0:\n",
    "                current_acc = 100.0 * train_correct / train_total\n",
    "                print(f\"  Epoch [{epoch+1}/{epochs}] Batch [{batch_idx}/{len(train_loader)}] \"\n",
    "                      f\"Loss: {loss.item():.4f} Acc: {current_acc:.2f}%\")\n",
    "        \n",
    "        # === 验证阶段 ===\n",
    "        har_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, targets in dev_loader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                outputs = har_model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        # 计算指标\n",
    "        train_acc = 100.0 * train_correct / train_total\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "        val_f1 = f1_score(all_targets, all_preds, average='weighted') * 100\n",
    "        \n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss / len(dev_loader))\n",
    "        val_accs.append(val_acc)\n",
    "        val_f1s.append(val_f1)\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # 计算epoch时间\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # 打印训练信息\n",
    "        print(f\"Epoch [{epoch+1:3d}/{epochs}] \"\n",
    "              f\"Train Loss: {train_losses[-1]:.4f} \"\n",
    "              f\"Train Acc: {train_acc:.2f}% \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f} \"\n",
    "              f\"Val Acc: {val_acc:.2f}% \"\n",
    "              f\"Val F1: {val_f1:.2f}% \"\n",
    "              f\"LR: {current_lr:.2e} \"\n",
    "              f\"Time: {epoch_time:.1f}s\")\n",
    "        \n",
    "        # 早停检查和模型保存\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # 保存最佳模型\n",
    "            if traditional_config.get('save_checkpoints', True):\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': har_model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'best_val_acc': best_val_acc,\n",
    "                    'best_val_f1': best_val_f1,\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_losses': val_losses,\n",
    "                    'val_accs': val_accs,\n",
    "                    'model_config': model_config,\n",
    "                    'traditional_config': traditional_config\n",
    "                }, output_dir / 'best_model.pth')\n",
    "            \n",
    "            print(f\"    ✓ 新的最佳验证准确率: {best_val_acc:.2f}% (F1: {best_val_f1:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\n早停触发! 验证准确率在{patience}个epoch内未改善\")\n",
    "                print(f\"最佳验证准确率: {best_val_acc:.2f}%\")\n",
    "                print(f\"最佳验证F1分数: {best_val_f1:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        # 每10个epoch保存一次检查点\n",
    "        if (epoch + 1) % 10 == 0 and traditional_config.get('save_checkpoints', True):\n",
    "            checkpoint_path = output_dir / f'checkpoint_epoch_{epoch+1}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': har_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'val_accs': val_accs\n",
    "            }, checkpoint_path)\n",
    "    \n",
    "    print(f\"\\n✓ 训练完成!\")\n",
    "    print(f\"  最佳验证准确率: {best_val_acc:.2f}%\")\n",
    "    print(f\"  最佳验证F1分数: {best_val_f1:.2f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 训练执行失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(\"✓ 训练循环执行完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 测试评估 ==========\n",
    "\n",
    "print(f\"\\n开始测试最佳模型...\")\n",
    "\n",
    "try:\n",
    "    # 加载最佳模型\n",
    "    if traditional_config.get('save_checkpoints', True):\n",
    "        checkpoint_path = output_dir / 'best_model.pth'\n",
    "        if checkpoint_path.exists():\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            har_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(\"✓ 已加载最佳模型权重\")\n",
    "        else:\n",
    "            print(\"⚠️  最佳模型文件不存在，使用当前模型\")\n",
    "    \n",
    "    # 测试评估\n",
    "    har_model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    all_test_preds = []\n",
    "    all_test_targets = []\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = har_model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            test_total += targets.size(0)\n",
    "            test_correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            all_test_preds.extend(predicted.cpu().numpy())\n",
    "            all_test_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    test_acc = 100.0 * test_correct / test_total\n",
    "    test_f1 = f1_score(all_test_targets, all_test_preds, average='weighted') * 100\n",
    "    test_loss_avg = test_loss / len(test_loader)\n",
    "    \n",
    "    print(f\"✓ 测试完成!\")\n",
    "    print(f\"  测试损失: {test_loss_avg:.4f}\")\n",
    "    print(f\"  测试准确率: {test_acc:.2f}%\")\n",
    "    print(f\"  测试F1分数: {test_f1:.2f}%\")\n",
    "    \n",
    "    # 生成详细分类报告\n",
    "    print(f\"\\n详细分类报告:\")\n",
    "    class_names = [ACTIVITY_LABEL[i] if i < len(ACTIVITY_LABEL) else f\"类别{i}\" \n",
    "                  for i in range(num_classes)]\n",
    "    report = classification_report(\n",
    "        all_test_targets, all_test_preds, \n",
    "        target_names=class_names, \n",
    "        digits=3\n",
    "    )\n",
    "    print(report)\n",
    "    \n",
    "    # 生成混淆矩阵\n",
    "    print(f\"\\n混淆矩阵:\")\n",
    "    cm = confusion_matrix(all_test_targets, all_test_preds)\n",
    "    print(cm)\n",
    "    \n",
    "    # 计算每个类别的准确率\n",
    "    print(f\"\\n各类别准确率:\")\n",
    "    for i in range(num_classes):\n",
    "        class_mask = np.array(all_test_targets) == i\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_acc = np.mean(np.array(all_test_preds)[class_mask] == i) * 100\n",
    "            activity_name = ACTIVITY_LABEL[i] if i < len(ACTIVITY_LABEL) else f\"类别{i}\"\n",
    "            print(f\"  {activity_name}: {class_acc:.2f}% ({np.sum(class_mask)} 样本)\")\n",
    "    \n",
    "    # 保存结果\n",
    "    results = {\n",
    "        'dataset': dataset_name,\n",
    "        'model_config': model_config,\n",
    "        'training_config': traditional_config,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'test_loss': test_loss_avg,\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'training_history': {\n",
    "            'train_losses': train_losses,\n",
    "            'train_accs': train_accs,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accs': val_accs,\n",
    "            'val_f1s': val_f1s\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 保存结果到JSON文件\n",
    "    with open(output_dir / 'training_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump({k: v for k, v in results.items() if k != 'confusion_matrix'}, \n",
    "                 f, indent=2, default=str, ensure_ascii=False)\n",
    "    \n",
    "    # 保存分类报告\n",
    "    with open(output_dir / 'classification_report.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    # 保存混淆矩阵\n",
    "    np.save(output_dir / 'confusion_matrix.npy', cm)\n",
    "    \n",
    "    print(f\"✓ 结果已保存到: {output_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 测试评估失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(\"✓ 测试评估完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 训练结果可视化 ==========\n",
    "\n",
    "print(\"生成训练结果可视化...\")\n",
    "\n",
    "try:\n",
    "    # 设置matplotlib参数\n",
    "    plt.rcParams['figure.figsize'] = (15, 10)\n",
    "    plt.rcParams['font.size'] = 12\n",
    "    \n",
    "    # 创建子图\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'SHL数据集训练结果 - {dataset_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. 训练和验证损失\n",
    "    axes[0, 0].plot(train_losses, label='训练损失', color='blue', linewidth=2)\n",
    "    axes[0, 0].plot(val_losses, label='验证损失', color='red', linewidth=2)\n",
    "    axes[0, 0].set_title('训练和验证损失')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('损失')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. 训练和验证准确率\n",
    "    axes[0, 1].plot(train_accs, label='训练准确率', color='blue', linewidth=2)\n",
    "    axes[0, 1].plot(val_accs, label='验证准确率', color='red', linewidth=2)\n",
    "    axes[0, 1].set_title('训练和验证准确率')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('准确率 (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. 验证F1分数\n",
    "    axes[1, 0].plot(val_f1s, label='验证F1分数', color='green', linewidth=2)\n",
    "    axes[1, 0].set_title('验证F1分数')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('F1分数 (%)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. 混淆矩阵热图\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('混淆矩阵')\n",
    "    axes[1, 1].set_ylabel('真实标签')\n",
    "    axes[1, 1].set_xlabel('预测标签')\n",
    "    plt.setp(axes[1, 1].get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # 调整布局\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    \n",
    "    # 保存图片\n",
    "    plt.savefig(output_dir / 'training_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(output_dir / 'training_results.pdf', bbox_inches='tight')\n",
    "    \n",
    "    print(\"✓ 训练结果可视化已保存\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 生成类别准确率条形图\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    class_accuracies = []\n",
    "    for i in range(num_classes):\n",
    "        class_mask = np.array(all_test_targets) == i\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_acc = np.mean(np.array(all_test_preds)[class_mask] == i) * 100\n",
    "            class_accuracies.append(class_acc)\n",
    "        else:\n",
    "            class_accuracies.append(0)\n",
    "    \n",
    "    bars = plt.bar(range(num_classes), class_accuracies, \n",
    "                   color=plt.cm.Set3(np.linspace(0, 1, num_classes)))\n",
    "    plt.title('各活动类别测试准确率', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('活动类别')\n",
    "    plt.ylabel('准确率 (%)')\n",
    "    plt.xticks(range(num_classes), class_names, rotation=45, ha='right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 在条形图上添加数值\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.ylim(0, 110)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'class_accuracies.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(output_dir / 'class_accuracies.pdf', bbox_inches='tight')\n",
    "    \n",
    "    print(\"✓ 类别准确率图已保存\")\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 可视化生成失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"✓ 训练结果可视化完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 训练总结和结果报告 ==========\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"🎉 SHL数据集训练流程完成!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# === 训练配置总结 ===\n",
    "print(f\"\\n📋 训练配置摘要:\")\n",
    "print(f\"  数据集: {dataset_name}\")\n",
    "print(f\"  数据路径: {SHL_DATA_PATH}\")\n",
    "print(f\"  训练模式: {'配置驱动' if USE_CONFIG_MODE else '传统模式'}\")\n",
    "print(f\"  设备: {device}\")\n",
    "print(f\"  随机种子: {random_seed}\")\n",
    "\n",
    "# === 数据摘要 ===\n",
    "print(f\"\\n📊 数据摘要:\")\n",
    "print(f\"  训练样本: {central_train_data.shape[0]:,}\")\n",
    "print(f\"  验证样本: {central_dev_data.shape[0]:,}\")\n",
    "print(f\"  测试样本: {central_test_data.shape[0]:,}\")\n",
    "print(f\"  特征维度: {central_train_data.shape[1]:,}\")\n",
    "print(f\"  活动类别: {activity_count}个\")\n",
    "print(f\"  数据类型: {central_train_data.dtype}\")\n",
    "\n",
    "# === 模型配置摘要 ===\n",
    "print(f\"\\n🧠 模型配置摘要:\")\n",
    "print(f\"  模型类型: CBranchformer\")\n",
    "print(f\"  参数量: {sum(p.numel() for p in har_model.parameters()):,}\")\n",
    "print(f\"  投影维度: {model_config['projection_dim']}\")\n",
    "print(f\"  注意力头数: {model_config['num_heads']}\")\n",
    "print(f\"  网络层数: {model_config['num_layers']}\")\n",
    "print(f\"  补丁大小: {model_config['patch_size']}\")\n",
    "print(f\"  时间步长: {model_config['time_step']}\")\n",
    "print(f\"  卷积核大小: {model_config['conv_kernel_size']}\")\n",
    "print(f\"  Dropout率: {model_config['dropout_rate']}\")\n",
    "\n",
    "# === 训练超参数摘要 ===\n",
    "print(f\"\\n⚙️ 训练超参数摘要:\")\n",
    "print(f\"  批次大小: {batch_size}\")\n",
    "print(f\"  学习率: {learning_rate}\")\n",
    "print(f\"  训练轮数: {epochs} (实际: {len(train_losses)})\")\n",
    "print(f\"  权重衰减: {weight_decay}\")\n",
    "print(f\"  梯度裁剪: {gradient_clip_norm}\")\n",
    "print(f\"  早停耐心: {patience}\")\n",
    "print(f\"  优化器: Adam\")\n",
    "print(f\"  调度器: CosineAnnealingLR\")\n",
    "print(f\"  损失函数: 加权交叉熵\")\n",
    "\n",
    "# === 训练结果摘要 ===\n",
    "print(f\"\\n🏆 训练结果摘要:\")\n",
    "print(f\"  最佳验证准确率: {best_val_acc:.2f}%\")\n",
    "print(f\"  最佳验证F1分数: {best_val_f1:.2f}%\")\n",
    "print(f\"  测试准确率: {test_acc:.2f}%\")\n",
    "print(f\"  测试F1分数: {test_f1:.2f}%\")\n",
    "print(f\"  测试损失: {test_loss_avg:.4f}\")\n",
    "\n",
    "# === 性能分析 ===\n",
    "print(f\"\\n📈 性能分析:\")\n",
    "if test_acc >= 90:\n",
    "    print(f\"  🟢 优秀: 测试准确率达到 {test_acc:.2f}%\")\n",
    "elif test_acc >= 80:\n",
    "    print(f\"  🟡 良好: 测试准确率为 {test_acc:.2f}%\")\n",
    "elif test_acc >= 70:\n",
    "    print(f\"  🟠 一般: 测试准确率为 {test_acc:.2f}%\")\n",
    "else:\n",
    "    print(f\"  🔴 待改进: 测试准确率仅为 {test_acc:.2f}%\")\n",
    "\n",
    "# 过拟合检查\n",
    "train_val_gap = train_accs[-1] - val_accs[-1]\n",
    "if train_val_gap > 10:\n",
    "    print(f\"  ⚠️ 可能过拟合: 训练准确率比验证准确率高 {train_val_gap:.1f}%\")\n",
    "    print(f\"     建议: 增加dropout率、减少模型complexity或增加数据\")\n",
    "elif train_val_gap < -5:\n",
    "    print(f\"  ⚠️ 可能欠拟合: 验证准确率比训练准确率高 {abs(train_val_gap):.1f}%\")\n",
    "    print(f\"     建议: 增加模型复杂度或减少正则化\")\n",
    "else:\n",
    "    print(f\"  ✅ 拟合良好: 训练验证准确率差距为 {train_val_gap:.1f}%\")\n",
    "\n",
    "# === 最佳表现类别分析 ===\n",
    "print(f\"\\n🎯 各类别性能分析:\")\n",
    "class_perfs = []\n",
    "for i in range(num_classes):\n",
    "    class_mask = np.array(all_test_targets) == i\n",
    "    if np.sum(class_mask) > 0:\n",
    "        class_acc = np.mean(np.array(all_test_preds)[class_mask] == i) * 100\n",
    "        activity_name = ACTIVITY_LABEL[i] if i < len(ACTIVITY_LABEL) else f\"类别{i}\"\n",
    "        class_perfs.append((activity_name, class_acc, np.sum(class_mask)))\n",
    "\n",
    "# 按准确率排序\n",
    "class_perfs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"  最佳表现: {class_perfs[0][0]} ({class_perfs[0][1]:.1f}%)\")\n",
    "print(f\"  最差表现: {class_perfs[-1][0]} ({class_perfs[-1][1]:.1f}%)\")\n",
    "print(f\"  平均准确率: {np.mean([perf[1] for perf in class_perfs]):.1f}%\")\n",
    "\n",
    "# === 文件输出摘要 ===\n",
    "print(f\"\\n📁 输出文件摘要:\")\n",
    "output_files = [\n",
    "    'best_model.pth',\n",
    "    'training_results.json', \n",
    "    'classification_report.txt',\n",
    "    'confusion_matrix.npy',\n",
    "    'training_results.png',\n",
    "    'training_results.pdf',\n",
    "    'class_accuracies.png',\n",
    "    'class_accuracies.pdf'\n",
    "]\n",
    "\n",
    "for file in output_files:\n",
    "    file_path = output_dir / file\n",
    "    if file_path.exists():\n",
    "        file_size = file_path.stat().st_size / 1024  # KB\n",
    "        print(f\"  ✅ {file} ({file_size:.1f}KB)\")\n",
    "    else:\n",
    "        print(f\"  ❌ {file} (未生成)\")\n",
    "\n",
    "print(f\"\\n  📂 所有结果保存在: {output_dir}\")\n",
    "\n",
    "# === 改进建议 ===\n",
    "print(f\"\\n💡 改进建议:\")\n",
    "if test_acc < 85:\n",
    "    print(f\"  • 尝试增加训练轮数或调整学习率\")\n",
    "    print(f\"  • 尝试不同的模型架构参数组合\")\n",
    "    print(f\"  • 检查数据质量和标签准确性\")\n",
    "    print(f\"  • 考虑数据增强技术\")\n",
    "\n",
    "if len(train_losses) == epochs:\n",
    "    print(f\"  • 训练可能未收敛，考虑增加训练轮数\")\n",
    "\n",
    "if total_size_mb > 500:\n",
    "    print(f\"  • 数据集较大，考虑使用更大的批次大小以提高效率\")\n",
    "\n",
    "print(f\"\\n🔄 下次训练时，只需修改第一个配置cell中的参数即可!\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"训练流程全部完成! 🎉\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HAR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
