{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ‡å‡†åº“å¯¼å…¥\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# é¡¹ç›®å†…éƒ¨å¯¼å…¥\n",
    "try:\n",
    "    from config.config_loader import ConfigLoader\n",
    "    from config.config_bridge import ConfigBridge\n",
    "except ImportError:\n",
    "    print(\"è­¦å‘Š: é…ç½®æ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†åªä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼\")\n",
    "    ConfigLoader = None\n",
    "    ConfigBridge = None\n",
    "\n",
    "import utils_torch as utils\n",
    "import model_cbranchformer as model\n",
    "\n",
    "print(\"æ‰€æœ‰æ¨¡å—å¯¼å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ä» utils_torch.py ç§»å…¥çš„å…³é”®ä»£ç  ===\n",
    "class HARDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# === é…ç½®æ¡¥æ¥å™¨ (ç®€åŒ–ç‰ˆ) ===\n",
    "class DotDict(dict):\n",
    "    \"\"\"æ”¯æŒç‚¹å·è®¿é—®çš„å­—å…¸\"\"\"\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self._dict = {}\n",
    "        for key, value in data.items():\n",
    "            self._dict[key] = self._to_dot_notation(value)\n",
    "            setattr(self, key, self._dict[key])\n",
    "    \n",
    "    def keys(self):\n",
    "        return self._dict.keys()\n",
    "    \n",
    "    def items(self):\n",
    "        return [(k, getattr(self, k)) for k in self._dict.keys()]\n",
    "    \n",
    "    def get(self, key, default=None):\n",
    "        return getattr(self, key, default)\n",
    "    \n",
    "    def _to_dot_notation(self, data):\n",
    "        if isinstance(data, dict):\n",
    "            return DotDict(data)\n",
    "        elif isinstance(data, list):\n",
    "            return [self._to_dot_notation(i) for i in data]\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "class SimplifiedConfigBridge:\n",
    "    \"\"\"ç®€åŒ–çš„é…ç½®æ¡¥æ¥å™¨\"\"\"\n",
    "    def __init__(self, config_path: str = None):\n",
    "        self.config_path = config_path\n",
    "        self.raw_config = {}\n",
    "        \n",
    "        if config_path and os.path.exists(config_path):\n",
    "            with open(config_path, 'r', encoding='utf-8') as f:\n",
    "                self.raw_config = yaml.safe_load(f)\n",
    "        \n",
    "        self.config = self._to_dot_notation(self.raw_config)\n",
    "    \n",
    "    def _to_dot_notation(self, data):\n",
    "        if isinstance(data, dict):\n",
    "            return DotDict(data)\n",
    "        elif isinstance(data, list):\n",
    "            return [self._to_dot_notation(i) for i in data]\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "    def get_dataset_config(self) -> Dict[str, Any]:\n",
    "        return self.raw_config.get('dataset', {})\n",
    "\n",
    "    def get_training_config(self) -> Dict[str, Any]:\n",
    "        return self.raw_config.get('training', {})\n",
    "        \n",
    "    def get_visualization_config(self) -> Dict[str, Any]:\n",
    "        return self.raw_config.get('visualization', {})\n",
    "\n",
    "print(\"âœ“ è¾…åŠ©å·¥å…·å’Œç±»å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurableTrainer:\n",
    "    \"\"\"é…ç½®é©±åŠ¨çš„è®­ç»ƒå™¨ç±»\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: Optional[str] = None):\n",
    "        \"\"\"åˆå§‹åŒ–è®­ç»ƒå™¨\"\"\"\n",
    "        self.config_path = config_path\n",
    "        self.use_config = config_path is not None and os.path.exists(config_path)\n",
    "        \n",
    "        # åˆå§‹åŒ–é…ç½®æ¡¥æ¥å™¨\n",
    "        if self.use_config:\n",
    "            try:\n",
    "                self.config_bridge = SimplifiedConfigBridge(config_path)\n",
    "                print(f\"âœ“ é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}\")\n",
    "                self.use_config = False\n",
    "        \n",
    "        # åˆå§‹åŒ–å…¶ä»–å±æ€§\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.criterion = None\n",
    "        self.device = None\n",
    "        self.logger = None\n",
    "        self.gradient_clip_norm = 0\n",
    "        self.output_dir = None\n",
    "    \n",
    "    def get_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"è·å–è®­ç»ƒå‚æ•°\"\"\"\n",
    "        if self.use_config:\n",
    "            return self._get_config_parameters()\n",
    "        else:\n",
    "            return self._get_hardcoded_parameters()\n",
    "    \n",
    "    def _get_config_parameters(self) -> Dict[str, Any]:\n",
    "        \"\"\"ä»é…ç½®æ–‡ä»¶è·å–å‚æ•°\"\"\"\n",
    "        config = self.config_bridge.raw_config\n",
    "        \n",
    "        params = {\n",
    "            # åŸºæœ¬è®¾ç½®\n",
    "            'device': config.get('device', 'auto'),\n",
    "            'random_seed': config.get('seed', 42),\n",
    "            'output_dir': config.get('output_dir', './results/shl_multimodal'),\n",
    "            'save_checkpoints': config.get('save_checkpoints', True),\n",
    "            'verbose': config.get('verbose', True),\n",
    "            \n",
    "            # æ•°æ®é›†é…ç½®\n",
    "            'dataset_name': config.get('dataset', {}).get('name', 'SHL'),\n",
    "            'data_path': config.get('dataset', {}).get('path', './datasets/'),\n",
    "            \n",
    "            # è®­ç»ƒé…ç½®\n",
    "            'batch_size': config.get('training', {}).get('batch_size', 32),\n",
    "            'learning_rate': config.get('training', {}).get('learning_rate', 0.001),\n",
    "            'epochs': config.get('training', {}).get('epochs', 100),\n",
    "            'weight_decay': config.get('training', {}).get('weight_decay', 0.0001),\n",
    "            'gradient_clip_norm': config.get('training', {}).get('gradient_clip_norm', 1.0),\n",
    "            'early_stopping_patience': config.get('training', {}).get('early_stopping_patience', 10),\n",
    "        }\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def setup_environment(self, params: Dict[str, Any]) -> None:\n",
    "        \"\"\"è®¾ç½®è®­ç»ƒç¯å¢ƒ\"\"\"\n",
    "        try:\n",
    "            # è®¾ç½®éšæœºç§å­\n",
    "            seed = params['random_seed']\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed(seed)\n",
    "                torch.cuda.manual_seed_all(seed)\n",
    "            \n",
    "            # è®¾ç½®è®¾å¤‡\n",
    "            device_config = params['device']\n",
    "            if device_config == 'auto':\n",
    "                if torch.cuda.is_available():\n",
    "                    self.device = torch.device('cuda')\n",
    "                    print(f\"ä½¿ç”¨ CUDA è®¾å¤‡: {torch.cuda.get_device_name()}\")\n",
    "                elif torch.backends.mps.is_available():\n",
    "                    self.device = torch.device('mps')\n",
    "                    print(\"ä½¿ç”¨ MPS è®¾å¤‡\")\n",
    "                else:\n",
    "                    self.device = torch.device('cpu')\n",
    "                    print(\"ä½¿ç”¨ CPU è®¾å¤‡\")\n",
    "            else:\n",
    "                self.device = torch.device(device_config)\n",
    "                print(f\"ä½¿ç”¨æŒ‡å®šè®¾å¤‡: {self.device}\")\n",
    "            \n",
    "            # è®¾ç½®è¾“å‡ºç›®å½•\n",
    "            self.output_dir = Path(params['output_dir'])\n",
    "            self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # ä¿å­˜æ¢¯åº¦è£å‰ªå‚æ•°\n",
    "            self.gradient_clip_norm = params.get('gradient_clip_norm', 0)\n",
    "            \n",
    "            # PyTorch æ€§èƒ½ä¼˜åŒ–\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.deterministic = False\n",
    "            \n",
    "            print(\"âœ“ ç¯å¢ƒè®¾ç½®å®Œæˆ\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ç¯å¢ƒè®¾ç½®å¤±è´¥: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_data(self, params: Dict[str, Any]) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "        \"\"\"åŠ è½½æ•°æ® - å ä½ç¬¦æ–¹æ³•\"\"\"\n",
    "        # è¿™ä¸ªæ–¹æ³•åœ¨å®é™…ä½¿ç”¨ä¸­ä¼šè¢«å¤–éƒ¨æ•°æ®åŠ è½½é€»è¾‘æ›¿ä»£\n",
    "        print(\"âš ï¸  ä½¿ç”¨å ä½ç¬¦æ•°æ®åŠ è½½æ–¹æ³•\")\n",
    "        return None, None, None\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"æ‰§è¡Œè®­ç»ƒ\"\"\"\n",
    "        print(\"å¼€å§‹é…ç½®é©±åŠ¨è®­ç»ƒ...\")\n",
    "        \n",
    "        # è·å–å‚æ•°å¹¶è®¾ç½®ç¯å¢ƒ\n",
    "        params = self.get_params()\n",
    "        self.setup_environment(params)\n",
    "        \n",
    "        print(\"âœ“ é…ç½®é©±åŠ¨è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ\")\n",
    "        print(\"   å®é™…è®­ç»ƒå°†ç”±åç»­cellsæ‰§è¡Œ\")\n",
    "\n",
    "print(\"âœ“ ConfigurableTrainerç±»å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== é…ç½®ç”Ÿæˆ - æ”¯æŒSHL_Multimodalæ•°æ®é›† ==========\n",
    "# ğŸ”¥ æ¯æ¬¡è®­ç»ƒåªéœ€è¦ä¿®æ”¹è¿™ä¸ªcellä¸­çš„åƒæ•¸\n",
    "\n",
    "# === åŸºæœ¬é…ç½®å‚æ•° ===\n",
    "USE_CONFIG_MODE = False  # True=é…ç½®é©±åŠ¨æ¨¡å¼, False=ä¼ ç»Ÿæ¨¡å¼ (æ¨èFalseä»¥ç¡®ä¿ç¨³å®šæ€§)\n",
    "CONFIG_PATH = \"config/default_configs/shl_config.yaml\"  # SHLæ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„\n",
    "\n",
    "# === SHL_Multimodalæ•°æ®é›†é…ç½® ===\n",
    "dataset_name = \"SHL\"  # æ•°æ®é›†åç§°\n",
    "# ğŸ”¥ ä¿®æ”¹ä¸ºæ‚¨çš„å®é™…æ–‡ä»¶è·¯å¾„\n",
    "SHL_DATA_PATH = \"/Users/zilongzeng/Research/MazeruHAR/datasets/datasetStandardized/SHL_Multimodal\"\n",
    "\n",
    "data_config = {\n",
    "    'window_size': 128,\n",
    "    'step_size': 64,\n",
    "    'normalize': True,\n",
    "    'filter_freq': 20,\n",
    "    'modalities': ['imu', 'magnetometer']  # å¯ç”¨çš„æ¨¡æ€\n",
    "}\n",
    "\n",
    "# === ğŸ”¥ ä¸»è¦è®­ç»ƒé…ç½® (ç»å¸¸ä¿®æ”¹çš„å‚æ•°) ===\n",
    "TRAINING_CONFIG = {\n",
    "    # è®­ç»ƒåŸºæœ¬å‚æ•°\n",
    "    'batch_size': 32,               # ğŸ”¥ æ‰¹æ¬¡å¤§å°: 16, 32, 64\n",
    "    'learning_rate': 0.001,         # ğŸ”¥ å­¦ä¹ ç‡: 0.0001, 0.001, 0.01\n",
    "    'epochs': 10,                  # ğŸ”¥ è®­ç»ƒè½®æ•°: 50, 100, 150, 200\n",
    "    'weight_decay': 0.0001,         # æƒé‡è¡°å‡\n",
    "    'dropout_rate': 0.1,            # Dropoutç‡: 0.1, 0.2, 0.3\n",
    "    'label_smoothing': 0.1,         # æ ‡ç­¾å¹³æ»‘\n",
    "    'gradient_clip_norm': 1.0,      # æ¢¯åº¦è£å‰ª\n",
    "    'early_stopping_patience': 10, # æ—©åœè€å¿ƒå€¼: 5, 10, 15\n",
    "    \n",
    "    # æ¨¡å‹æ¶æ„å‚æ•°\n",
    "    'projection_dim': 192,          # ğŸ”¥ æŠ•å½±ç»´åº¦: 128, 192, 256\n",
    "    'num_heads': 4,                 # ğŸ”¥ æ³¨æ„åŠ›å¤´æ•°: 4, 6, 8\n",
    "    'num_layers': 3,                # ğŸ”¥ å±‚æ•°: 2, 3, 4\n",
    "    'patch_size': 16,               # è¡¥ä¸å¤§å°: 8, 16, 32\n",
    "    'time_step': 16,                # æ—¶é—´æ­¥é•¿: 8, 16, 32\n",
    "    'conv_kernel_size': 15,         # å·ç§¯æ ¸å¤§å°: 7, 15, 31\n",
    "}\n",
    "\n",
    "# === ä¼ ç»Ÿæ¨¡å¼é…ç½® ===\n",
    "traditional_config = {\n",
    "    # æ•°æ®é›†ç›¸å…³\n",
    "    'dataset_name': dataset_name,\n",
    "    'data_path': SHL_DATA_PATH,  # ä½¿ç”¨æ‚¨çš„å®é™…è·¯å¾„\n",
    "    'window_size': 128,\n",
    "    'step_size': 64,\n",
    "    'sample_rate': 100,\n",
    "    \n",
    "    # åˆå¹¶è®­ç»ƒé…ç½®\n",
    "    **TRAINING_CONFIG,\n",
    "    \n",
    "    # å…¶ä»–è®¾ç½®\n",
    "    'device': 'auto',               # 'auto', 'cpu', 'cuda', 'mps'\n",
    "    'random_seed': 42,              # éšæœºç§å­\n",
    "    'output_dir': './results/shl_multimodal',\n",
    "    'save_checkpoints': True,\n",
    "    'verbose': True,\n",
    "    'position_device': '',          # SHLæ•°æ®é›†ä¸ä½¿ç”¨ä½ç½®è®¾å¤‡åˆ†å‰²\n",
    "    'main_dir': './'\n",
    "}\n",
    "\n",
    "# === SHL_Multimodalæ•°æ®é›†ç‰¹å®šé…ç½® ===\n",
    "shl_specific_config = {\n",
    "    'activity_labels': [\n",
    "        'Still', 'Walking', 'Run', 'Bike', \n",
    "        'Car', 'Bus', 'Train', 'Subway'\n",
    "    ],\n",
    "    'modalities': {\n",
    "        'imu': {\n",
    "            'channels': 6,  # Accelerometer (3) + Gyroscope (3)\n",
    "            'enabled': True,  # ğŸ”¥ æ˜¯å¦å¯ç”¨IMUæ¨¡æ€\n",
    "            'preprocessing': {\n",
    "                'normalize': True,\n",
    "                'filter_freq': 20\n",
    "            }\n",
    "        },\n",
    "        'magnetometer': {\n",
    "            'channels': 3,  # Magnetometer\n",
    "            'enabled': True,  # ğŸ”¥ æ˜¯å¦å¯ç”¨ç£åŠ›è®¡æ¨¡æ€\n",
    "            'preprocessing': {\n",
    "                'normalize': True\n",
    "            }\n",
    "        },\n",
    "        'linear_acc': {\n",
    "            'channels': 3,  # Linear Acceleration\n",
    "            'enabled': False,  # ğŸ”¥ å¯ä»¥æ ¹æ®éœ€è¦å¯ç”¨\n",
    "            'preprocessing': {\n",
    "                'normalize': True\n",
    "            }\n",
    "        },\n",
    "        'gravity': {\n",
    "            'channels': 3,  # Gravity\n",
    "            'enabled': False,  # ğŸ”¥ å¯ä»¥æ ¹æ®éœ€è¦å¯ç”¨\n",
    "            'preprocessing': {\n",
    "                'normalize': True\n",
    "            }\n",
    "        },\n",
    "        'orientation': {\n",
    "            'channels': 3,  # Orientation\n",
    "            'enabled': False,  # ğŸ”¥ å¯ä»¥æ ¹æ®éœ€è¦å¯ç”¨\n",
    "            'preprocessing': {\n",
    "                'normalize': True\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'train_split': 0.7,\n",
    "    'val_split': 0.15,\n",
    "    'test_split': 0.15\n",
    "}\n",
    "\n",
    "# === ğŸ”¥ å¿«é€Ÿé…ç½®é¢„è®¾ (å–æ¶ˆæ³¨é‡Šä½¿ç”¨) ===\n",
    "# å¿«é€Ÿæµ‹è¯•é…ç½®\n",
    "# TRAINING_CONFIG.update({\n",
    "#     'batch_size': 64,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'epochs': 20,\n",
    "#     'early_stopping_patience': 5\n",
    "# })\n",
    "\n",
    "# ç²¾ç»†è®­ç»ƒé…ç½®\n",
    "# TRAINING_CONFIG.update({\n",
    "#     'batch_size': 16,\n",
    "#     'learning_rate': 0.0005,\n",
    "#     'epochs': 200,\n",
    "#     'early_stopping_patience': 20\n",
    "# })\n",
    "\n",
    "# å¤§æ¨¡å‹é…ç½®\n",
    "# TRAINING_CONFIG.update({\n",
    "#     'projection_dim': 256,\n",
    "#     'num_heads': 8,\n",
    "#     'num_layers': 4,\n",
    "#     'batch_size': 16  # å‡å°æ‰¹æ¬¡å¤§å°\n",
    "# })\n",
    "\n",
    "# === é…ç½®éªŒè¯ä¸åˆå§‹åŒ– ===\n",
    "if USE_CONFIG_MODE and ConfigBridge is not None:\n",
    "    print(\"âœ“ ä½¿ç”¨é…ç½®é©±åŠ¨æ¨¡å¼\")\n",
    "    print(f\"  é…ç½®æ–‡ä»¶è·¯å¾„: {CONFIG_PATH}\")\n",
    "    \n",
    "    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "    if not os.path.exists(CONFIG_PATH):\n",
    "        print(f\"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {CONFIG_PATH}\")\n",
    "        print(\"   åˆ›å»ºé»˜è®¤SHL_Multimodalé…ç½®æ–‡ä»¶...\")\n",
    "        \n",
    "        # åˆ›å»ºé»˜è®¤SHL_Multimodalé…ç½®\n",
    "        default_shl_config = {\n",
    "            'name': 'SHL_MultiModal_Experiment',\n",
    "            'dataset': {\n",
    "                'name': 'SHL',\n",
    "                'path': SHL_DATA_PATH,\n",
    "                **shl_specific_config\n",
    "            },\n",
    "            'architecture': {\n",
    "                'experts': {\n",
    "                    'imu': {\n",
    "                        'type': 'cbranchformer',\n",
    "                        'params': {\n",
    "                            'projection_dim': TRAINING_CONFIG['projection_dim'],\n",
    "                            'num_heads': TRAINING_CONFIG['num_heads'],\n",
    "                            'num_layers': TRAINING_CONFIG['num_layers'],\n",
    "                            'patch_size': TRAINING_CONFIG['patch_size'],\n",
    "                            'time_step': TRAINING_CONFIG['time_step'],\n",
    "                            'conv_kernel_size': TRAINING_CONFIG['conv_kernel_size'],\n",
    "                            'dropout_rate': TRAINING_CONFIG['dropout_rate'],\n",
    "                            'output_dim': TRAINING_CONFIG['projection_dim']\n",
    "                        }\n",
    "                    },\n",
    "                    'magnetometer': {\n",
    "                        'type': 'lstm',\n",
    "                        'params': {\n",
    "                            'hidden_dim': 64,\n",
    "                            'num_layers': 2,\n",
    "                            'dropout_rate': TRAINING_CONFIG['dropout_rate'],\n",
    "                            'bidirectional': True,\n",
    "                            'output_dim': 64\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                'fusion': {\n",
    "                    'strategy': 'concatenate',\n",
    "                    'params': {}\n",
    "                },\n",
    "                'fusion_output_dim': TRAINING_CONFIG['projection_dim'] + 64,\n",
    "                'dropout_rate': TRAINING_CONFIG['dropout_rate']\n",
    "            },\n",
    "            'training': {\n",
    "                'batch_size': TRAINING_CONFIG['batch_size'],\n",
    "                'learning_rate': TRAINING_CONFIG['learning_rate'],\n",
    "                'epochs': TRAINING_CONFIG['epochs'],\n",
    "                'optimizer': 'adam',\n",
    "                'scheduler': 'cosine',\n",
    "                'weight_decay': TRAINING_CONFIG['weight_decay'],\n",
    "                'label_smoothing': TRAINING_CONFIG['label_smoothing'],\n",
    "                'gradient_clip_norm': TRAINING_CONFIG['gradient_clip_norm'],\n",
    "                'early_stopping_patience': TRAINING_CONFIG['early_stopping_patience']\n",
    "            },\n",
    "            'device': traditional_config['device'],\n",
    "            'seed': traditional_config['random_seed'],\n",
    "            'output_dir': traditional_config['output_dir'],\n",
    "            'save_checkpoints': True,\n",
    "            'verbose': True\n",
    "        }\n",
    "        \n",
    "        # ç¡®ä¿ç›®å½•å­˜åœ¨\n",
    "        os.makedirs(os.path.dirname(CONFIG_PATH), exist_ok=True)\n",
    "        \n",
    "        # ä¿å­˜é…ç½®æ–‡ä»¶\n",
    "        with open(CONFIG_PATH, 'w', encoding='utf-8') as f:\n",
    "            yaml.dump(default_shl_config, f, default_flow_style=False, allow_unicode=True)\n",
    "        \n",
    "        print(f\"âœ“ å·²åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶: {CONFIG_PATH}\")\n",
    "    \n",
    "    # åŠ è½½å¹¶éªŒè¯é…ç½®\n",
    "    try:\n",
    "        with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "            loaded_config = yaml.safe_load(f)\n",
    "        \n",
    "        # éªŒè¯å¿…è¦çš„é…ç½®é¡¹\n",
    "        required_sections = ['dataset', 'training', 'device']\n",
    "        for section in required_sections:\n",
    "            if section not in loaded_config:\n",
    "                raise ValueError(f\"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…è¦éƒ¨åˆ†: {section}\")\n",
    "        \n",
    "        print(\"âœ“ é…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡\")\n",
    "        print(f\"  æ•°æ®é›†: {loaded_config['dataset']['name']}\")\n",
    "        print(f\"  æ•°æ®è·¯å¾„: {loaded_config['dataset']['path']}\")\n",
    "        print(f\"  æ‰¹æ¬¡å¤§å°: {loaded_config['training']['batch_size']}\")\n",
    "        print(f\"  å­¦ä¹ ç‡: {loaded_config['training']['learning_rate']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}\")\n",
    "        print(\"   åˆ‡æ¢åˆ°ä¼ ç»Ÿæ¨¡å¼...\")\n",
    "        USE_CONFIG_MODE = False\n",
    "\n",
    "else:\n",
    "    print(\"âœ“ ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼\")\n",
    "    if not USE_CONFIG_MODE:\n",
    "        print(\"  åŸå› : æ‰‹åŠ¨è®¾ç½®ä¸ºä¼ ç»Ÿæ¨¡å¼\")\n",
    "    elif ConfigBridge is None:\n",
    "        print(\"  åŸå› : ConfigBridgeæ¨¡å—æœªå¯ç”¨\")\n",
    "    \n",
    "    # æ›´æ–°ä¼ ç»Ÿé…ç½®\n",
    "    traditional_config.update(TRAINING_CONFIG)\n",
    "\n",
    "# === éªŒè¯æ•°æ®æ–‡ä»¶å­˜åœ¨æ€§ ===\n",
    "print(f\"\\néªŒè¯SHL_Multimodalæ•°æ®æ–‡ä»¶...\")\n",
    "clients_data_path = os.path.join(SHL_DATA_PATH, 'clientsData.hkl')\n",
    "clients_label_path = os.path.join(SHL_DATA_PATH, 'clientsLabel.hkl')\n",
    "\n",
    "if os.path.exists(clients_data_path) and os.path.exists(clients_label_path):\n",
    "    print(f\"âœ“ æ•°æ®æ–‡ä»¶å­˜åœ¨:\")\n",
    "    print(f\"  æ•°æ®æ–‡ä»¶: {clients_data_path}\")\n",
    "    print(f\"  æ ‡ç­¾æ–‡ä»¶: {clients_label_path}\")\n",
    "else:\n",
    "    print(f\"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨:\")\n",
    "    print(f\"  æ•°æ®æ–‡ä»¶: {clients_data_path} ({'å­˜åœ¨' if os.path.exists(clients_data_path) else 'ä¸å­˜åœ¨'})\")\n",
    "    print(f\"  æ ‡ç­¾æ–‡ä»¶: {clients_label_path} ({'å­˜åœ¨' if os.path.exists(clients_label_path) else 'ä¸å­˜åœ¨'})\")\n",
    "    print(f\"  è¯·ç¡®ä¿SHL_Multimodalæ•°æ®å·²æ­£ç¡®å¤„ç†å¹¶ä¿å­˜\")\n",
    "\n",
    "# === è®¾ç½®å…¨å±€å˜é‡ ===\n",
    "dataset_name = traditional_config['dataset_name']\n",
    "batch_size = traditional_config['batch_size']\n",
    "learning_rate = traditional_config['learning_rate']\n",
    "epochs = traditional_config['epochs']\n",
    "random_seed = traditional_config['random_seed']\n",
    "position_device = traditional_config['position_device']\n",
    "main_dir = traditional_config['main_dir']\n",
    "\n",
    "# === æœ€ç»ˆé…ç½®æ‘˜è¦ ===\n",
    "print(f\"\"\"\n",
    "========== ğŸ”¥ é…ç½®æ‘˜è¦ ==========\n",
    "æ¨¡å¼: {'é…ç½®é©±åŠ¨' if USE_CONFIG_MODE else 'ä¼ ç»Ÿæ¨¡å¼'}\n",
    "æ•°æ®é›†: {dataset_name}\n",
    "æ•°æ®è·¯å¾„: {SHL_DATA_PATH}\n",
    "è®­ç»ƒå‚æ•°:\n",
    "  - æ‰¹æ¬¡å¤§å°: {traditional_config['batch_size']}\n",
    "  - å­¦ä¹ ç‡: {traditional_config['learning_rate']}\n",
    "  - è®­ç»ƒè½®æ•°: {traditional_config['epochs']}\n",
    "  - æŠ•å½±ç»´åº¦: {traditional_config['projection_dim']}\n",
    "  - æ³¨æ„åŠ›å¤´æ•°: {traditional_config['num_heads']}\n",
    "  - ç½‘ç»œå±‚æ•°: {traditional_config['num_layers']}\n",
    "è¾“å‡ºç›®å½•: {traditional_config['output_dir']}\n",
    "================================\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æ•°æ®åŠ è½½ - SHL_Multimodalæ•°æ®é›†æ”¯æŒ ==========\n",
    "\n",
    "print(\"å¼€å§‹åŠ è½½SHL_Multimodalæ•°æ®é›†...\")\n",
    "\n",
    "# === SHL_Multimodalæ•°æ®é›†åŠ è½½å‡½æ•° ===\n",
    "def load_shl_multimodal_dataset(data_path: str, config: Dict[str, Any]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    åŠ è½½SHL_Multimodalæ•°æ®é›†\n",
    "    \n",
    "    Args:\n",
    "        data_path: æ•°æ®é›†è·¯å¾„ (åº”è¯¥æ˜¯åŒ…å«clientsData.hklå’ŒclientsLabel.hklçš„ç›®å½•)\n",
    "        config: æ•°æ®é…ç½®\n",
    "    \n",
    "    Returns:\n",
    "        (combined_data, combined_labels)\n",
    "    \"\"\"\n",
    "    import hickle as hkl\n",
    "    \n",
    "    # SHL_Multimodalæ•°æ®é›†æ–‡ä»¶è·¯å¾„\n",
    "    clients_data_path = os.path.join(data_path, 'clientsData.hkl')\n",
    "    clients_label_path = os.path.join(data_path, 'clientsLabel.hkl')\n",
    "    \n",
    "    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "    if not os.path.exists(clients_data_path) or not os.path.exists(clients_label_path):\n",
    "        print(f\"âŒ SHL_Multimodalæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨:\")\n",
    "        print(f\"   æ•°æ®æ–‡ä»¶: {clients_data_path} ({'å­˜åœ¨' if os.path.exists(clients_data_path) else 'ä¸å­˜åœ¨'})\")\n",
    "        print(f\"   æ ‡ç­¾æ–‡ä»¶: {clients_label_path} ({'å­˜åœ¨' if os.path.exists(clients_label_path) else 'ä¸å­˜åœ¨'})\")\n",
    "        print(f\"   è¯·ç¡®ä¿å·²æ­£ç¡®è¿è¡ŒDATA_SHL_NEW.ipynbç”Ÿæˆæ•°æ®\")\n",
    "        raise FileNotFoundError(\"SHL_Multimodalæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨\")\n",
    "    \n",
    "    try:\n",
    "        # åŠ è½½æ•°æ®\n",
    "        print(f\"æ­£åœ¨åŠ è½½SHL_Multimodalæ•°æ®æ–‡ä»¶...\")\n",
    "        print(f\"  ä»è·¯å¾„: {data_path}\")\n",
    "        \n",
    "        clients_data = hkl.load(clients_data_path)\n",
    "        clients_labels = hkl.load(clients_label_path)\n",
    "        \n",
    "        print(f\"âœ“ æˆåŠŸåŠ è½½SHL_Multimodalæ•°æ®\")\n",
    "        print(f\"  å®¢æˆ·ç«¯/æµæ•°é‡: {len(clients_data)}\")\n",
    "        print(f\"  æ ‡ç­¾æ•°é‡: {len(clients_labels)}\")\n",
    "        \n",
    "        # æ£€æŸ¥æ•°æ®ç»“æ„\n",
    "        if len(clients_data) != len(clients_labels):\n",
    "            raise ValueError(f\"æ•°æ®å’Œæ ‡ç­¾æ•°é‡ä¸åŒ¹é…: {len(clients_data)} vs {len(clients_labels)}\")\n",
    "        \n",
    "        # åˆå¹¶æ‰€æœ‰å®¢æˆ·ç«¯/æµæ•°æ®\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for i, (client_data, client_label) in enumerate(zip(clients_data, clients_labels)):\n",
    "            if client_data is not None and len(client_data) > 0:\n",
    "                # ç¡®ä¿æ•°æ®æ˜¯numpyæ•°ç»„\n",
    "                if not isinstance(client_data, np.ndarray):\n",
    "                    client_data = np.array(client_data)\n",
    "                if not isinstance(client_label, np.ndarray):\n",
    "                    client_label = np.array(client_label)\n",
    "                \n",
    "                all_data.append(client_data)\n",
    "                all_labels.append(client_label)\n",
    "                print(f\"  å®¢æˆ·ç«¯/æµ {i}: {client_data.shape} (æ ·æœ¬ x ç‰¹å¾)\")\n",
    "                print(f\"    æ ‡ç­¾èŒƒå›´: {np.min(client_label)} - {np.max(client_label)}\")\n",
    "            else:\n",
    "                print(f\"  å®¢æˆ·ç«¯/æµ {i}: è·³è¿‡ (æ•°æ®ä¸ºç©º)\")\n",
    "        \n",
    "        if not all_data:\n",
    "            raise ValueError(\"æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®å¯åŠ è½½\")\n",
    "        \n",
    "        # åˆå¹¶æ•°æ®\n",
    "        print(f\"åˆå¹¶ {len(all_data)} ä¸ªå®¢æˆ·ç«¯/æµçš„æ•°æ®...\")\n",
    "        combined_data = np.vstack(all_data)\n",
    "        combined_labels = np.hstack(all_labels)\n",
    "        \n",
    "        print(f\"âœ“ æ•°æ®åˆå¹¶å®Œæˆ\")\n",
    "        print(f\"  æ€»æ ·æœ¬æ•°: {combined_data.shape[0]:,}\")\n",
    "        print(f\"  ç‰¹å¾ç»´åº¦: {combined_data.shape[1]}\")\n",
    "        print(f\"  æ•°æ®ç±»å‹: {combined_data.dtype}\")\n",
    "        print(f\"  æ•°æ®èŒƒå›´: [{np.min(combined_data):.3f}, {np.max(combined_data):.3f}]\")\n",
    "        print(f\"  æ ‡ç­¾èŒƒå›´: {np.min(combined_labels)} - {np.max(combined_labels)}\")\n",
    "        print(f\"  å”¯ä¸€æ ‡ç­¾: {sorted(np.unique(combined_labels))}\")\n",
    "        \n",
    "        # æ£€æŸ¥æ•°æ®è´¨é‡\n",
    "        if np.any(np.isnan(combined_data)):\n",
    "            nan_count = np.sum(np.isnan(combined_data))\n",
    "            print(f\"âš ï¸  å‘ç° {nan_count} ä¸ªNaNå€¼ï¼Œå°†è¿›è¡Œå¤„ç†\")\n",
    "            combined_data = np.nan_to_num(combined_data, nan=0.0)\n",
    "        \n",
    "        if np.any(np.isinf(combined_data)):\n",
    "            inf_count = np.sum(np.isinf(combined_data))\n",
    "            print(f\"âš ï¸  å‘ç° {inf_count} ä¸ªæ— ç©·å€¼ï¼Œå°†è¿›è¡Œå¤„ç†\")\n",
    "            combined_data = np.nan_to_num(combined_data, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        return combined_data, combined_labels\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åŠ è½½SHL_Multimodalæ•°æ®æ—¶å‡ºé”™: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def split_shl_data(data: np.ndarray, labels: np.ndarray, \n",
    "                   train_ratio: float = 0.7, val_ratio: float = 0.15,\n",
    "                   random_seed: int = 42) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    åˆ†å‰²SHL_Multimodalæ•°æ®é›†\n",
    "    \n",
    "    Args:\n",
    "        data: è¾“å…¥æ•°æ®\n",
    "        labels: æ ‡ç­¾\n",
    "        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹\n",
    "        val_ratio: éªŒè¯é›†æ¯”ä¾‹\n",
    "        random_seed: éšæœºç§å­\n",
    "    \n",
    "    Returns:\n",
    "        (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # è®¾ç½®éšæœºç§å­\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    print(f\"åˆ†å‰²æ•°æ®é›† (train:{train_ratio:.1%}, val:{val_ratio:.1%}, test:{1-train_ratio-val_ratio:.1%})...\")\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬è¿›è¡Œåˆ†å±‚æŠ½æ ·\n",
    "    unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "    min_count = np.min(label_counts)\n",
    "    \n",
    "    print(f\"æ ‡ç­¾åˆ†å¸ƒ:\")\n",
    "    activity_labels = shl_specific_config.get('activity_labels', [f'Class_{i}' for i in unique_labels])\n",
    "    for label, count in zip(unique_labels, label_counts):\n",
    "        label_name = activity_labels[label] if label < len(activity_labels) else f'Class_{label}'\n",
    "        print(f\"  {label_name}: {count:,} æ ·æœ¬\")\n",
    "    \n",
    "    # å¦‚æœæœ€å°‘çš„ç±»åˆ«æ ·æœ¬æ•°å°äº3ï¼Œåˆ™ä¸ä½¿ç”¨åˆ†å±‚æŠ½æ ·\n",
    "    use_stratify = min_count >= 3\n",
    "    if not use_stratify:\n",
    "        print(f\"âš ï¸  æœ€å°‘ç±»åˆ«æ ·æœ¬æ•°ä¸º {min_count}ï¼Œä¸ä½¿ç”¨åˆ†å±‚æŠ½æ ·\")\n",
    "    \n",
    "    try:\n",
    "        # ç¬¬ä¸€æ¬¡åˆ†å‰²: åˆ†ç¦»è®­ç»ƒé›†å’Œä¸´æ—¶é›†(éªŒè¯+æµ‹è¯•)\n",
    "        stratify_param = labels if use_stratify else None\n",
    "        train_data, temp_data, train_labels, temp_labels = train_test_split(\n",
    "            data, labels, test_size=(1-train_ratio), \n",
    "            random_state=random_seed, stratify=stratify_param\n",
    "        )\n",
    "        \n",
    "        # ç¬¬äºŒæ¬¡åˆ†å‰²: ä»ä¸´æ—¶é›†ä¸­åˆ†ç¦»éªŒè¯é›†å’Œæµ‹è¯•é›†\n",
    "        val_ratio_adjusted = val_ratio / (1 - train_ratio)  # è°ƒæ•´éªŒè¯é›†åœ¨ä¸´æ—¶é›†ä¸­çš„æ¯”ä¾‹\n",
    "        stratify_param_temp = temp_labels if use_stratify else None\n",
    "        val_data, test_data, val_labels, test_labels = train_test_split(\n",
    "            temp_data, temp_labels, test_size=(1-val_ratio_adjusted), \n",
    "            random_state=random_seed, stratify=stratify_param_temp\n",
    "        )\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"âš ï¸  åˆ†å±‚æŠ½æ ·å¤±è´¥ ({e})ï¼Œä½¿ç”¨éšæœºæŠ½æ ·\")\n",
    "        # é™çº§åˆ°éšæœºæŠ½æ ·\n",
    "        train_data, temp_data, train_labels, temp_labels = train_test_split(\n",
    "            data, labels, test_size=(1-train_ratio), random_state=random_seed\n",
    "        )\n",
    "        val_ratio_adjusted = val_ratio / (1 - train_ratio)\n",
    "        val_data, test_data, val_labels, test_labels = train_test_split(\n",
    "            temp_data, temp_labels, test_size=(1-val_ratio_adjusted), random_state=random_seed\n",
    "        )\n",
    "    \n",
    "    print(f\"âœ“ æ•°æ®åˆ†å‰²å®Œæˆ:\")\n",
    "    print(f\"  è®­ç»ƒé›†: {train_data.shape[0]:,} æ ·æœ¬ ({train_data.shape[0]/data.shape[0]*100:.1f}%)\")\n",
    "    print(f\"  éªŒè¯é›†: {val_data.shape[0]:,} æ ·æœ¬ ({val_data.shape[0]/data.shape[0]*100:.1f}%)\")\n",
    "    print(f\"  æµ‹è¯•é›†: {test_data.shape[0]:,} æ ·æœ¬ ({test_data.shape[0]/data.shape[0]*100:.1f}%)\")\n",
    "    \n",
    "    return train_data, train_labels, val_data, val_labels, test_data, test_labels\n",
    "\n",
    "# === æ ¹æ®æ¨¡å¼åŠ è½½æ•°æ® ===\n",
    "try:\n",
    "    if USE_CONFIG_MODE and ConfigBridge is not None:\n",
    "        # é…ç½®é©±åŠ¨æ¨¡å¼\n",
    "        print(\"ä½¿ç”¨é…ç½®é©±åŠ¨æ¨¡å¼åŠ è½½æ•°æ®...\")\n",
    "        \n",
    "        # åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹å¹¶åŠ è½½æ•°æ®\n",
    "        trainer = ConfigurableTrainer(CONFIG_PATH)\n",
    "        # æ³¨æ„: å®é™…çš„æ•°æ®åŠ è½½é€»è¾‘éœ€è¦åœ¨è¿™é‡Œå®ç°\n",
    "        print(\"âš ï¸  é…ç½®é©±åŠ¨æ¨¡å¼æ•°æ®åŠ è½½éœ€è¦è¿›ä¸€æ­¥å®ç°\")\n",
    "        USE_CONFIG_MODE = False  # ä¸´æ—¶åˆ‡æ¢åˆ°ä¼ ç»Ÿæ¨¡å¼\n",
    "        \n",
    "    if not USE_CONFIG_MODE:\n",
    "        # ä¼ ç»Ÿæ¨¡å¼\n",
    "        print(\"ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼åŠ è½½æ•°æ®...\")\n",
    "        \n",
    "        # åŠ è½½SHL_Multimodalæ•°æ®é›†\n",
    "        all_data, all_labels = load_shl_multimodal_dataset(SHL_DATA_PATH, traditional_config)\n",
    "        \n",
    "        # åˆ†å‰²æ•°æ®é›†\n",
    "        train_split = shl_specific_config.get('train_split', 0.7)\n",
    "        val_split = shl_specific_config.get('val_split', 0.15)\n",
    "        random_seed = traditional_config.get('random_seed', 42)\n",
    "        \n",
    "        (central_train_data, central_train_label, \n",
    "         central_dev_data, central_dev_label, \n",
    "         central_test_data, central_test_label) = split_shl_data(\n",
    "            all_data, all_labels, train_split, val_split, random_seed\n",
    "        )\n",
    "        \n",
    "        print(\"âœ“ ä¼ ç»Ÿæ¨¡å¼æ•°æ®åŠ è½½å®Œæˆ\")\n",
    "    \n",
    "    # === æ•°æ®åå¤„ç†å’ŒéªŒè¯ ===\n",
    "    print(f\"\\næ•°æ®åŠ è½½æ‘˜è¦:\")\n",
    "    print(f\"è®­ç»ƒé›†å½¢çŠ¶: {central_train_data.shape}\")\n",
    "    print(f\"éªŒè¯é›†å½¢çŠ¶: {central_dev_data.shape}\")  \n",
    "    print(f\"æµ‹è¯•é›†å½¢çŠ¶: {central_test_data.shape}\")\n",
    "    print(f\"è®­ç»ƒæ ‡ç­¾èŒƒå›´: {np.min(central_train_label)} - {np.max(central_train_label)}\")\n",
    "    print(f\"å”¯ä¸€æ ‡ç­¾æ•°é‡: {len(np.unique(central_train_label))}\")\n",
    "    \n",
    "    # æ´»åŠ¨æ ‡ç­¾æ˜ å°„ (SHLæ•°æ®é›†)\n",
    "    ACTIVITY_LABEL = shl_specific_config['activity_labels']\n",
    "    activity_count = len(ACTIVITY_LABEL)\n",
    "    \n",
    "    print(f\"\\næ´»åŠ¨ç±»åˆ« ({activity_count}ä¸ª):\")\n",
    "    for i, activity in enumerate(ACTIVITY_LABEL):\n",
    "        train_count = np.sum(central_train_label == i)\n",
    "        val_count = np.sum(central_dev_label == i)\n",
    "        test_count = np.sum(central_test_label == i)\n",
    "        total_count = train_count + val_count + test_count\n",
    "        print(f\"  {i}: {activity}\")\n",
    "        print(f\"     è®­ç»ƒ: {train_count:,}, éªŒè¯: {val_count:,}, æµ‹è¯•: {test_count:,}, æ€»è®¡: {total_count:,}\")\n",
    "    \n",
    "    # éªŒè¯æ•°æ®å®Œæ•´æ€§\n",
    "    print(f\"\\néªŒè¯æ•°æ®å®Œæ•´æ€§...\")\n",
    "    assert central_train_data.shape[0] == central_train_label.shape[0], \"è®­ç»ƒæ•°æ®å’Œæ ‡ç­¾æ•°é‡ä¸åŒ¹é…\"\n",
    "    assert central_dev_data.shape[0] == central_dev_label.shape[0], \"éªŒè¯æ•°æ®å’Œæ ‡ç­¾æ•°é‡ä¸åŒ¹é…\"\n",
    "    assert central_test_data.shape[0] == central_test_label.shape[0], \"æµ‹è¯•æ•°æ®å’Œæ ‡ç­¾æ•°é‡ä¸åŒ¹é…\"\n",
    "    \n",
    "    # æ£€æŸ¥æ ‡ç­¾èŒƒå›´\n",
    "    all_labels_combined = np.concatenate([central_train_label, central_dev_label, central_test_label])\n",
    "    unique_labels = np.unique(all_labels_combined)\n",
    "    assert np.min(unique_labels) >= 0, \"æ ‡ç­¾åŒ…å«è´Ÿå€¼\"\n",
    "    assert np.max(unique_labels) < activity_count, f\"æ ‡ç­¾è¶…å‡ºèŒƒå›´ (æœ€å¤§: {np.max(unique_labels)}, æœŸæœ› < {activity_count})\"\n",
    "    \n",
    "    # æ£€æŸ¥æ•°æ®ç»´åº¦ä¸€è‡´æ€§\n",
    "    expected_features = central_train_data.shape[1]\n",
    "    assert central_dev_data.shape[1] == expected_features, f\"éªŒè¯é›†ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: {central_dev_data.shape[1]} vs {expected_features}\"\n",
    "    assert central_test_data.shape[1] == expected_features, f\"æµ‹è¯•é›†ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: {central_test_data.shape[1]} vs {expected_features}\"\n",
    "    \n",
    "    print(\"âœ“ æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡\")\n",
    "    \n",
    "    # æ•°æ®ç»Ÿè®¡ä¿¡æ¯\n",
    "    print(f\"\\næ•°æ®ç»Ÿè®¡ä¿¡æ¯:\")\n",
    "    print(f\"  ç‰¹å¾ç»´åº¦: {expected_features}\")\n",
    "    print(f\"  æ•°æ®ç±»å‹: {central_train_data.dtype}\")\n",
    "    print(f\"  è®­ç»ƒæ•°æ®èŒƒå›´: [{np.min(central_train_data):.3f}, {np.max(central_train_data):.3f}]\")\n",
    "    print(f\"  è®­ç»ƒæ•°æ®å‡å€¼: {np.mean(central_train_data):.3f}\")\n",
    "    print(f\"  è®­ç»ƒæ•°æ®æ ‡å‡†å·®: {np.std(central_train_data):.3f}\")\n",
    "    \n",
    "    # è®¾ç½®å…¼å®¹æ€§å˜é‡ (ä¸åŸå§‹ä»£ç ä¿æŒä¸€è‡´)\n",
    "    client_orientation_train = None\n",
    "    client_orientation_test = None  \n",
    "    orientations_names = None\n",
    "    position_device = traditional_config.get('position_device', '')\n",
    "    \n",
    "    print(\"âœ“ SHL_Multimodalæ•°æ®é›†åŠ è½½å®Œæˆ!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # å¦‚æœæ•°æ®åŠ è½½å¤±è´¥ï¼Œç”Ÿæˆç¤ºä¾‹æ•°æ®ä»¥ä¾¿æµ‹è¯•\n",
    "    print(f\"\\nâš ï¸  æ•°æ®åŠ è½½å¤±è´¥ï¼Œç”Ÿæˆç¤ºä¾‹æ•°æ®ä»¥ä¾¿æµ‹è¯•...\")\n",
    "    print(f\"è¯·æ£€æŸ¥ä»¥ä¸‹é—®é¢˜:\")\n",
    "    print(f\"1. æ•°æ®è·¯å¾„æ˜¯å¦æ­£ç¡®: {SHL_DATA_PATH}\")\n",
    "    print(f\"2. æ˜¯å¦å­˜åœ¨ clientsData.hkl å’Œ clientsLabel.hkl æ–‡ä»¶\")\n",
    "    print(f\"3. æ˜¯å¦å·²è¿è¡Œ DATA_SHL_NEW.ipynb ç”Ÿæˆæ•°æ®\")\n",
    "    \n",
    "    # ç”Ÿæˆç¤ºä¾‹SHL_Multimodalæ•°æ®\n",
    "    n_train = 1000\n",
    "    n_val = 200\n",
    "    n_test = 300\n",
    "    \n",
    "    # æ ¹æ®SHL_Multimodalçš„å®é™…ç‰¹å¾ç»´åº¦è®¾ç½®\n",
    "    # å‡è®¾åŒ…å«å¤šä¸ªä¼ æ„Ÿå™¨æ¨¡æ€çš„ç‰¹å¾\n",
    "    n_features = 128 * 18  # 128ä¸ªæ—¶é—´æ­¥ * 18ä¸ªä¼ æ„Ÿå™¨é€šé“ (Acc(3) + Gyr(3) + Mag(3) + LAcc(3) + Gra(3) + Ori(3))\n",
    "    n_classes = len(shl_specific_config['activity_labels'])\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(f\"ç”Ÿæˆç¤ºä¾‹æ•°æ®:\")\n",
    "    print(f\"  ç‰¹å¾ç»´åº¦: {n_features} (å‡è®¾ 128æ—¶é—´æ­¥ Ã— 18ä¼ æ„Ÿå™¨é€šé“)\")\n",
    "    print(f\"  ç±»åˆ«æ•°: {n_classes}\")\n",
    "    \n",
    "    central_train_data = np.random.randn(n_train, n_features).astype(np.float32)\n",
    "    central_train_label = np.random.randint(0, n_classes, n_train)\n",
    "    \n",
    "    central_dev_data = np.random.randn(n_val, n_features).astype(np.float32)\n",
    "    central_dev_label = np.random.randint(0, n_classes, n_val)\n",
    "    \n",
    "    central_test_data = np.random.randn(n_test, n_features).astype(np.float32)\n",
    "    central_test_label = np.random.randint(0, n_classes, n_test)\n",
    "    \n",
    "    # è®¾ç½®æ´»åŠ¨æ ‡ç­¾å’Œè®¡æ•°\n",
    "    ACTIVITY_LABEL = shl_specific_config['activity_labels']\n",
    "    activity_count = len(ACTIVITY_LABEL)\n",
    "    \n",
    "    # å…¼å®¹æ€§å˜é‡\n",
    "    client_orientation_train = None\n",
    "    client_orientation_test = None\n",
    "    orientations_names = None\n",
    "    position_device = ''\n",
    "    \n",
    "    print(f\"âœ“ ç¤ºä¾‹æ•°æ®ç”Ÿæˆå®Œæˆ:\")\n",
    "    print(f\"  è®­ç»ƒé›†: {central_train_data.shape}\")\n",
    "    print(f\"  éªŒè¯é›†: {central_dev_data.shape}\")\n",
    "    print(f\"  æµ‹è¯•é›†: {central_test_data.shape}\")\n",
    "    print(f\"  ç±»åˆ«æ•°: {activity_count}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"æ•°æ®åŠ è½½é˜¶æ®µå®Œæˆ\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# === æ•°æ®åŠ è½½åçš„é¢å¤–æ£€æŸ¥ ===\n",
    "print(f\"\\næœ€ç»ˆæ•°æ®æ£€æŸ¥:\")\n",
    "print(f\"æ•°æ®é›†ç±»å‹: SHL_Multimodal\")\n",
    "print(f\"æ•°æ®æ¥æº: {SHL_DATA_PATH}\")\n",
    "print(f\"è®­ç»ƒæ ·æœ¬: {central_train_data.shape[0]:,}\")\n",
    "print(f\"éªŒè¯æ ·æœ¬: {central_dev_data.shape[0]:,}\")\n",
    "print(f\"æµ‹è¯•æ ·æœ¬: {central_test_data.shape[0]:,}\")\n",
    "print(f\"ç‰¹å¾ç»´åº¦: {central_train_data.shape[1]:,}\")\n",
    "print(f\"æ´»åŠ¨ç±»åˆ«: {activity_count}ä¸ª\")\n",
    "print(f\"æ•°æ®ç±»å‹: {central_train_data.dtype}\")\n",
    "\n",
    "# æ£€æŸ¥å†…å­˜ä½¿ç”¨\n",
    "train_size_mb = central_train_data.nbytes / (1024 * 1024)\n",
    "total_size_mb = (central_train_data.nbytes + central_dev_data.nbytes + central_test_data.nbytes) / (1024 * 1024)\n",
    "print(f\"å†…å­˜ä½¿ç”¨: è®­ç»ƒé›† {train_size_mb:.1f}MB, æ€»è®¡ {total_size_mb:.1f}MB\")\n",
    "\n",
    "if total_size_mb > 1000:  # è¶…è¿‡1GBæ—¶è­¦å‘Š\n",
    "    print(f\"âš ï¸  æ•°æ®é›†è¾ƒå¤§ ({total_size_mb:.1f}MB)ï¼Œå»ºè®®ä½¿ç”¨è¾ƒå°çš„æ‰¹æ¬¡å¤§å°\")\n",
    "    if traditional_config['batch_size'] > 32:\n",
    "        traditional_config['batch_size'] = 32\n",
    "        print(f\"   è‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å¤§å°ä¸º: {traditional_config['batch_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æ•°æ®é¢„å¤„ç†å’Œæ•°æ®åŠ è½½å™¨åˆ›å»º ==========\n",
    "\n",
    "print(\"å¼€å§‹è®­ç»ƒå‡†å¤‡...\")\n",
    "\n",
    "try:\n",
    "    # å¦‚æœåœ¨RealWorldæˆ–HHARä¸Šä½¿ç”¨æŒ‡å®šä½ç½®/è®¾å¤‡ï¼Œæˆ‘ä»¬ç§»é™¤ä¸€ä¸ªå¹¶å°†å…¶ç”¨ä½œæµ‹è¯•é›†ï¼Œå¹¶å°†å…¶ä»–ç”¨äºè®­ç»ƒ\n",
    "    # SHLæ•°æ®é›†ä¸éœ€è¦æ­¤æ­¥éª¤ï¼Œä½†ä¿ç•™å…¼å®¹æ€§\n",
    "    if position_device != '' or dataset_name == 'UCI':\n",
    "        print(f\"æ³¨æ„: SHLæ•°æ®é›†ä¸æ”¯æŒä½ç½®è®¾å¤‡åˆ†å‰² (position_device='{position_device}')\")\n",
    "        print(\"ä½¿ç”¨æ ‡å‡†æ•°æ®åˆ†å‰²...\")\n",
    "    \n",
    "    # è®¡ç®—ç±»æƒé‡ (å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†)\n",
    "    print(\"è®¡ç®—ç±»æƒé‡...\")\n",
    "    from sklearn.utils import class_weight\n",
    "    \n",
    "    temp_weights = class_weight.compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(central_train_label),\n",
    "        y=central_train_label.ravel()\n",
    "    )\n",
    "    class_weights = {j: temp_weights[j] for j in range(len(temp_weights))}\n",
    "    \n",
    "    print(\"âœ“ ç±»æƒé‡è®¡ç®—å®Œæˆ:\")\n",
    "    for i, weight in class_weights.items():\n",
    "        activity_name = ACTIVITY_LABEL[i] if i < len(ACTIVITY_LABEL) else f\"ç±»åˆ«{i}\"\n",
    "        print(f\"  {activity_name}: {weight:.3f}\")\n",
    "\n",
    "    # åˆ›å»ºPyTorchæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨\n",
    "    print(\"åˆ›å»ºPyTorchæ•°æ®é›†...\")\n",
    "    \n",
    "    # åˆ›å»ºç‰¹å¾å’Œæ ‡ç­¾çš„å¼ é‡\n",
    "    train_features = torch.FloatTensor(central_train_data)\n",
    "    train_labels = torch.LongTensor(central_train_label)\n",
    "    dev_features = torch.FloatTensor(central_dev_data)\n",
    "    dev_labels = torch.LongTensor(central_dev_label)\n",
    "    test_features = torch.FloatTensor(central_test_data)\n",
    "    test_labels = torch.LongTensor(central_test_label)\n",
    "\n",
    "    # åˆ›å»ºæ•°æ®é›†\n",
    "    train_dataset = HARDataset(train_features, train_labels)\n",
    "    dev_dataset = HARDataset(dev_features, dev_labels)\n",
    "    test_dataset = HARDataset(test_features, test_labels)\n",
    "\n",
    "    # è·å–æ‰¹æ¬¡å¤§å°\n",
    "    batch_size = traditional_config.get('batch_size', 32)\n",
    "\n",
    "    # åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, \n",
    "        num_workers=0, pin_memory=False, drop_last=False\n",
    "    )\n",
    "    dev_loader = torch.utils.data.DataLoader(\n",
    "        dev_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=0, pin_memory=False, drop_last=False\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=0, pin_memory=False, drop_last=False\n",
    "    )\n",
    "\n",
    "    print(f\"âœ“ æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:\")\n",
    "    print(f\"  è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}\")\n",
    "    print(f\"  éªŒè¯æ‰¹æ¬¡æ•°: {len(dev_loader)}\")\n",
    "    print(f\"  æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}\")\n",
    "    print(f\"  æ‰¹æ¬¡å¤§å°: {batch_size}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(\"âœ“ æ•°æ®é¢„å¤„ç†é˜¶æ®µå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æ¨¡å‹åˆ›å»ºå’Œè®­ç»ƒè®¾ç½® ==========\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"å¼€å§‹æ¨¡å‹è®­ç»ƒ\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "try:\n",
    "    # === ä¼ ç»Ÿæ¨¡å¼è®­ç»ƒ ===\n",
    "    print(\"ä½¿ç”¨ä¼ ç»Ÿè®­ç»ƒæ¨¡å¼...\")\n",
    "    \n",
    "    # è·å–è®­ç»ƒå‚æ•°\n",
    "    learning_rate = traditional_config.get('learning_rate', 0.001)\n",
    "    epochs = traditional_config.get('epochs', 100)\n",
    "    weight_decay = traditional_config.get('weight_decay', 0.0001)\n",
    "    random_seed = traditional_config.get('random_seed', 42)\n",
    "    \n",
    "    # è®¾ç½®è®¾å¤‡\n",
    "    device_config = traditional_config.get('device', 'auto')\n",
    "    if device_config == 'auto':\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device('cuda')\n",
    "            print(f\"ä½¿ç”¨ CUDA è®¾å¤‡: {torch.cuda.get_device_name()}\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = torch.device('mps')\n",
    "            print(\"ä½¿ç”¨ MPS è®¾å¤‡\")\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "            print(\"ä½¿ç”¨ CPU è®¾å¤‡\")\n",
    "    else:\n",
    "        device = torch.device(device_config)\n",
    "        print(f\"ä½¿ç”¨æŒ‡å®šè®¾å¤‡: {device}\")\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡å‹\n",
    "    input_dim = central_train_data.shape[1]  # ç‰¹å¾ç»´åº¦\n",
    "    num_classes = len(np.unique(central_train_label))  # ç±»åˆ«æ•°é‡\n",
    "    \n",
    "    print(f\"åˆ›å»ºæ¨¡å‹:\")\n",
    "    print(f\"  è¾“å…¥ç»´åº¦: {input_dim}\")\n",
    "    print(f\"  è¾“å‡ºç±»åˆ«æ•°: {num_classes}\")\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡å‹å®ä¾‹\n",
    "    model_config = {\n",
    "        'input_dim': input_dim,\n",
    "        'num_classes': num_classes,\n",
    "        'projection_dim': traditional_config.get('projection_dim', 192),\n",
    "        'num_heads': traditional_config.get('num_heads', 4),\n",
    "        'num_layers': traditional_config.get('num_layers', 3),\n",
    "        'patch_size': traditional_config.get('patch_size', 16),\n",
    "        'time_step': traditional_config.get('time_step', 16),\n",
    "        'conv_kernel_size': traditional_config.get('conv_kernel_size', 15),\n",
    "        'dropout_rate': traditional_config.get('dropout_rate', 0.1)\n",
    "    }\n",
    "    \n",
    "    # ä½¿ç”¨CBranchformeræ¨¡å‹\n",
    "    har_model = model.CBranchformerHAR(\n",
    "        input_dim=model_config['input_dim'],\n",
    "        num_classes=model_config['num_classes'],\n",
    "        projection_dim=model_config['projection_dim'],\n",
    "        num_heads=model_config['num_heads'],\n",
    "        num_layers=model_config['num_layers'],\n",
    "        patch_size=model_config['patch_size'],\n",
    "        time_step=model_config['time_step'],\n",
    "        conv_kernel_size=model_config['conv_kernel_size'],\n",
    "        dropout_rate=model_config['dropout_rate']\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"âœ“ æ¨¡å‹åˆ›å»ºå®Œæˆ\")\n",
    "    print(f\"  æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in har_model.parameters()):,}\")\n",
    "    \n",
    "    # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°\n",
    "    optimizer = torch.optim.Adam(\n",
    "        har_model.parameters(), \n",
    "        lr=learning_rate, \n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    # ä½¿ç”¨åŠ æƒäº¤å‰ç†µæŸå¤±å¤„ç†ç±»åˆ«ä¸å¹³è¡¡\n",
    "    class_weights_tensor = torch.FloatTensor([class_weights[i] for i in range(num_classes)]).to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    \n",
    "    # å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    print(f\"âœ“ ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°è®¾ç½®å®Œæˆ\")\n",
    "    print(f\"  ä¼˜åŒ–å™¨: Adam (lr={learning_rate}, weight_decay={weight_decay})\")\n",
    "    print(f\"  æŸå¤±å‡½æ•°: åŠ æƒäº¤å‰ç†µ\")\n",
    "    print(f\"  å­¦ä¹ ç‡è°ƒåº¦: Cosine Annealing\")\n",
    "    \n",
    "    # è®¾ç½®è¾“å‡ºç›®å½•\n",
    "    output_dir = Path(traditional_config.get('output_dir', './results/shl_multimodal'))\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"  è¾“å‡ºç›®å½•: {output_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(\"âœ“ æ¨¡å‹åˆ›å»ºå’Œè®­ç»ƒè®¾ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== è®­ç»ƒå¾ªç¯æ‰§è¡Œ ==========\n",
    "\n",
    "print(f\"\\nå¼€å§‹è®­ç»ƒ (å…±{epochs}ä¸ªepoch)...\")\n",
    "\n",
    "try:\n",
    "    # è®­ç»ƒå‚æ•°\n",
    "    best_val_acc = 0.0\n",
    "    best_val_f1 = 0.0\n",
    "    patience = traditional_config.get('early_stopping_patience', 10)\n",
    "    patience_counter = 0\n",
    "    gradient_clip_norm = traditional_config.get('gradient_clip_norm', 1.0)\n",
    "    \n",
    "    # è®°å½•è®­ç»ƒå†å²\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    val_f1s = []\n",
    "    \n",
    "    print(f\"è®­ç»ƒé…ç½®:\")\n",
    "    print(f\"  è®¾å¤‡: {device}\")\n",
    "    print(f\"  æ‰¹æ¬¡å¤§å°: {batch_size}\")\n",
    "    print(f\"  å­¦ä¹ ç‡: {learning_rate}\")\n",
    "    print(f\"  è®­ç»ƒè½®æ•°: {epochs}\")\n",
    "    print(f\"  æ—©åœè€å¿ƒ: {patience}\")\n",
    "    print(f\"  æ¢¯åº¦è£å‰ª: {gradient_clip_norm}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # === è®­ç»ƒé˜¶æ®µ ===\n",
    "        har_model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = har_model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # æ¢¯åº¦è£å‰ª\n",
    "            if gradient_clip_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(har_model.parameters(), gradient_clip_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            # æ‰“å°æ‰¹æ¬¡è¿›åº¦ (æ¯50ä¸ªæ‰¹æ¬¡)\n",
    "            if batch_idx % 50 == 0 and batch_idx > 0:\n",
    "                current_acc = 100.0 * train_correct / train_total\n",
    "                print(f\"  Epoch [{epoch+1}/{epochs}] Batch [{batch_idx}/{len(train_loader)}] \"\n",
    "                      f\"Loss: {loss.item():.4f} Acc: {current_acc:.2f}%\")\n",
    "        \n",
    "        # === éªŒè¯é˜¶æ®µ ===\n",
    "        har_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, targets in dev_loader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                outputs = har_model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        # è®¡ç®—æŒ‡æ ‡\n",
    "        train_acc = 100.0 * train_correct / train_total\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "        val_f1 = f1_score(all_targets, all_preds, average='weighted') * 100\n",
    "        \n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss / len(dev_loader))\n",
    "        val_accs.append(val_acc)\n",
    "        val_f1s.append(val_f1)\n",
    "        \n",
    "        # æ›´æ–°å­¦ä¹ ç‡\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # è®¡ç®—epochæ—¶é—´\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # æ‰“å°è®­ç»ƒä¿¡æ¯\n",
    "        print(f\"Epoch [{epoch+1:3d}/{epochs}] \"\n",
    "              f\"Train Loss: {train_losses[-1]:.4f} \"\n",
    "              f\"Train Acc: {train_acc:.2f}% \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f} \"\n",
    "              f\"Val Acc: {val_acc:.2f}% \"\n",
    "              f\"Val F1: {val_f1:.2f}% \"\n",
    "              f\"LR: {current_lr:.2e} \"\n",
    "              f\"Time: {epoch_time:.1f}s\")\n",
    "        \n",
    "        # æ—©åœæ£€æŸ¥å’Œæ¨¡å‹ä¿å­˜\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "            if traditional_config.get('save_checkpoints', True):\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': har_model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'best_val_acc': best_val_acc,\n",
    "                    'best_val_f1': best_val_f1,\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_losses': val_losses,\n",
    "                    'val_accs': val_accs,\n",
    "                    'model_config': model_config,\n",
    "                    'traditional_config': traditional_config\n",
    "                }, output_dir / 'best_model.pth')\n",
    "            \n",
    "            print(f\"    âœ“ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}% (F1: {best_val_f1:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\næ—©åœè§¦å‘! éªŒè¯å‡†ç¡®ç‡åœ¨{patience}ä¸ªepochå†…æœªæ”¹å–„\")\n",
    "                print(f\"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%\")\n",
    "                print(f\"æœ€ä½³éªŒè¯F1åˆ†æ•°: {best_val_f1:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        # æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹\n",
    "        if (epoch + 1) % 10 == 0 and traditional_config.get('save_checkpoints', True):\n",
    "            checkpoint_path = output_dir / f'checkpoint_epoch_{epoch+1}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': har_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'val_accs': val_accs\n",
    "            }, checkpoint_path)\n",
    "    \n",
    "    print(f\"\\nâœ“ è®­ç»ƒå®Œæˆ!\")\n",
    "    print(f\"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%\")\n",
    "    print(f\"  æœ€ä½³éªŒè¯F1åˆ†æ•°: {best_val_f1:.2f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ è®­ç»ƒæ‰§è¡Œå¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(\"âœ“ è®­ç»ƒå¾ªç¯æ‰§è¡Œå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æµ‹è¯•è¯„ä¼° ==========\n",
    "\n",
    "print(f\"\\nå¼€å§‹æµ‹è¯•æœ€ä½³æ¨¡å‹...\")\n",
    "\n",
    "try:\n",
    "    # åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "    if traditional_config.get('save_checkpoints', True):\n",
    "        checkpoint_path = output_dir / 'best_model.pth'\n",
    "        if checkpoint_path.exists():\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            har_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(\"âœ“ å·²åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡\")\n",
    "        else:\n",
    "            print(\"âš ï¸  æœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹\")\n",
    "    \n",
    "    # æµ‹è¯•è¯„ä¼°\n",
    "    har_model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    all_test_preds = []\n",
    "    all_test_targets = []\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = har_model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            test_total += targets.size(0)\n",
    "            test_correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            all_test_preds.extend(predicted.cpu().numpy())\n",
    "            all_test_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    test_acc = 100.0 * test_correct / test_total\n",
    "    test_f1 = f1_score(all_test_targets, all_test_preds, average='weighted') * 100\n",
    "    test_loss_avg = test_loss / len(test_loader)\n",
    "    \n",
    "    print(f\"âœ“ æµ‹è¯•å®Œæˆ!\")\n",
    "    print(f\"  æµ‹è¯•æŸå¤±: {test_loss_avg:.4f}\")\n",
    "    print(f\"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%\")\n",
    "    print(f\"  æµ‹è¯•F1åˆ†æ•°: {test_f1:.2f}%\")\n",
    "    \n",
    "    # ç”Ÿæˆè¯¦ç»†åˆ†ç±»æŠ¥å‘Š\n",
    "    print(f\"\\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\n",
    "    class_names = [ACTIVITY_LABEL[i] if i < len(ACTIVITY_LABEL) else f\"ç±»åˆ«{i}\" \n",
    "                  for i in range(num_classes)]\n",
    "    report = classification_report(\n",
    "        all_test_targets, all_test_preds, \n",
    "        target_names=class_names, \n",
    "        digits=3\n",
    "    )\n",
    "    print(report)\n",
    "    \n",
    "    # ç”Ÿæˆæ··æ·†çŸ©é˜µ\n",
    "    print(f\"\\næ··æ·†çŸ©é˜µ:\")\n",
    "    cm = confusion_matrix(all_test_targets, all_test_preds)\n",
    "    print(cm)\n",
    "    \n",
    "    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡\n",
    "    print(f\"\\nå„ç±»åˆ«å‡†ç¡®ç‡:\")\n",
    "    for i in range(num_classes):\n",
    "        class_mask = np.array(all_test_targets) == i\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_acc = np.mean(np.array(all_test_preds)[class_mask] == i) * 100\n",
    "            activity_name = ACTIVITY_LABEL[i] if i < len(ACTIVITY_LABEL) else f\"ç±»åˆ«{i}\"\n",
    "            print(f\"  {activity_name}: {class_acc:.2f}% ({np.sum(class_mask)} æ ·æœ¬)\")\n",
    "    \n",
    "    # ä¿å­˜ç»“æœ\n",
    "    results = {\n",
    "        'dataset': dataset_name,\n",
    "        'model_config': model_config,\n",
    "        'training_config': traditional_config,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'test_loss': test_loss_avg,\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'training_history': {\n",
    "            'train_losses': train_losses,\n",
    "            'train_accs': train_accs,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accs': val_accs,\n",
    "            'val_f1s': val_f1s\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶\n",
    "    with open(output_dir / 'training_results.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump({k: v for k, v in results.items() if k != 'confusion_matrix'}, \n",
    "                 f, indent=2, default=str, ensure_ascii=False)\n",
    "    \n",
    "    # ä¿å­˜åˆ†ç±»æŠ¥å‘Š\n",
    "    with open(output_dir / 'classification_report.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    # ä¿å­˜æ··æ·†çŸ©é˜µ\n",
    "    np.save(output_dir / 'confusion_matrix.npy', cm)\n",
    "    \n",
    "    print(f\"âœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æµ‹è¯•è¯„ä¼°å¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "print(\"âœ“ æµ‹è¯•è¯„ä¼°å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== è®­ç»ƒç»“æœå¯è§†åŒ– ==========\n",
    "\n",
    "print(\"ç”Ÿæˆè®­ç»ƒç»“æœå¯è§†åŒ–...\")\n",
    "\n",
    "try:\n",
    "    # è®¾ç½®matplotlibå‚æ•°\n",
    "    plt.rcParams['figure.figsize'] = (15, 10)\n",
    "    plt.rcParams['font.size'] = 12\n",
    "    \n",
    "    # åˆ›å»ºå­å›¾\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'SHLæ•°æ®é›†è®­ç»ƒç»“æœ - {dataset_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. è®­ç»ƒå’ŒéªŒè¯æŸå¤±\n",
    "    axes[0, 0].plot(train_losses, label='è®­ç»ƒæŸå¤±', color='blue', linewidth=2)\n",
    "    axes[0, 0].plot(val_losses, label='éªŒè¯æŸå¤±', color='red', linewidth=2)\n",
    "    axes[0, 0].set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('æŸå¤±')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡\n",
    "    axes[0, 1].plot(train_accs, label='è®­ç»ƒå‡†ç¡®ç‡', color='blue', linewidth=2)\n",
    "    axes[0, 1].plot(val_accs, label='éªŒè¯å‡†ç¡®ç‡', color='red', linewidth=2)\n",
    "    axes[0, 1].set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('å‡†ç¡®ç‡ (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. éªŒè¯F1åˆ†æ•°\n",
    "    axes[1, 0].plot(val_f1s, label='éªŒè¯F1åˆ†æ•°', color='green', linewidth=2)\n",
    "    axes[1, 0].set_title('éªŒè¯F1åˆ†æ•°')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('F1åˆ†æ•° (%)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. æ··æ·†çŸ©é˜µçƒ­å›¾\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('æ··æ·†çŸ©é˜µ')\n",
    "    axes[1, 1].set_ylabel('çœŸå®æ ‡ç­¾')\n",
    "    axes[1, 1].set_xlabel('é¢„æµ‹æ ‡ç­¾')\n",
    "    plt.setp(axes[1, 1].get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # è°ƒæ•´å¸ƒå±€\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    \n",
    "    # ä¿å­˜å›¾ç‰‡\n",
    "    plt.savefig(output_dir / 'training_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(output_dir / 'training_results.pdf', bbox_inches='tight')\n",
    "    \n",
    "    print(\"âœ“ è®­ç»ƒç»“æœå¯è§†åŒ–å·²ä¿å­˜\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ç”Ÿæˆç±»åˆ«å‡†ç¡®ç‡æ¡å½¢å›¾\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    class_accuracies = []\n",
    "    for i in range(num_classes):\n",
    "        class_mask = np.array(all_test_targets) == i\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_acc = np.mean(np.array(all_test_preds)[class_mask] == i) * 100\n",
    "            class_accuracies.append(class_acc)\n",
    "        else:\n",
    "            class_accuracies.append(0)\n",
    "    \n",
    "    bars = plt.bar(range(num_classes), class_accuracies, \n",
    "                   color=plt.cm.Set3(np.linspace(0, 1, num_classes)))\n",
    "    plt.title('å„æ´»åŠ¨ç±»åˆ«æµ‹è¯•å‡†ç¡®ç‡', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('æ´»åŠ¨ç±»åˆ«')\n",
    "    plt.ylabel('å‡†ç¡®ç‡ (%)')\n",
    "    plt.xticks(range(num_classes), class_names, rotation=45, ha='right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.ylim(0, 110)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'class_accuracies.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(output_dir / 'class_accuracies.pdf', bbox_inches='tight')\n",
    "    \n",
    "    print(\"âœ“ ç±»åˆ«å‡†ç¡®ç‡å›¾å·²ä¿å­˜\")\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"âœ“ è®­ç»ƒç»“æœå¯è§†åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== è®­ç»ƒæ€»ç»“å’Œç»“æœæŠ¥å‘Š ==========\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ‰ SHLæ•°æ®é›†è®­ç»ƒæµç¨‹å®Œæˆ!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# === è®­ç»ƒé…ç½®æ€»ç»“ ===\n",
    "print(f\"\\nğŸ“‹ è®­ç»ƒé…ç½®æ‘˜è¦:\")\n",
    "print(f\"  æ•°æ®é›†: {dataset_name}\")\n",
    "print(f\"  æ•°æ®è·¯å¾„: {SHL_DATA_PATH}\")\n",
    "print(f\"  è®­ç»ƒæ¨¡å¼: {'é…ç½®é©±åŠ¨' if USE_CONFIG_MODE else 'ä¼ ç»Ÿæ¨¡å¼'}\")\n",
    "print(f\"  è®¾å¤‡: {device}\")\n",
    "print(f\"  éšæœºç§å­: {random_seed}\")\n",
    "\n",
    "# === æ•°æ®æ‘˜è¦ ===\n",
    "print(f\"\\nğŸ“Š æ•°æ®æ‘˜è¦:\")\n",
    "print(f\"  è®­ç»ƒæ ·æœ¬: {central_train_data.shape[0]:,}\")\n",
    "print(f\"  éªŒè¯æ ·æœ¬: {central_dev_data.shape[0]:,}\")\n",
    "print(f\"  æµ‹è¯•æ ·æœ¬: {central_test_data.shape[0]:,}\")\n",
    "print(f\"  ç‰¹å¾ç»´åº¦: {central_train_data.shape[1]:,}\")\n",
    "print(f\"  æ´»åŠ¨ç±»åˆ«: {activity_count}ä¸ª\")\n",
    "print(f\"  æ•°æ®ç±»å‹: {central_train_data.dtype}\")\n",
    "\n",
    "# === æ¨¡å‹é…ç½®æ‘˜è¦ ===\n",
    "print(f\"\\nğŸ§  æ¨¡å‹é…ç½®æ‘˜è¦:\")\n",
    "print(f\"  æ¨¡å‹ç±»å‹: CBranchformer\")\n",
    "print(f\"  å‚æ•°é‡: {sum(p.numel() for p in har_model.parameters()):,}\")\n",
    "print(f\"  æŠ•å½±ç»´åº¦: {model_config['projection_dim']}\")\n",
    "print(f\"  æ³¨æ„åŠ›å¤´æ•°: {model_config['num_heads']}\")\n",
    "print(f\"  ç½‘ç»œå±‚æ•°: {model_config['num_layers']}\")\n",
    "print(f\"  è¡¥ä¸å¤§å°: {model_config['patch_size']}\")\n",
    "print(f\"  æ—¶é—´æ­¥é•¿: {model_config['time_step']}\")\n",
    "print(f\"  å·ç§¯æ ¸å¤§å°: {model_config['conv_kernel_size']}\")\n",
    "print(f\"  Dropoutç‡: {model_config['dropout_rate']}\")\n",
    "\n",
    "# === è®­ç»ƒè¶…å‚æ•°æ‘˜è¦ ===\n",
    "print(f\"\\nâš™ï¸ è®­ç»ƒè¶…å‚æ•°æ‘˜è¦:\")\n",
    "print(f\"  æ‰¹æ¬¡å¤§å°: {batch_size}\")\n",
    "print(f\"  å­¦ä¹ ç‡: {learning_rate}\")\n",
    "print(f\"  è®­ç»ƒè½®æ•°: {epochs} (å®é™…: {len(train_losses)})\")\n",
    "print(f\"  æƒé‡è¡°å‡: {weight_decay}\")\n",
    "print(f\"  æ¢¯åº¦è£å‰ª: {gradient_clip_norm}\")\n",
    "print(f\"  æ—©åœè€å¿ƒ: {patience}\")\n",
    "print(f\"  ä¼˜åŒ–å™¨: Adam\")\n",
    "print(f\"  è°ƒåº¦å™¨: CosineAnnealingLR\")\n",
    "print(f\"  æŸå¤±å‡½æ•°: åŠ æƒäº¤å‰ç†µ\")\n",
    "\n",
    "# === è®­ç»ƒç»“æœæ‘˜è¦ ===\n",
    "print(f\"\\nğŸ† è®­ç»ƒç»“æœæ‘˜è¦:\")\n",
    "print(f\"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%\")\n",
    "print(f\"  æœ€ä½³éªŒè¯F1åˆ†æ•°: {best_val_f1:.2f}%\")\n",
    "print(f\"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%\")\n",
    "print(f\"  æµ‹è¯•F1åˆ†æ•°: {test_f1:.2f}%\")\n",
    "print(f\"  æµ‹è¯•æŸå¤±: {test_loss_avg:.4f}\")\n",
    "\n",
    "# === æ€§èƒ½åˆ†æ ===\n",
    "print(f\"\\nğŸ“ˆ æ€§èƒ½åˆ†æ:\")\n",
    "if test_acc >= 90:\n",
    "    print(f\"  ğŸŸ¢ ä¼˜ç§€: æµ‹è¯•å‡†ç¡®ç‡è¾¾åˆ° {test_acc:.2f}%\")\n",
    "elif test_acc >= 80:\n",
    "    print(f\"  ğŸŸ¡ è‰¯å¥½: æµ‹è¯•å‡†ç¡®ç‡ä¸º {test_acc:.2f}%\")\n",
    "elif test_acc >= 70:\n",
    "    print(f\"  ğŸŸ  ä¸€èˆ¬: æµ‹è¯•å‡†ç¡®ç‡ä¸º {test_acc:.2f}%\")\n",
    "else:\n",
    "    print(f\"  ğŸ”´ å¾…æ”¹è¿›: æµ‹è¯•å‡†ç¡®ç‡ä»…ä¸º {test_acc:.2f}%\")\n",
    "\n",
    "# è¿‡æ‹Ÿåˆæ£€æŸ¥\n",
    "train_val_gap = train_accs[-1] - val_accs[-1]\n",
    "if train_val_gap > 10:\n",
    "    print(f\"  âš ï¸ å¯èƒ½è¿‡æ‹Ÿåˆ: è®­ç»ƒå‡†ç¡®ç‡æ¯”éªŒè¯å‡†ç¡®ç‡é«˜ {train_val_gap:.1f}%\")\n",
    "    print(f\"     å»ºè®®: å¢åŠ dropoutç‡ã€å‡å°‘æ¨¡å‹complexityæˆ–å¢åŠ æ•°æ®\")\n",
    "elif train_val_gap < -5:\n",
    "    print(f\"  âš ï¸ å¯èƒ½æ¬ æ‹Ÿåˆ: éªŒè¯å‡†ç¡®ç‡æ¯”è®­ç»ƒå‡†ç¡®ç‡é«˜ {abs(train_val_gap):.1f}%\")\n",
    "    print(f\"     å»ºè®®: å¢åŠ æ¨¡å‹å¤æ‚åº¦æˆ–å‡å°‘æ­£åˆ™åŒ–\")\n",
    "else:\n",
    "    print(f\"  âœ… æ‹Ÿåˆè‰¯å¥½: è®­ç»ƒéªŒè¯å‡†ç¡®ç‡å·®è·ä¸º {train_val_gap:.1f}%\")\n",
    "\n",
    "# === æœ€ä½³è¡¨ç°ç±»åˆ«åˆ†æ ===\n",
    "print(f\"\\nğŸ¯ å„ç±»åˆ«æ€§èƒ½åˆ†æ:\")\n",
    "class_perfs = []\n",
    "for i in range(num_classes):\n",
    "    class_mask = np.array(all_test_targets) == i\n",
    "    if np.sum(class_mask) > 0:\n",
    "        class_acc = np.mean(np.array(all_test_preds)[class_mask] == i) * 100\n",
    "        activity_name = ACTIVITY_LABEL[i] if i < len(ACTIVITY_LABEL) else f\"ç±»åˆ«{i}\"\n",
    "        class_perfs.append((activity_name, class_acc, np.sum(class_mask)))\n",
    "\n",
    "# æŒ‰å‡†ç¡®ç‡æ’åº\n",
    "class_perfs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"  æœ€ä½³è¡¨ç°: {class_perfs[0][0]} ({class_perfs[0][1]:.1f}%)\")\n",
    "print(f\"  æœ€å·®è¡¨ç°: {class_perfs[-1][0]} ({class_perfs[-1][1]:.1f}%)\")\n",
    "print(f\"  å¹³å‡å‡†ç¡®ç‡: {np.mean([perf[1] for perf in class_perfs]):.1f}%\")\n",
    "\n",
    "# === æ–‡ä»¶è¾“å‡ºæ‘˜è¦ ===\n",
    "print(f\"\\nğŸ“ è¾“å‡ºæ–‡ä»¶æ‘˜è¦:\")\n",
    "output_files = [\n",
    "    'best_model.pth',\n",
    "    'training_results.json', \n",
    "    'classification_report.txt',\n",
    "    'confusion_matrix.npy',\n",
    "    'training_results.png',\n",
    "    'training_results.pdf',\n",
    "    'class_accuracies.png',\n",
    "    'class_accuracies.pdf'\n",
    "]\n",
    "\n",
    "for file in output_files:\n",
    "    file_path = output_dir / file\n",
    "    if file_path.exists():\n",
    "        file_size = file_path.stat().st_size / 1024  # KB\n",
    "        print(f\"  âœ… {file} ({file_size:.1f}KB)\")\n",
    "    else:\n",
    "        print(f\"  âŒ {file} (æœªç”Ÿæˆ)\")\n",
    "\n",
    "print(f\"\\n  ğŸ“‚ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}\")\n",
    "\n",
    "# === æ”¹è¿›å»ºè®® ===\n",
    "print(f\"\\nğŸ’¡ æ”¹è¿›å»ºè®®:\")\n",
    "if test_acc < 85:\n",
    "    print(f\"  â€¢ å°è¯•å¢åŠ è®­ç»ƒè½®æ•°æˆ–è°ƒæ•´å­¦ä¹ ç‡\")\n",
    "    print(f\"  â€¢ å°è¯•ä¸åŒçš„æ¨¡å‹æ¶æ„å‚æ•°ç»„åˆ\")\n",
    "    print(f\"  â€¢ æ£€æŸ¥æ•°æ®è´¨é‡å’Œæ ‡ç­¾å‡†ç¡®æ€§\")\n",
    "    print(f\"  â€¢ è€ƒè™‘æ•°æ®å¢å¼ºæŠ€æœ¯\")\n",
    "\n",
    "if len(train_losses) == epochs:\n",
    "    print(f\"  â€¢ è®­ç»ƒå¯èƒ½æœªæ”¶æ•›ï¼Œè€ƒè™‘å¢åŠ è®­ç»ƒè½®æ•°\")\n",
    "\n",
    "if total_size_mb > 500:\n",
    "    print(f\"  â€¢ æ•°æ®é›†è¾ƒå¤§ï¼Œè€ƒè™‘ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡å¤§å°ä»¥æé«˜æ•ˆç‡\")\n",
    "\n",
    "print(f\"\\nğŸ”„ ä¸‹æ¬¡è®­ç»ƒæ—¶ï¼Œåªéœ€ä¿®æ”¹ç¬¬ä¸€ä¸ªé…ç½®cellä¸­çš„å‚æ•°å³å¯!\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"è®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆ! ğŸ‰\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HAR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
