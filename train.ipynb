{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MazeruHAR åŠ¨æ€è®­ç»ƒå¼•æ“\n",
    "\n",
    "æ¬¢è¿ä½¿ç”¨ MazeruHAR çš„é…ç½®é©±åŠ¨è®­ç»ƒå¼•æ“ã€‚æ­¤ Notebook æ—¨åœ¨æä¾›ä¸€ä¸ªçµæ´»ã€å¯é‡ç°ä¸”æ˜“äºä½¿ç”¨çš„è®­ç»ƒæµç¨‹ã€‚\n",
    "\n",
    "**æ ¸å¿ƒç†å¿µ:** **ä»£ç åªå†™ä¸€æ¬¡ï¼Œå®éªŒé…ç½®ä¸‡åƒã€‚**\n",
    "\n",
    "æ‚¨åªéœ€è¦æ‰§è¡Œä»¥ä¸‹ä¸¤ä¸ªç®€å•æ­¥éª¤å³å¯å¼€å§‹è®­ç»ƒï¼š\n",
    "\n",
    "1.  **é…ç½®å®éªŒ**: ä¿®æ”¹ä½äº `config/` ç›®å½•ä¸‹çš„ `.yaml` é…ç½®æ–‡ä»¶ã€‚æ‚¨å¯ä»¥å¤åˆ¶ `config/default_configs/shl_config.yaml` å¹¶æ ¹æ®æ‚¨çš„éœ€æ±‚è¿›è¡Œè°ƒæ•´ï¼Œæ¯”å¦‚æ›´æ¢æ•°æ®é›†ã€æ¨¡å‹æ¶æ„æˆ–è¶…å‚æ•°ã€‚\n",
    "2.  **è¿è¡Œ Notebook**: åœ¨ä¸‹é¢çš„ **â€œå®éªŒé…ç½®â€**å•å…ƒæ ¼ä¸­è®¾ç½®å¥½é…ç½®æ–‡ä»¶çš„è·¯å¾„ï¼Œç„¶åä»å¤´åˆ°å°¾è¿è¡Œæ­¤ Notebook å³å¯ã€‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 1: ç¯å¢ƒè®¾ç½®ä¸åº“å¯¼å…¥\n",
    "\n",
    "æ­¤å•å…ƒæ ¼è´Ÿè´£å¯¼å…¥æ‰€æœ‰å¿…éœ€çš„åº“å¹¶è®¾ç½®åˆå§‹ç¯å¢ƒã€‚å®ƒæ•´åˆäº†é¡¹ç›®æ‰€éœ€çš„æ‰€æœ‰ä¾èµ–é¡¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸä»é¡¹ç›®æ–‡ä»¶å¯¼å…¥æ¨¡å—ã€‚\n",
      "æ‰€æœ‰æ¨¡å—å·²å‡†å¤‡å°±ç»ªã€‚\n"
     ]
    }
   ],
   "source": [
    "# æ ‡å‡†åº“å¯¼å…¥\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import random\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR, ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- é¡¹ç›®å†…éƒ¨æ¨¡å—å¯¼å…¥ ---\n",
    "# ä¸ºäº†ç¡®ä¿ Notebook çš„å¯ç§»æ¤æ€§ï¼Œæˆ‘ä»¬å°†å…³é”®çš„æ¨¡å—ä»£ç ç›´æ¥åŒ…å«è¿›æ¥ï¼Œ\n",
    "# åŒæ—¶ä¿ç•™ä»æ–‡ä»¶å¯¼å…¥çš„é€»è¾‘ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆã€‚\n",
    "try:\n",
    "    # ä¼˜å…ˆä»æ–‡ä»¶ç³»ç»Ÿå¯¼å…¥\n",
    "    from config.config_loader import ConfigLoader\n",
    "    from model_layer.dynamic_har_model import create_dynamic_har_model\n",
    "    import utils_torch as utils\n",
    "    print(\"æˆåŠŸä»é¡¹ç›®æ–‡ä»¶å¯¼å…¥æ¨¡å—ã€‚\")\n",
    "except ImportError as e:\n",
    "    print(f\"ä»æ–‡ä»¶å¯¼å…¥æ¨¡å—å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨ Notebook å†…è”å®šä¹‰ã€‚\")\n",
    "    \n",
    "    # ========================== Fallback Logic: Start ==========================\n",
    "    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œå®šä¹‰å®Œæ•´çš„åå¤‡ï¼ˆFallbackï¼‰å®ç°ï¼Œä»¥ä¿è¯ Notebook èƒ½ç‹¬ç«‹è¿è¡Œã€‚\n",
    "\n",
    "    # 1. Fallback for 'HARDataset'\n",
    "    class HARDataset(Dataset):\n",
    "        \"\"\"ä¸€ä¸ªæ ‡å‡†çš„ PyTorch æ•°æ®é›†ç±»ã€‚\"\"\"\n",
    "        def __init__(self, data: torch.Tensor, labels: torch.Tensor):\n",
    "            self.data = data\n",
    "            self.labels = labels\n",
    "        def __len__(self) -> int:\n",
    "            return len(self.data)\n",
    "        def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "            return self.data[idx], self.labels[idx]\n",
    "\n",
    "    # 2. Fallback for 'create_dynamic_har_model'\n",
    "    def create_dynamic_har_model(config: Dict[str, Any]) -> nn.Module:\n",
    "        \"\"\"\n",
    "        åˆ›å»ºä¸€ä¸ªç®€å•çš„å ä½ç¬¦CNNæ¨¡å‹ç”¨äºHARã€‚\n",
    "        æ­¤å‡½æ•°ä½œä¸ºå®é™…åŠ¨æ€æ¨¡å‹å·¥å‚çš„æ›¿ä»£å“ã€‚\n",
    "        \"\"\"\n",
    "        dataset_params = config.get('dataset', {})\n",
    "        # ä»é…ç½®ä¸­è·å–æ¨¡å‹å‚æ•°ï¼Œæˆ–ä½¿ç”¨é»˜è®¤å€¼\n",
    "        n_features = dataset_params.get('n_features', 9)      # e.g., 9-axis sensor data\n",
    "        num_classes = dataset_params.get('num_classes', 8)    # e.g., 8 activity classes\n",
    "        seq_len = dataset_params.get('window_size', 128)      # Sequence length\n",
    "\n",
    "        class SimpleHAR_CNN(nn.Module):\n",
    "            def __init__(self, in_channels: int, num_classes: int, sequence_length: int):\n",
    "                super().__init__()\n",
    "                # è¾“å…¥å½¢çŠ¶: (batch, features, sequence_len)ï¼Œä¾‹å¦‚ (B, 9, 128)\n",
    "                self.conv1 = nn.Conv1d(in_channels, 32, kernel_size=5, stride=1, padding=2)\n",
    "                self.relu1 = nn.ReLU()\n",
    "                self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "                \n",
    "                self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "                self.relu2 = nn.ReLU()\n",
    "                self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "                \n",
    "                # ç»è¿‡ä¸¤æ¬¡æ­¥é•¿ä¸º2çš„æœ€å¤§æ± åŒ–åï¼Œåºåˆ—é•¿åº¦å‡åŠä¸¤æ¬¡ï¼šsequence_length / 4\n",
    "                flattened_size = 64 * (sequence_length // 4)\n",
    "                self.flatten = nn.Flatten()\n",
    "                self.fc1 = nn.Linear(flattened_size, 128)\n",
    "                self.relu3 = nn.ReLU()\n",
    "                self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "            def forward(self, data_dict: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "                # å‡è®¾æ‰€æœ‰è¾“å…¥æ•°æ®éƒ½åœ¨ 'imu' è¿™ä¸ªé”®ä¸‹\n",
    "                x = data_dict['imu']  # é¢„æœŸå½¢çŠ¶: (Batch, Length, Features), e.g., (B, 128, 9)\n",
    "                x = x.permute(0, 2, 1) # è½¬æ¢ä¸º: (Batch, Features, Length), e.g., (B, 9, 128)\n",
    "                \n",
    "                x = self.pool1(self.relu1(self.conv1(x)))\n",
    "                x = self.pool2(self.relu2(self.conv2(x)))\n",
    "                x = self.flatten(x)\n",
    "                x = self.relu3(self.fc1(x))\n",
    "                x = self.fc2(x)\n",
    "                return x\n",
    "        \n",
    "        print(f\"åˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„CNNå ä½æ¨¡å‹ (è¾“å…¥ç‰¹å¾: {n_features}, ç±»åˆ«æ•°: {num_classes})\")\n",
    "        return SimpleHAR_CNN(n_features, num_classes, seq_len)\n",
    "\n",
    "    # 3. Fallback for 'utils.load_dataset_pytorch'\n",
    "    class MockDatasetLoader:\n",
    "        \"\"\"ä¸€ä¸ªæ¨¡æ‹Ÿçš„æ•°æ®åŠ è½½å™¨ï¼Œç”¨äºç”Ÿæˆåˆæˆæ•°æ®ã€‚\"\"\"\n",
    "        def __init__(self, config: Dict[str, Any], seed: int):\n",
    "            print(\"æ­£åœ¨ç”Ÿæˆæ¨¡æ‹ŸHARæ•°æ®...\")\n",
    "            # ä»é…ç½®ä¸­è·å–æ•°æ®å‚æ•°\n",
    "            n_features = config.get('n_features', 9)\n",
    "            seq_len = config.get('window_size', 128)\n",
    "            num_classes = config.get('num_classes', 8)\n",
    "            \n",
    "            # è®¾ç½®æ ·æœ¬æ•°é‡\n",
    "            n_train_samples = 2000\n",
    "            n_test_samples = 500\n",
    "            \n",
    "            # ä½¿ç”¨torchç”Ÿæˆéšæœºæ•°æ®\n",
    "            self.central_train_data = torch.randn(n_train_samples, seq_len, n_features)\n",
    "            self.central_train_label = torch.randint(0, num_classes, (n_train_samples,))\n",
    "            self.central_test_data = torch.randn(n_test_samples, seq_len, n_features)\n",
    "            self.central_test_label = torch.randint(0, num_classes, (n_test_samples,))\n",
    "            print(\"æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå®Œæ¯•ã€‚\")\n",
    "\n",
    "    def load_dataset_pytorch(dataset_name: str, batch_size: int, type: str, seed: int, data_path: str, config: Dict[str, Any]):\n",
    "        \"\"\"åŠ è½½æ•°æ®é›†çš„åå¤‡å‡½æ•°ï¼Œè¿”å›ä¸€ä¸ªåŒ…å«åˆæˆæ•°æ®çš„åŠ è½½å™¨å¯¹è±¡ã€‚\"\"\"\n",
    "        # æ³¨æ„ï¼šè¿™é‡Œçš„å‚æ•°æ˜¯ä¸ºäº†ä¸åŸå§‹è°ƒç”¨ç­¾åä¿æŒä¸€è‡´\n",
    "        return MockDatasetLoader(config.get('dataset', {}), seed)\n",
    "\n",
    "    # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„ 'utils' æ¨¡å—ï¼Œå¹¶å°†åå¤‡å‡½æ•°/ç±»é™„åŠ åˆ°å®ƒä¸Šé¢\n",
    "    utils = type('utils', (), {\n",
    "        'HARDataset': HARDataset,\n",
    "        'load_dataset_pytorch': load_dataset_pytorch\n",
    "    })\n",
    "    \n",
    "    # =========================== Fallback Logic: End ===========================\n",
    "\n",
    "print(\"æ‰€æœ‰æ¨¡å—å·²å‡†å¤‡å°±ç»ªã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 2: å®éªŒé…ç½®\n",
    "\n",
    "**è¿™æ˜¯æ‚¨å”¯ä¸€éœ€è¦ä¿®æ”¹çš„å•å…ƒæ ¼ã€‚**\n",
    "\n",
    "è¯·å°† `CONFIG_PATH`å˜é‡è®¾ç½®ä¸ºæ‚¨æƒ³è¦ä½¿ç”¨çš„é…ç½®æ–‡ä»¶çš„è·¯å¾„ã€‚æ‰€æœ‰å®éªŒå‚æ•°éƒ½å°†ä»æ­¤æ–‡ä»¶åŠ è½½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ å°†ä½¿ç”¨é…ç½®æ–‡ä»¶: 'config/default_configs/shl_config.yaml'\n"
     ]
    }
   ],
   "source": [
    "# ================== æ ¸å¿ƒé…ç½® ==================\n",
    "# ğŸ”¥ åªéœ€ä¿®æ”¹æ­¤å¤„çš„é…ç½®æ–‡ä»¶è·¯å¾„å³å¯å¼€å§‹æ–°çš„å®éªŒ\n",
    "CONFIG_PATH = 'config/default_configs/shl_config.yaml'  # é»˜è®¤ä½¿ç”¨æ ¹ç›®å½•ä¸‹çš„config.yaml\n",
    "# ==============================================\n",
    "\n",
    "# æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "if not os.path.exists(CONFIG_PATH):\n",
    "    print(f\"âŒ é”™è¯¯: é…ç½®æ–‡ä»¶ '{CONFIG_PATH}' æœªæ‰¾åˆ°!\")\n",
    "    print(\"è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ï¼Œæˆ–åˆ›å»ºä¸€ä¸ªæ–°çš„é…ç½®æ–‡ä»¶ã€‚\")\n",
    "else:\n",
    "    print(f\"âœ“ å°†ä½¿ç”¨é…ç½®æ–‡ä»¶: '{CONFIG_PATH}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 3: æ ¸å¿ƒè®­ç»ƒå™¨ç±»\n",
    "\n",
    "ä¸‹é¢çš„ `ConfigurableTrainer` ç±»æ˜¯æ•´ä¸ªè®­ç»ƒæµç¨‹çš„æ ¸å¿ƒã€‚å®ƒå°è£…äº†ä»é…ç½®åŠ è½½ã€ç¯å¢ƒè®¾ç½®ã€æ•°æ®å¤„ç†ã€æ¨¡å‹åˆ›å»ºã€è®­ç»ƒã€è¯„ä¼°åˆ°ç»“æœå¯è§†åŒ–çš„æ‰€æœ‰é€»è¾‘ã€‚æ‚¨æ— éœ€ä¿®æ”¹æ­¤ç±»ä¸­çš„ä»»ä½•ä»£ç ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurableTrainer:\n",
    "    \"\"\"é…ç½®é©±åŠ¨çš„è®­ç»ƒå™¨ç±»\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str):\n",
    "        self.config_path = config_path\n",
    "        self.config = self._load_config(config_path)\n",
    "        self.params = self._extract_parameters()\n",
    "        self.device = None\n",
    "        self.output_dir = None\n",
    "        self.logger = None\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.criterion = None\n",
    "        self.history = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': [], 'val_f1': []}\n",
    "\n",
    "    def _load_config(self, path: str) -> Dict[str, Any]:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return yaml.safe_load(f)\n",
    "\n",
    "    def _extract_parameters(self) -> Dict[str, Any]:\n",
    "        \"\"\"å°†é…ç½®æ–‡ä»¶ä¸­çš„æ‰€æœ‰å±‚çº§å±•å¹³åˆ°ä¸€ä¸ªå­—å…¸ä¸­ä»¥ä¾¿äºè®¿é—®ã€‚\"\"\"\n",
    "        params = {}\n",
    "        for key, value in self.config.items():\n",
    "            if isinstance(value, dict):\n",
    "                params.update(value)\n",
    "            else:\n",
    "                params[key] = value\n",
    "        return params\n",
    "\n",
    "    def setup_environment(self):\n",
    "        \"\"\"è®¾ç½®éšæœºç§å­ã€è®¾å¤‡å’Œæ—¥å¿—è®°å½•\"\"\"\n",
    "        seed = self.params.get('seed', 42)\n",
    "        random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "        device_pref = self.params.get('device', 'auto')\n",
    "        if device_pref == 'auto':\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device_pref)\n",
    "        \n",
    "        self.output_dir = Path(self.params.get('output_dir', './results/default_experiment'))\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # è®¾ç½®æ—¥å¿—\n",
    "        logging.basicConfig(level=logging.INFO, \n",
    "                            format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "                            handlers=[logging.FileHandler(self.output_dir / 'training.log'), logging.StreamHandler(sys.stdout)])\n",
    "        self.logger = logging.getLogger()\n",
    "        self.logger.info(f\"ç¯å¢ƒè®¾ç½®å®Œæˆã€‚è®¾å¤‡: {self.device}, è¾“å‡ºç›®å½•: {self.output_dir}\")\n",
    "\n",
    "    def load_data(self) -> Tuple[DataLoader, DataLoader, DataLoader, np.ndarray]:\n",
    "        \"\"\"æ ¹æ®é…ç½®åŠ è½½å¹¶å‡†å¤‡æ•°æ®\"\"\"\n",
    "        self.logger.info(f\"åŠ è½½æ•°æ®é›†: {self.params['name']}\")\n",
    "        \n",
    "        # æ­¤å¤„è°ƒç”¨äº†åœ¨å•å…ƒæ ¼3ä¸­å®šä¹‰çš„åå¤‡ï¼ˆFallbackï¼‰æˆ–å®é™…çš„utilså‡½æ•°\n",
    "        dataset_loader = utils.load_dataset_pytorch(\n",
    "            self.params['name'], \n",
    "            self.params['batch_size'], \n",
    "            'BALANCED', # å‡è®¾ç±»å‹\n",
    "            self.params['seed'], \n",
    "            './datasets/', \n",
    "            self.config # ä¼ é€’å®Œæ•´é…ç½®ç»™åå¤‡å‡½æ•°\n",
    "        )\n",
    "        train_data, train_labels = dataset_loader.central_train_data, dataset_loader.central_train_label\n",
    "        test_data, test_labels = dataset_loader.central_test_data, dataset_loader.central_test_label\n",
    "        \n",
    "        # åˆ†å‰²éªŒè¯é›†\n",
    "        train_data, dev_data, train_labels, dev_labels = train_test_split(\n",
    "            train_data.numpy(), train_labels.numpy(), \n",
    "            test_size=0.15, random_state=self.params['seed'], stratify=train_labels.numpy()\n",
    "        )\n",
    "\n",
    "        # è½¬æ¢ä¸ºTensor\n",
    "        train_dataset = utils.HARDataset(torch.FloatTensor(train_data), torch.LongTensor(train_labels))\n",
    "        dev_dataset = utils.HARDataset(torch.FloatTensor(dev_data), torch.LongTensor(dev_labels))\n",
    "        test_dataset = utils.HARDataset(torch.FloatTensor(test_data), torch.LongTensor(test_labels))\n",
    "\n",
    "        # åˆ›å»ºDataLoaders\n",
    "        common_params = {'batch_size': self.params['batch_size'], 'num_workers': 0, 'pin_memory': True}\n",
    "        train_loader = DataLoader(train_dataset, shuffle=True, **common_params)\n",
    "        dev_loader = DataLoader(dev_dataset, shuffle=False, **common_params)\n",
    "        test_loader = DataLoader(test_dataset, shuffle=False, **common_params)\n",
    "\n",
    "        self.logger.info(\"æ•°æ®åŠ è½½å’Œåˆ†å‰²å®Œæˆã€‚\")\n",
    "        return train_loader, dev_loader, test_loader, train_labels\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"æ ¹æ®é…ç½®æ„å»ºæ¨¡å‹\"\"\"\n",
    "        self.logger.info(\"æ„å»ºæ¨¡å‹...\")\n",
    "        # ä½¿ç”¨åœ¨å•å…ƒæ ¼3ä¸­å®šä¹‰çš„åå¤‡ï¼ˆFallbackï¼‰æˆ–å®é™…çš„å·¥å‚å‡½æ•°\n",
    "        self.model = create_dynamic_har_model(self.config).to(self.device)\n",
    "        self.logger.info(f\"æ¨¡å‹æ„å»ºå®Œæˆã€‚æ€»å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}\")\n",
    "        \n",
    "    def setup_training_components(self, train_labels: np.ndarray):\n",
    "        \"\"\"è®¾ç½®ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°å’Œè°ƒåº¦å™¨\"\"\"\n",
    "        weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "        class_weights = torch.FloatTensor(weights).to(self.device)\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=self.params.get('label_smoothing', 0.0))\n",
    "\n",
    "        opt_name = self.params.get('optimizer', 'adamw').lower()\n",
    "        opt_map = {'adam': optim.Adam, 'adamw': optim.AdamW, 'sgd': optim.SGD}\n",
    "        self.optimizer = opt_map[opt_name](self.model.parameters(), lr=self.params['learning_rate'], weight_decay=self.params.get('weight_decay', 1e-4))\n",
    "\n",
    "        scheduler_name = self.params.get('scheduler', 'cosine')\n",
    "        if scheduler_name == 'cosine':\n",
    "            self.scheduler = CosineAnnealingLR(self.optimizer, T_max=self.params['epochs'])\n",
    "        elif scheduler_name == 'step':\n",
    "            self.scheduler = StepLR(self.optimizer, step_size=30, gamma=0.1)\n",
    "        else: \n",
    "            self.scheduler = None\n",
    "        self.logger.info(\"è®­ç»ƒç»„ä»¶è®¾ç½®å®Œæˆã€‚\")\n",
    "\n",
    "    def train(self, train_loader, dev_loader):\n",
    "        self.logger.info(\"--- å¼€å§‹è®­ç»ƒ ---\")\n",
    "        best_val_f1 = 0.0\n",
    "        patience_counter = 0\n",
    "        patience = self.params.get('early_stopping_patience', 10)\n",
    "\n",
    "        for epoch in range(self.params['epochs']):\n",
    "            self.model.train()\n",
    "            train_loss, train_correct, train_total = 0, 0, 0\n",
    "            for data, targets in train_loader:\n",
    "                # åŠ¨æ€æ„å»ºè¾“å…¥å­—å…¸ï¼Œè¿™é‡Œå‡è®¾æ‰€æœ‰æ•°æ®éƒ½æ˜¯ 'imu' æ¨¡æ€\n",
    "                data_dict = {'imu': data}\n",
    "                data_dict = {k: v.to(self.device) for k, v in data_dict.items()}\n",
    "                targets = targets.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(data_dict)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                if self.params.get('gradient_clip_norm', 0) > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.params['gradient_clip_norm'])\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                train_total += targets.size(0)\n",
    "                train_correct += (predicted == targets).sum().item()\n",
    "\n",
    "            if self.scheduler: self.scheduler.step()\n",
    "\n",
    "            # éªŒè¯\n",
    "            val_loss, val_f1, val_acc = self.evaluate(dev_loader, is_test=False)\n",
    "            train_acc = train_correct / train_total\n",
    "            self.logger.info(f\"Epoch {epoch+1}/{self.params['epochs']} | Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "            \n",
    "            # ä¿å­˜å†å²è®°å½•\n",
    "            self.history['train_loss'].append(train_loss/len(train_loader))\n",
    "            self.history['train_accuracy'].append(train_acc)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_accuracy'].append(val_acc)\n",
    "            self.history['val_f1'].append(val_f1)\n",
    "\n",
    "            # æ—©åœä¸æ¨¡å‹ä¿å­˜\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                patience_counter = 0\n",
    "                if self.params.get('save_checkpoints', True):\n",
    "                    torch.save(self.model.state_dict(), self.output_dir / 'best_model.pth')\n",
    "                    self.logger.info(f\"æ–°æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼ŒéªŒè¯F1åˆ†æ•°: {best_val_f1:.4f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                self.logger.info(f\"æ—©åœè§¦å‘äº epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    def evaluate(self, data_loader, is_test=True):\n",
    "        self.model.eval()\n",
    "        total_loss, total_correct, total_samples = 0, 0, 0\n",
    "        all_preds, all_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for data, targets in data_loader:\n",
    "                data_dict = {'imu': data}\n",
    "                data_dict = {k: v.to(self.device) for k, v in data_dict.items()}\n",
    "                targets = targets.to(self.device)\n",
    "\n",
    "                outputs = self.model(data_dict)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_samples += targets.size(0)\n",
    "                total_correct += (predicted == targets).sum().item()\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        accuracy = total_correct / total_samples\n",
    "        f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "\n",
    "        if is_test:\n",
    "            self.logger.info(f\"--- æµ‹è¯•ç»“æœ ---\")\n",
    "            self.logger.info(f\"æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}\")\n",
    "            self.logger.info(f\"æµ‹è¯•F1åˆ†æ•° (åŠ æƒ): {f1:.4f}\")\n",
    "            report = classification_report(all_targets, all_preds, target_names=self.params['activity_labels'], zero_division=0)\n",
    "            self.logger.info(f\"åˆ†ç±»æŠ¥å‘Š:\\n{report}\")\n",
    "            if self.params.get('plot_confusion_matrix', True):\n",
    "                self.plot_confusion_matrix(all_targets, all_preds, self.params['activity_labels'])\n",
    "        \n",
    "        return avg_loss, f1, accuracy\n",
    "\n",
    "    def plot_learning_curves(self):\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history['train_loss'], label='Train Loss')\n",
    "        plt.plot(self.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Loss vs. Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.history['train_accuracy'], label='Train Accuracy')\n",
    "        plt.plot(self.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Accuracy vs. Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_path = self.output_dir / 'learning_curves.png'\n",
    "        plt.savefig(save_path)\n",
    "        self.logger.info(f\"å­¦ä¹ æ›²çº¿å·²ä¿å­˜è‡³ {save_path}\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred, labels):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        save_path = self.output_dir / 'confusion_matrix.png'\n",
    "        plt.savefig(save_path)\n",
    "        self.logger.info(f\"æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³ {save_path}\")\n",
    "        plt.close() # å…³é—­å›¾åƒï¼Œé˜²æ­¢åœ¨notebookä¸­é‡å¤æ˜¾ç¤º\n",
    "        \n",
    "    def run(self):\n",
    "        self.setup_environment()\n",
    "        train_loader, dev_loader, test_loader, train_labels = self.load_data()\n",
    "        self.build_model()\n",
    "        self.setup_training_components(train_labels)\n",
    "        self.train(train_loader, dev_loader)\n",
    "        \n",
    "        # æ£€æŸ¥æœ€ä½³æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "        best_model_path = self.output_dir / 'best_model.pth'\n",
    "        if os.path.exists(best_model_path):\n",
    "            self.logger.info(\"åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°...\")\n",
    "            self.model.load_state_dict(torch.load(best_model_path))\n",
    "            self.evaluate(test_loader)\n",
    "        else:\n",
    "            self.logger.warning(\"æœªæ‰¾åˆ°ä¿å­˜çš„æœ€ä½³æ¨¡å‹ï¼Œå°†ä½¿ç”¨å½“å‰æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚\")\n",
    "            self.evaluate(test_loader)\n",
    "\n",
    "        if self.params.get('plot_learning_curves', True):\n",
    "            self.plot_learning_curves()\n",
    "        self.logger.info(\"è®­ç»ƒæµç¨‹å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 4: æ‰§è¡Œè®­ç»ƒæµç¨‹\n",
    "\n",
    "æœ€åï¼Œæˆ‘ä»¬å®ä¾‹åŒ– `ConfigurableTrainer` ç±»å¹¶è°ƒç”¨å…¶ `run` æ–¹æ³•æ¥å¯åŠ¨æ•´ä¸ªè®­ç»ƒå’Œè¯„ä¼°æµç¨‹ã€‚æ‰€æœ‰æ“ä½œéƒ½å°†ç”±ä¹‹å‰åŠ è½½çš„é…ç½®é©±åŠ¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 18:28:36,460 [INFO] - ç¯å¢ƒè®¾ç½®å®Œæˆã€‚è®¾å¤‡: cpu, è¾“å‡ºç›®å½•: results/shl_multimodal_run\n",
      "2025-07-25 18:28:36,460 [INFO] - åŠ è½½æ•°æ®é›†: shl\n",
      "2025-07-25 18:28:36,576 [ERROR] - è®­ç»ƒæµç¨‹ä¸­å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/zilongzeng/Research/MazeruHAR/utils_torch.py\", line 431, in load_dataset_pytorch\n",
      "    client_data.append(pickle.load(open(main_dir + 'datasetStandardized/' + dataset_name + '/UserData' + str(i) + '.pkl', 'rb')))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './datasets/datasetStandardized/shl/UserData0.pkl'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/fr/14dg9d1n4vx9sqmyh_9pqm3h0000gn/T/ipykernel_71802/2785309581.py\", line 4, in <module>\n",
      "    trainer.run()\n",
      "  File \"/var/folders/fr/14dg9d1n4vx9sqmyh_9pqm3h0000gn/T/ipykernel_71802/723311530.py\", line 245, in run\n",
      "    train_loader, dev_loader, test_loader, train_labels = self.load_data()\n",
      "  File \"/var/folders/fr/14dg9d1n4vx9sqmyh_9pqm3h0000gn/T/ipykernel_71802/723311530.py\", line 59, in load_data\n",
      "    dataset_loader = utils.load_dataset_pytorch(\n",
      "  File \"/Users/zilongzeng/Research/MazeruHAR/utils_torch.py\", line 435, in load_dataset_pytorch\n",
      "    client_data.append(hkl.load(main_dir + 'datasetStandardized/' + dataset_name + '/UserData' + str(i) + '.hkl'))\n",
      "  File \"/opt/anaconda3/envs/HAR/lib/python3.9/site-packages/hickle/hickle.py\", line 321, in load\n",
      "    h5f, path, close_flag = file_opener(file_obj, path, 'r', filename)\n",
      "  File \"/opt/anaconda3/envs/HAR/lib/python3.9/site-packages/hickle/fileio.py\", line 147, in file_opener\n",
      "    return h5.File(f, mode.replace('b','')),path,True\n",
      "  File \"/opt/anaconda3/envs/HAR/lib/python3.9/site-packages/h5py/_hl/files.py\", line 564, in __init__\n",
      "    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n",
      "  File \"/opt/anaconda3/envs/HAR/lib/python3.9/site-packages/h5py/_hl/files.py\", line 238, in make_fid\n",
      "    fid = h5f.open(name, flags, fapl=fapl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\n",
      "FileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = './datasets/datasetStandardized/shl/UserData0.hkl', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "\n",
      "âŒ è®­ç»ƒæµç¨‹å› ä¸¥é‡é”™è¯¯è€Œç»ˆæ­¢ã€‚è¯·æŸ¥çœ‹ä¸Šé¢çš„æ—¥å¿—å’Œé”™è¯¯ä¿¡æ¯ã€‚\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' and '__file__' not in locals(): # ç¡®ä¿åœ¨Jupyterç¯å¢ƒä¸­è¿è¡Œ\n",
    "    try:\n",
    "        trainer = ConfigurableTrainer(config_path=CONFIG_PATH)\n",
    "        trainer.run()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nç”¨æˆ·ä¸­æ–­äº†è®­ç»ƒæµç¨‹ã€‚\")\n",
    "    except Exception as e:\n",
    "        logging.exception(\"è®­ç»ƒæµç¨‹ä¸­å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸\")\n",
    "        print(f\"\\nâŒ è®­ç»ƒæµç¨‹å› ä¸¥é‡é”™è¯¯è€Œç»ˆæ­¢ã€‚è¯·æŸ¥çœ‹ä¸Šé¢çš„æ—¥å¿—å’Œé”™è¯¯ä¿¡æ¯ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HAR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
