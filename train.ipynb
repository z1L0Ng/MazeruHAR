{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "config-driven-header",
   "metadata": {},
   "source": [
    "# 配置驱动的HAR训练流程 - 任务2.3完整实现\n",
    "\n",
    "这个notebook实现了完全由配置文件驱动的训练执行引擎。\n",
    "通过修改配置文件即可启动不同的实验，无需修改代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844a56b8",
   "metadata": {},
   "source": [
    "## 步骤 1: 导入所有必需的库\n",
    "首先，我们导入所有需要的标准库、第三方库和项目内部模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准库导入\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "# 第三方库导入\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 项目内部导入 (占位符) ---\n",
    "# 在这个独立的notebook中，我们将必要的辅助函数和模型直接包含进来\n",
    "#真实的模块导入\n",
    "try:\n",
    "    from config.config_loader import ConfigLoader\n",
    "    from config.config_bridge import ConfigBridge\n",
    "except ImportError:\n",
    "    print(\"警告: 配置模块未找到，将只使用传统模式\")\n",
    "    ConfigLoader = None\n",
    "    ConfigBridge = None\n",
    "\n",
    "import utils_torch as utils\n",
    "import model_cbranchformer as model\n",
    "#ConfigLoader = None\n",
    "#ConfigBridge = None\n",
    "\n",
    "\n",
    "print(\"所有模块导入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b2e1f8",
   "metadata": {},
   "source": [
    "## 步骤 2: 定义辅助工具、模型和配置桥接器\n",
    "为了使此Notebook可以独立运行，我们将原本在 `utils_torch.py`、`model_cbranchformer.py` 和 `config_bridge.py` 中的关键代码直接定义在这里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e89d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 从 utils_torch.py 移入的关键代码 ===\n",
    "class HARDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "def load_dataset_pytorch(dataset_name, client_count, data_config, random_seed, base_path):\n",
    "    # 这是一个简化的数据加载器，用于演示目的\n",
    "    # 它会生成随机数据来模拟真实的数据集加载\n",
    "    print(f\"正在生成 {dataset_name} 的模拟数据...\")\n",
    "    \n",
    "    # 根据数据集名称定义参数\n",
    "    if dataset_name.upper() == 'UCI':\n",
    "        train_samples, test_samples = 7352, 2947\n",
    "        seq_len, channels = 128, 6\n",
    "    else: # 默认或其它\n",
    "        train_samples, test_samples = 10000, 2000\n",
    "        seq_len, channels = 100, 9\n",
    "    \n",
    "    # 创建模拟数据\n",
    "    train_data = torch.randn(train_samples, seq_len, channels)\n",
    "    train_label = torch.randint(0, 6, (train_samples,))\n",
    "    test_data = torch.randn(test_samples, seq_len, channels)\n",
    "    test_label = torch.randint(0, 6, (test_samples,))\n",
    "\n",
    "    class MockDataset:\n",
    "        def __init__(self):\n",
    "            self.central_train_data = train_data\n",
    "            self.central_train_label = train_label\n",
    "            self.central_test_data = test_data\n",
    "            self.central_test_label = test_label\n",
    "            self.central_dev_data = None # 让主程序自己分割验证集\n",
    "            self.central_dev_label = None\n",
    "\n",
    "    return MockDataset()\n",
    "\n",
    "def return_client_by_dataset(dataset_name):\n",
    "    # 模拟函数\n",
    "    return 1 # 集中式训练\n",
    "utils = type('utils', (), {'HARDataset': HARDataset, 'load_dataset_pytorch': load_dataset_pytorch, 'return_client_by_dataset': return_client_by_dataset})\n",
    "\n",
    "# === 从 model_cbranchformer.py 移入的关键代码 (简化版) ===\n",
    "class cbranchformer_har_base(nn.Module):\n",
    "    def __init__(self, input_shape, activity_count, **kwargs):\n",
    "        super().__init__()\n",
    "        # 这是一个非常简化的模型结构，用于演示\n",
    "        self.flatten = nn.Flatten()\n",
    "        in_features = input_shape[0] * input_shape[1]\n",
    "        self.fc1 = nn.Linear(in_features, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(kwargs.get('dropout_rate', 0.5))\n",
    "        self.fc2 = nn.Linear(128, activity_count)\n",
    "        print(f\"创建了一个简化的 cbranchformer_har_base 模型\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class MobileHART_XS(nn.Module):\n",
    "    def __init__(self, input_shape, activity_count, **kwargs):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        in_features = input_shape[0] * input_shape[1]\n",
    "        self.fc1 = nn.Linear(in_features, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, activity_count)\n",
    "        print(f\"创建了一个简化的 MobileHART_XS 模型\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = type('model', (), {'cbranchformer_har_base': cbranchformer_har_base, 'MobileHART_XS': MobileHART_XS})\n",
    "\n",
    "# === 从 config_bridge.py 移入的关键代码 ===\n",
    "# === 修复后的ConfigBridge类 ===\n",
    "class ConfigBridge:\n",
    "    def __init__(self, config_path: str):\n",
    "        \"\"\"改进的ConfigBridge初始化\"\"\"\n",
    "        try:\n",
    "            with open(config_path, 'r', encoding='utf-8') as f:\n",
    "                self.raw_config = yaml.safe_load(f)\n",
    "            self.config = self._to_dot_notation(self.raw_config)\n",
    "            self.use_new_config = True\n",
    "            print(f\"✓ 配置文件加载成功: {config_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"✗ 配置文件不存在: {config_path}\")\n",
    "            raise\n",
    "        except yaml.YAMLError as e:\n",
    "            print(f\"✗ YAML解析错误: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"✗ 配置加载失败: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _to_dot_notation(self, data):\n",
    "        \"\"\"将字典转换为点表示法对象，修复values()方法缺失问题\"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            # 创建一个自定义类，支持字典操作\n",
    "            class DotDict:\n",
    "                def __init__(self, data_dict):\n",
    "                    self._dict = data_dict\n",
    "                    for k, v in data_dict.items():\n",
    "                        setattr(self, k, self._to_dot_notation(v))\n",
    "                \n",
    "                def values(self):\n",
    "                    \"\"\"添加values方法支持\"\"\"\n",
    "                    return [getattr(self, k) for k in self._dict.keys()]\n",
    "                \n",
    "                def keys(self):\n",
    "                    \"\"\"添加keys方法支持\"\"\"\n",
    "                    return self._dict.keys()\n",
    "                \n",
    "                def items(self):\n",
    "                    \"\"\"添加items方法支持\"\"\"\n",
    "                    return [(k, getattr(self, k)) for k in self._dict.keys()]\n",
    "                \n",
    "                def get(self, key, default=None):\n",
    "                    \"\"\"添加get方法支持\"\"\"\n",
    "                    return getattr(self, key, default)\n",
    "                \n",
    "                def _to_dot_notation(self, data):\n",
    "                    \"\"\"递归转换方法\"\"\"\n",
    "                    if isinstance(data, dict):\n",
    "                        return DotDict(data)\n",
    "                    elif isinstance(data, list):\n",
    "                        return [self._to_dot_notation(i) for i in data]\n",
    "                    else:\n",
    "                        return data\n",
    "            \n",
    "            return DotDict(data)\n",
    "        elif isinstance(data, list):\n",
    "            return [self._to_dot_notation(i) for i in data]\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "    def get_dataset_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取数据集配置\"\"\"\n",
    "        return self.raw_config.get('dataset', {})\n",
    "\n",
    "    def get_training_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取训练配置\"\"\"\n",
    "        return self.raw_config.get('training', {})\n",
    "        \n",
    "    def get_visualization_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取可视化配置\"\"\"\n",
    "        return self.raw_config.get('visualization', {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07368b",
   "metadata": {},
   "source": [
    "## 步骤 3: 定义核心训练器类\n",
    "这里是包含完整 `ConfigurableTrainer` 类的代码。`validate_epoch` 方法已经被修复和补全。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trainer-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurableTrainer:\n",
    "    \"\"\"配置驱动的训练器类 - 完整实现任务2.3\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: Optional[str] = None):\n",
    "        \"\"\"初始化训练器\"\"\"\n",
    "        self.config_path = config_path\n",
    "        self.use_config = config_path is not None and ConfigBridge is not None\n",
    "        \n",
    "        # 初始化配置桥接器\n",
    "        if self.use_config:\n",
    "            try:\n",
    "                self.config_bridge = ConfigBridge(config_path)\n",
    "                self.config = self.config_bridge.config\n",
    "                self.use_config = self.config_bridge.use_new_config\n",
    "            except Exception as e:\n",
    "                print(f\"配置文件加载失败: {e}\")\n",
    "                print(\"回退到传统硬编码模式\")\n",
    "                self.use_config = False\n",
    "                self.config_bridge = None\n",
    "                self.config = None\n",
    "        else:\n",
    "            self.config_bridge = None\n",
    "            self.config = None\n",
    "        \n",
    "        # 初始化训练状态\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.criterion = None\n",
    "        self.device = None\n",
    "        self.output_dir = None\n",
    "        self.logger = None\n",
    "        self.gradient_clip_norm = 0\n",
    "        \n",
    "        # 训练历史\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'train_accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_accuracy': []\n",
    "        }\n",
    "        \n",
    "        print(f\"训练器初始化完成，使用配置模式: {'新配置系统' if self.use_config else '传统硬编码'}\")\n",
    "    \n",
    "    def setup_logging(self, output_dir: Path, verbose: bool = True):\n",
    "        \"\"\"设置日志系统\"\"\"\n",
    "        log_level = logging.INFO if verbose else logging.WARNING\n",
    "        log_dir = output_dir / 'logs'\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 清除已有的处理器\n",
    "        for handler in logging.root.handlers[:]:\n",
    "            logging.root.removeHandler(handler)\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=log_level,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_dir / 'training.log'),\n",
    "                logging.StreamHandler(sys.stdout)\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.info(\"日志系统初始化完成\")\n",
    "    \n",
    "    def get_parameters(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取训练参数，支持配置文件和硬编码两种模式\"\"\"\n",
    "        if self.use_config:\n",
    "            return self._get_config_parameters()\n",
    "        else:\n",
    "            return self._get_hardcoded_parameters()\n",
    "    \n",
    "    def setup_environment(self, params: Dict[str, Any]) -> None:\n",
    "        \"\"\"设置训练环境\"\"\"\n",
    "        try:\n",
    "            # 设置随机种子\n",
    "            seed = params['random_seed']\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed(seed)\n",
    "                torch.cuda.manual_seed_all(seed)\n",
    "            \n",
    "            # 设置设备\n",
    "            device_config = params['device']\n",
    "            if device_config == 'auto':\n",
    "                if torch.cuda.is_available():\n",
    "                    self.device = torch.device('cuda')\n",
    "                    print(f\"使用 CUDA 设备: {torch.cuda.get_device_name()}\")\n",
    "                elif torch.backends.mps.is_available():\n",
    "                    self.device = torch.device('mps')\n",
    "                    print(\"使用 MPS 设备\")\n",
    "                else:\n",
    "                    self.device = torch.device('cpu')\n",
    "                    print(\"使用 CPU 设备\")\n",
    "            else:\n",
    "                self.device = torch.device(device_config)\n",
    "                print(f\"使用指定设备: {self.device}\")\n",
    "            \n",
    "            # 设置输出目录\n",
    "            self.output_dir = Path(params['output_dir'])\n",
    "            self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # 设置日志系统\n",
    "            self.setup_logging(self.output_dir, params['verbose'])\n",
    "            \n",
    "            # 保存梯度裁剪参数\n",
    "            self.gradient_clip_norm = params.get('gradient_clip_norm', 0)\n",
    "            \n",
    "            # PyTorch 性能优化\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.deterministic = False\n",
    "            \n",
    "            self.logger.info(\"环境设置完成\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"环境设置失败: {e}\")\n",
    "            raise\n",
    "\n",
    "    # 这里需要添加其他所有方法...\n",
    "    # 由于篇幅限制，我将在下面的cells中提供其他方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fddd184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ConfigurableTrainer 缺失的方法实现 ===\n",
    "\n",
    "def load_data(self, params: Dict[str, Any]) -> Tuple[torch.utils.data.DataLoader, ...]:\n",
    "    \"\"\"加载数据集\"\"\"\n",
    "    dataset_name = params['dataset_name']\n",
    "    data_config = params['data_config']\n",
    "    random_seed = params['random_seed']\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    self.logger.info(f\"开始加载数据集: {dataset_name}\")\n",
    "    \n",
    "    try:\n",
    "        # 使用现有的数据加载函数\n",
    "        client_count = utils.return_client_by_dataset(dataset_name)\n",
    "        loaded_dataset = utils.load_dataset_pytorch(\n",
    "            dataset_name, client_count, data_config, random_seed, './datasets/'\n",
    "        )\n",
    "        \n",
    "        # 获取数据\n",
    "        central_train_data = loaded_dataset.central_train_data\n",
    "        central_train_label = loaded_dataset.central_train_label\n",
    "        central_test_data = loaded_dataset.central_test_data\n",
    "        central_test_label = loaded_dataset.central_test_label\n",
    "        \n",
    "        # 处理验证集\n",
    "        if hasattr(loaded_dataset, 'central_dev_data') and loaded_dataset.central_dev_data is not None:\n",
    "            central_dev_data = loaded_dataset.central_dev_data\n",
    "            central_dev_label = loaded_dataset.central_dev_label\n",
    "        else:\n",
    "            # 从训练集分割验证集\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            central_train_data_np = central_train_data.cpu().numpy() if torch.is_tensor(central_train_data) else central_train_data\n",
    "            central_train_label_np = central_train_label.cpu().numpy() if torch.is_tensor(central_train_label) else central_train_label\n",
    "            \n",
    "            central_train_data_np, central_dev_data, central_train_label_np, central_dev_label = train_test_split(\n",
    "                central_train_data_np, \n",
    "                central_train_label_np,\n",
    "                test_size=0.125, \n",
    "                random_state=random_seed,\n",
    "                stratify=central_train_label_np\n",
    "            )\n",
    "            central_train_data = torch.FloatTensor(central_train_data_np)\n",
    "            central_train_label = torch.LongTensor(central_train_label_np)\n",
    "            central_dev_data = torch.FloatTensor(central_dev_data)\n",
    "            central_dev_label = torch.LongTensor(central_dev_label)\n",
    "        \n",
    "        # 确保数据为torch tensor\n",
    "        if not torch.is_tensor(central_train_data):\n",
    "            central_train_data = torch.FloatTensor(central_train_data)\n",
    "            central_train_label = torch.LongTensor(central_train_label)\n",
    "            central_test_data = torch.FloatTensor(central_test_data)\n",
    "            central_test_label = torch.LongTensor(central_test_label)\n",
    "        \n",
    "        # 创建数据集\n",
    "        train_dataset = utils.HARDataset(central_train_data, central_train_label)\n",
    "        dev_dataset = utils.HARDataset(central_dev_data, central_dev_label)\n",
    "        test_dataset = utils.HARDataset(central_test_data, central_test_label)\n",
    "        \n",
    "        # 创建数据加载器\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True, \n",
    "            num_workers=2, pin_memory=True\n",
    "        )\n",
    "        dev_loader = torch.utils.data.DataLoader(\n",
    "            dev_dataset, batch_size=batch_size, shuffle=False, \n",
    "            num_workers=2, pin_memory=True\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset, batch_size=batch_size, shuffle=False, \n",
    "            num_workers=2, pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.logger.info(\n",
    "            f\"数据加载完成: 训练集={len(train_dataset)}, \"\n",
    "            f\"验证集={len(dev_dataset)}, 测试集={len(test_dataset)}\"\n",
    "        )\n",
    "        \n",
    "        return train_loader, dev_loader, test_loader, central_train_label.cpu().numpy()\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"数据加载失败: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_model(self, params: Dict[str, Any]) -> nn.Module:\n",
    "    \"\"\"创建模型\"\"\"\n",
    "    architecture = params['architecture']\n",
    "    input_shape = params['input_shape']\n",
    "    activity_count = params['activity_count']\n",
    "    \n",
    "    self.logger.info(f\"创建模型: {architecture}, 输入形状={input_shape}, 类别数={activity_count}\")\n",
    "    \n",
    "    try:\n",
    "        if architecture == \"HART\":\n",
    "            self.model = model.cbranchformer_har_base(\n",
    "                input_shape=input_shape,\n",
    "                activity_count=activity_count,\n",
    "                projection_dim=params['projection_dim'],\n",
    "                patch_size=params['frame_length'],\n",
    "                time_step=params['time_step'],\n",
    "                num_heads=3,\n",
    "                filter_attention_head=params['filter_attention_head'],\n",
    "                conv_kernels=params['conv_kernels'],\n",
    "                dropout_rate=params['dropout_rate'],\n",
    "                use_tokens=params['token_based']\n",
    "            ).to(self.device)\n",
    "        else:\n",
    "            # MobileHART或其他架构\n",
    "            self.model = model.MobileHART_XS(\n",
    "                input_shape=input_shape,\n",
    "                activity_count=activity_count\n",
    "            ).to(self.device)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        self.logger.info(f\"模型创建完成，总参数数量: {total_params:,}\")\n",
    "        \n",
    "        return self.model\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"模型创建失败: {e}\")\n",
    "        raise\n",
    "\n",
    "def setup_training(self, params: Dict[str, Any], train_labels: np.ndarray) -> None:\n",
    "    \"\"\"设置训练组件\"\"\"\n",
    "    try:\n",
    "        # 计算类权重\n",
    "        from sklearn.utils import class_weight\n",
    "        temp_weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(train_labels),\n",
    "            y=train_labels.ravel()\n",
    "        )\n",
    "        class_weights = {j: temp_weights[j] for j in range(len(temp_weights))}\n",
    "        class_weights_tensor = torch.FloatTensor(\n",
    "            [class_weights[i] for i in range(params['activity_count'])]\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # 创建损失函数\n",
    "        self.criterion = nn.CrossEntropyLoss(\n",
    "            weight=class_weights_tensor, \n",
    "            label_smoothing=params['label_smoothing']\n",
    "        )\n",
    "        \n",
    "        # 创建优化器\n",
    "        optimizer_name = params['optimizer_name'].lower()\n",
    "        if optimizer_name == 'adam':\n",
    "            self.optimizer = optim.Adam(\n",
    "                self.model.parameters(),\n",
    "                lr=params['learning_rate'],\n",
    "                weight_decay=params['weight_decay']\n",
    "            )\n",
    "        elif optimizer_name == 'adamw':\n",
    "            self.optimizer = optim.AdamW(\n",
    "                self.model.parameters(),\n",
    "                lr=params['learning_rate'],\n",
    "                weight_decay=params['weight_decay']\n",
    "            )\n",
    "        elif optimizer_name == 'sgd':\n",
    "            self.optimizer = optim.SGD(\n",
    "                self.model.parameters(),\n",
    "                lr=params['learning_rate'],\n",
    "                weight_decay=params['weight_decay'],\n",
    "                momentum=0.9\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的优化器: {optimizer_name}\")\n",
    "        \n",
    "        # 创建学习率调度器\n",
    "        scheduler_name = params['scheduler_name']\n",
    "        if scheduler_name == 'cosine':\n",
    "            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                self.optimizer, T_max=params['local_epoch']\n",
    "            )\n",
    "        elif scheduler_name == 'step':\n",
    "            self.scheduler = optim.lr_scheduler.StepLR(\n",
    "                self.optimizer, step_size=30, gamma=0.1\n",
    "            )\n",
    "        elif scheduler_name == 'plateau':\n",
    "            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, mode='min', patience=5, factor=0.5\n",
    "            )\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "        \n",
    "        self.logger.info(f\"训练组件设置完成: 优化器={optimizer_name}, 调度器={scheduler_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"训练组件设置失败: {e}\")\n",
    "        raise\n",
    "\n",
    "def evaluate_model(self, test_loader: DataLoader, params: Dict[str, Any]) -> None:\n",
    "    \"\"\"评估模型\"\"\"\n",
    "    self.model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(self.device), targets.to(self.device)\n",
    "            outputs = self.model(data)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    \n",
    "    # 计算F1分数\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    \n",
    "    self.logger.info(f\"测试结果: Loss={test_loss:.4f}, Accuracy={test_acc:.4f}, F1-Score={f1:.4f}\")\n",
    "\n",
    "    if params['plot_confusion_matrix']:\n",
    "        self.plot_confusion_matrix(all_targets, all_preds, params['activity_labels'])\n",
    "\n",
    "def plot_confusion_matrix(self, y_true, y_pred, labels):\n",
    "    \"\"\"绘制混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    cm_path = self.output_dir / 'confusion_matrix.png'\n",
    "    plt.savefig(cm_path)\n",
    "    self.logger.info(f\"混淆矩阵已保存至 {cm_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_curves(self):\n",
    "    \"\"\"绘制学习曲线\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 损失曲线\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(self.history['train_loss'], label='Train Loss')\n",
    "    plt.plot(self.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 准确率曲线\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(self.history['train_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(self.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    curves_path = self.output_dir / 'learning_curves.png'\n",
    "    plt.savefig(curves_path)\n",
    "    self.logger.info(f\"学习曲线已保存至 {curves_path}\")\n",
    "    plt.show()\n",
    "\n",
    "def _get_hardcoded_parameters(self) -> Dict[str, Any]:\n",
    "    \"\"\"硬编码参数（向后兼容）\"\"\"\n",
    "    return {\n",
    "        'experiment_name': 'Hardcoded_Experiment',\n",
    "        'description': '使用硬编码参数的实验',\n",
    "        'dataset_name': 'UCI',\n",
    "        'data_config': 'BALANCED',\n",
    "        'activity_labels': ['Walk', 'Upstair', 'Downstair', 'Sit', 'Stand', 'Lay'],\n",
    "        'activity_count': 6,\n",
    "        'architecture': 'HART',\n",
    "        'segment_size': 128,\n",
    "        'num_input_channels': 6,\n",
    "        'input_shape': (128, 6),\n",
    "        'batch_size': 256,\n",
    "        'learning_rate': 5e-3,\n",
    "        'local_epoch': 50,\n",
    "        'dropout_rate': 0.3,\n",
    "        'weight_decay': 1e-4,\n",
    "        'gradient_clip_norm': 1.0,\n",
    "        'label_smoothing': 0.1,\n",
    "        'optimizer_name': 'adam',\n",
    "        'scheduler_name': 'cosine',\n",
    "        'early_stopping_patience': 10,\n",
    "        'projection_dim': 192,\n",
    "        'frame_length': 16,\n",
    "        'time_step': 16,\n",
    "        'filter_attention_head': 4,\n",
    "        'conv_kernels': [3, 7, 15, 31, 31, 31],\n",
    "        'token_based': False,\n",
    "        'random_seed': 42,\n",
    "        'device': 'auto',\n",
    "        'output_dir': './results/hardcoded_experiment',\n",
    "        'save_checkpoints': True,\n",
    "        'verbose': True,\n",
    "        'show_train_verbose': 1,\n",
    "        'plot_learning_curves': True,\n",
    "        'plot_confusion_matrix': True,\n",
    "    }\n",
    "\n",
    "# 将所有方法添加到ConfigurableTrainer类中\n",
    "ConfigurableTrainer.load_data = load_data\n",
    "ConfigurableTrainer.create_model = create_model\n",
    "ConfigurableTrainer.setup_training = setup_training\n",
    "ConfigurableTrainer.evaluate_model = evaluate_model\n",
    "ConfigurableTrainer.plot_confusion_matrix = plot_confusion_matrix\n",
    "ConfigurableTrainer.plot_learning_curves = plot_learning_curves\n",
    "ConfigurableTrainer._get_hardcoded_parameters = _get_hardcoded_parameters\n",
    "\n",
    "print(\"✓ ConfigurableTrainer所有缺失方法已添加完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2372225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ConfigurableTrainer 方法 - 第一部分 ===\n",
    "\n",
    "def _get_config_parameters(self) -> Dict[str, Any]:\n",
    "    \"\"\"从配置文件获取参数 - 修复版本\"\"\"\n",
    "    try:\n",
    "        dataset_config = self.config_bridge.get_dataset_config()\n",
    "        training_config = self.config_bridge.get_training_config()\n",
    "        visualization_config = self.config_bridge.get_visualization_config()\n",
    "        \n",
    "        # 验证配置的完整性\n",
    "        if not dataset_config or not training_config:\n",
    "            raise ValueError(\"配置文件不完整\")\n",
    "        \n",
    "        # 计算总通道数\n",
    "        total_channels = sum(mod['channels'] for mod in dataset_config['modalities'])\n",
    "        \n",
    "        # 修复：正确获取专家配置\n",
    "        experts_config = self.config.architecture.experts\n",
    "        if hasattr(experts_config, 'values'):\n",
    "            # 如果有values方法，使用它\n",
    "            main_expert = list(experts_config.values())[0]\n",
    "        else:\n",
    "            # 否则通过属性名获取第一个专家\n",
    "            expert_names = list(experts_config.keys()) if hasattr(experts_config, 'keys') else dir(experts_config)\n",
    "            expert_names = [name for name in expert_names if not name.startswith('_')]\n",
    "            if expert_names:\n",
    "                main_expert = getattr(experts_config, expert_names[0])\n",
    "            else:\n",
    "                # 回退方案：使用默认值\n",
    "                print(\"⚠ 无法获取专家配置，使用默认值\")\n",
    "                main_expert = type('Expert', (), {\n",
    "                    'type': 'HART',\n",
    "                    'params': {\n",
    "                        'projection_dim': 192,\n",
    "                        'frame_length': 16,\n",
    "                        'time_step': 16,\n",
    "                        'filter_attention_head': 4,\n",
    "                        'conv_kernels': [3, 7, 15, 31, 31, 31],\n",
    "                        'token_based': False\n",
    "                    }\n",
    "                })()\n",
    "        \n",
    "        architecture_type = main_expert.type.upper()\n",
    "        \n",
    "        return {\n",
    "            # 实验信息\n",
    "            'experiment_name': self.config.name,\n",
    "            'description': getattr(self.config, 'description', ''),\n",
    "            \n",
    "            # 数据集参数\n",
    "            'dataset_name': dataset_config['name'],\n",
    "            'data_config': 'BALANCED',\n",
    "            'activity_labels': dataset_config['activity_labels'],\n",
    "            'activity_count': len(dataset_config['activity_labels']),\n",
    "            \n",
    "            # 模型参数\n",
    "            'architecture': architecture_type,\n",
    "            'segment_size': dataset_config['modalities'][0]['sequence_length'],\n",
    "            'num_input_channels': total_channels,\n",
    "            'input_shape': (dataset_config['modalities'][0]['sequence_length'], total_channels),\n",
    "            \n",
    "            # 训练参数\n",
    "            'batch_size': training_config['batch_size'],\n",
    "            'learning_rate': training_config['learning_rate'],\n",
    "            'local_epoch': training_config['epochs'],\n",
    "            'dropout_rate': self.config.architecture.dropout_rate,\n",
    "            \n",
    "            # 高级参数\n",
    "            'weight_decay': training_config.get('weight_decay', 1e-4),\n",
    "            'gradient_clip_norm': training_config.get('gradient_clip_norm', 1.0),\n",
    "            'label_smoothing': training_config.get('label_smoothing', 0.1),\n",
    "            'optimizer_name': training_config.get('optimizer', 'adam'),\n",
    "            'scheduler_name': training_config.get('scheduler', 'cosine'),\n",
    "            'early_stopping_patience': training_config.get('early_stopping_patience', 10),\n",
    "            \n",
    "            # 模型特定参数 - 添加安全的属性获取\n",
    "            'projection_dim': getattr(main_expert.params, 'projection_dim', 192) if hasattr(main_expert, 'params') else 192,\n",
    "            'frame_length': getattr(main_expert.params, 'frame_length', 16) if hasattr(main_expert, 'params') else 16,\n",
    "            'time_step': getattr(main_expert.params, 'time_step', 16) if hasattr(main_expert, 'params') else 16,\n",
    "            'filter_attention_head': getattr(main_expert.params, 'filter_attention_head', 4) if hasattr(main_expert, 'params') else 4,\n",
    "            'conv_kernels': getattr(main_expert.params, 'conv_kernels', [3, 7, 15, 31, 31, 31]) if hasattr(main_expert, 'params') else [3, 7, 15, 31, 31, 31],\n",
    "            'token_based': getattr(main_expert.params, 'token_based', False) if hasattr(main_expert, 'params') else False,\n",
    "            \n",
    "            # 系统参数\n",
    "            'random_seed': self.config.seed,\n",
    "            'device': self.config.device,\n",
    "            'output_dir': self.config.output_dir,\n",
    "            'save_checkpoints': self.config.save_checkpoints,\n",
    "            'verbose': self.config.verbose,\n",
    "            \n",
    "            # 可视化参数\n",
    "            'show_train_verbose': 1 if self.config.verbose else 0,\n",
    "            'plot_learning_curves': visualization_config.get('plot_learning_curves', True),\n",
    "            'plot_confusion_matrix': visualization_config.get('plot_confusion_matrix', True),\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        # 修复：安全的错误处理\n",
    "        error_msg = f\"配置参数获取失败: {e}\"\n",
    "        if self.logger:\n",
    "            self.logger.error(error_msg)\n",
    "        else:\n",
    "            print(f\"✗ {error_msg}\")\n",
    "        raise\n",
    "\n",
    "# 将这些方法添加到ConfigurableTrainer类中\n",
    "ConfigurableTrainer._get_config_parameters = _get_config_parameters\n",
    "\n",
    "print(\"✓ ConfigurableTrainer配置方法添加完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dab5edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ConfigurableTrainer 方法 - 第二部分 ===\n",
    "\n",
    "def train_epoch(self, train_loader: DataLoader) -> Tuple[float, float]:\n",
    "    \"\"\"训练一个epoch\"\"\"\n",
    "    self.model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        try:\n",
    "            data, targets = data.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            # 梯度清零\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = self.model(data)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            if self.gradient_clip_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_norm)\n",
    "            \n",
    "            # 参数更新\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # 统计\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            # 打印进度\n",
    "            if batch_idx % 100 == 0:\n",
    "                self.logger.info(f'批次 {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"训练批次 {batch_idx} 出错: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    self.logger.info(f\"训练 - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate_epoch(self, val_loader: DataLoader) -> Tuple[float, float]:\n",
    "    \"\"\"验证一个epoch\"\"\"\n",
    "    self.model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, targets) in enumerate(val_loader):\n",
    "            try:\n",
    "                data, targets = data.to(self.device), targets.to(self.device)\n",
    "                \n",
    "                # 前向传播\n",
    "                outputs = self.model(data)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # 计算准确率\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "                \n",
    "                # 保存预测和真实标签用于后续分析\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"验证批次 {batch_idx} 出错: {e}\")\n",
    "                continue\n",
    "    \n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    # 计算F1分数\n",
    "    try:\n",
    "        f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "        self.logger.info(f\"验证 - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%, F1: {f1:.4f}\")\n",
    "    except ImportError:\n",
    "        self.logger.info(f\"验证 - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# 将这些方法添加到ConfigurableTrainer类中\n",
    "ConfigurableTrainer.train_epoch = train_epoch\n",
    "ConfigurableTrainer.validate_epoch = validate_epoch\n",
    "\n",
    "print(\"✓ ConfigurableTrainer训练方法添加完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974207ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(self):\n",
    "    \"\"\"执行完整的训练和评估流程 - 修复版本\"\"\"\n",
    "    try:\n",
    "        params = self.get_parameters()\n",
    "        self.setup_environment(params)  # 这里会初始化logger\n",
    "        self.logger.info(f\"--- 实验开始: {params['experiment_name']} ---\")\n",
    "        self.logger.info(f\"参数:\\n{json.dumps(params, indent=2, ensure_ascii=False)}\")\n",
    "\n",
    "        train_loader, dev_loader, test_loader, train_labels = self.load_data(params)\n",
    "        self.create_model(params)\n",
    "        self.setup_training(params, train_labels)\n",
    "\n",
    "        self.logger.info(\"--- 开始训练 ---\")\n",
    "        best_val_accuracy = 0.0\n",
    "        patience_counter = 0\n",
    "        early_stopping_patience = params.get('early_stopping_patience', 10)\n",
    "        \n",
    "        for epoch in range(params['local_epoch']):\n",
    "            self.logger.info(f\"\\nEpoch {epoch+1}/{params['local_epoch']}\")\n",
    "            \n",
    "            # 训练阶段\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_accuracy'].append(train_acc)\n",
    "            \n",
    "            # 验证阶段\n",
    "            val_loss, val_acc = self.validate_epoch(dev_loader)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_accuracy'].append(val_acc)\n",
    "            \n",
    "            # 学习率调度\n",
    "            if self.scheduler:\n",
    "                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    self.scheduler.step(val_loss)\n",
    "                else:\n",
    "                    self.scheduler.step()\n",
    "            \n",
    "            # 早停检查\n",
    "            if val_acc > best_val_accuracy:\n",
    "                best_val_accuracy = val_acc\n",
    "                patience_counter = 0\n",
    "                \n",
    "                # 保存最佳模型\n",
    "                if params['save_checkpoints']:\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch + 1,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\n",
    "                        'best_val_accuracy': best_val_accuracy,\n",
    "                        'config': params\n",
    "                    }\n",
    "                    torch.save(checkpoint, self.output_dir / 'best_model.pth')\n",
    "                    self.logger.info(f\"保存最佳模型 (验证准确率: {best_val_accuracy:.2f}%)\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            # 早停条件\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                self.logger.info(f\"早停触发 (patience: {early_stopping_patience})\")\n",
    "                break\n",
    "            \n",
    "            # 定期保存检查点\n",
    "            if (epoch + 1) % 10 == 0 and params['save_checkpoints']:\n",
    "                checkpoint_path = self.output_dir / f'checkpoint_epoch_{epoch+1}.pth'\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_accuracy': val_acc\n",
    "                }, checkpoint_path)\n",
    "        \n",
    "        self.logger.info(f\"训练完成！最佳验证准确率: {best_val_accuracy:.2f}%\")\n",
    "        \n",
    "        # 绘制学习曲线\n",
    "        if params.get('plot_learning_curves', True):\n",
    "            self.plot_learning_curves()\n",
    "        \n",
    "        # 测试评估\n",
    "        self.logger.info(\"--- 开始测试评估 ---\")\n",
    "        self.evaluate_model(test_loader, params)\n",
    "        \n",
    "        self.logger.info(\"--- 实验完成 ---\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"训练过程中发生错误: {e}\"\n",
    "        if self.logger:\n",
    "            self.logger.error(error_msg)\n",
    "        else:\n",
    "            print(f\"✗ {error_msg}\")\n",
    "        raise\n",
    "\n",
    "# 将修复后的方法应用到ConfigurableTrainer类\n",
    "ConfigurableTrainer._get_config_parameters = _get_config_parameters\n",
    "ConfigurableTrainer.setup_environment = setup_environment\n",
    "ConfigurableTrainer.run = run\n",
    "\n",
    "print(\"✓ 修复后的ConfigBridge和ConfigurableTrainer方法加载完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea644ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 修复 DataLoader 多进程错误 ===\n",
    "\n",
    "def load_data_fixed(self, params: Dict[str, Any]) -> Tuple[torch.utils.data.DataLoader, ...]:\n",
    "    \"\"\"修复版本的数据加载方法\"\"\"\n",
    "    dataset_name = params['dataset_name']\n",
    "    data_config = params['data_config']\n",
    "    random_seed = params['random_seed']\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    self.logger.info(f\"开始加载数据集: {dataset_name}\")\n",
    "    \n",
    "    try:\n",
    "        # 使用现有的数据加载函数\n",
    "        client_count = utils.return_client_by_dataset(dataset_name)\n",
    "        loaded_dataset = utils.load_dataset_pytorch(\n",
    "            dataset_name, client_count, data_config, random_seed, './datasets/'\n",
    "        )\n",
    "        \n",
    "        # 获取数据\n",
    "        central_train_data = loaded_dataset.central_train_data\n",
    "        central_train_label = loaded_dataset.central_train_label\n",
    "        central_test_data = loaded_dataset.central_test_data\n",
    "        central_test_label = loaded_dataset.central_test_label\n",
    "        \n",
    "        # 处理验证集\n",
    "        if hasattr(loaded_dataset, 'central_dev_data') and loaded_dataset.central_dev_data is not None:\n",
    "            central_dev_data = loaded_dataset.central_dev_data\n",
    "            central_dev_label = loaded_dataset.central_dev_label\n",
    "        else:\n",
    "            # 从训练集分割验证集\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            central_train_data_np = central_train_data.cpu().numpy() if torch.is_tensor(central_train_data) else central_train_data\n",
    "            central_train_label_np = central_train_label.cpu().numpy() if torch.is_tensor(central_train_label) else central_train_label\n",
    "            \n",
    "            central_train_data_np, central_dev_data, central_train_label_np, central_dev_label = train_test_split(\n",
    "                central_train_data_np, \n",
    "                central_train_label_np,\n",
    "                test_size=0.125, \n",
    "                random_state=random_seed,\n",
    "                stratify=central_train_label_np\n",
    "            )\n",
    "            central_train_data = torch.FloatTensor(central_train_data_np)\n",
    "            central_train_label = torch.LongTensor(central_train_label_np)\n",
    "            central_dev_data = torch.FloatTensor(central_dev_data)\n",
    "            central_dev_label = torch.LongTensor(central_dev_label)\n",
    "        \n",
    "        # 确保数据为torch tensor\n",
    "        if not torch.is_tensor(central_train_data):\n",
    "            central_train_data = torch.FloatTensor(central_train_data)\n",
    "            central_train_label = torch.LongTensor(central_train_label)\n",
    "            central_test_data = torch.FloatTensor(central_test_data)\n",
    "            central_test_label = torch.LongTensor(central_test_label)\n",
    "        \n",
    "        # 创建数据集\n",
    "        train_dataset = utils.HARDataset(central_train_data, central_train_label)\n",
    "        dev_dataset = utils.HARDataset(central_dev_data, central_dev_label)\n",
    "        test_dataset = utils.HARDataset(central_test_data, central_test_label)\n",
    "        \n",
    "        # 🔧 修复：设置 num_workers=0 避免多进程问题\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=batch_size, shuffle=True, \n",
    "            num_workers=0,  # 修改：使用单进程\n",
    "            pin_memory=False  # 修改：关闭pin_memory\n",
    "        )\n",
    "        dev_loader = torch.utils.data.DataLoader(\n",
    "            dev_dataset, batch_size=batch_size, shuffle=False, \n",
    "            num_workers=0,  # 修改：使用单进程\n",
    "            pin_memory=False  # 修改：关闭pin_memory\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset, batch_size=batch_size, shuffle=False, \n",
    "            num_workers=0,  # 修改：使用单进程\n",
    "            pin_memory=False  # 修改：关闭pin_memory\n",
    "        )\n",
    "        \n",
    "        self.logger.info(\n",
    "            f\"数据加载完成: 训练集={len(train_dataset)}, \"\n",
    "            f\"验证集={len(dev_dataset)}, 测试集={len(test_dataset)}\"\n",
    "        )\n",
    "        self.logger.info(\"注意：使用单进程模式避免多进程序列化问题\")\n",
    "        \n",
    "        return train_loader, dev_loader, test_loader, central_train_label.cpu().numpy()\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"数据加载失败: {e}\")\n",
    "        raise\n",
    "\n",
    "# 替换原来的load_data方法\n",
    "ConfigurableTrainer.load_data = load_data_fixed\n",
    "\n",
    "print(\"✓ DataLoader 多进程问题已修复\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5e4508",
   "metadata": {},
   "source": [
    "## 步骤 4: 创建配置文件\n",
    "这是本脚本的核心驱动力。下面的单元格将创建一个名为 `config.yaml` 的文件。你可以直接修改这个单元格中的内容来改变实验参数，而无需触碰上面的类定义代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f513fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "\n",
    "# 实验元数据\n",
    "name: \"UCI_HART_Baseline_Experiment\"\n",
    "description: \"使用HART模型在UCI-HAR数据集上进行的基线实验\"\n",
    "seed: 42\n",
    "device: \"auto\"  # 'auto', 'cpu', 'cuda'\n",
    "output_dir: \"./results/uci_hart_baseline\"\n",
    "save_checkpoints: true\n",
    "verbose: true\n",
    "\n",
    "# 数据集配置\n",
    "dataset:\n",
    "  name: \"UCI\"\n",
    "  activity_labels: ['Walk', 'Upstair', 'Downstair', 'Sit', 'Stand', 'Lay']\n",
    "  modalities:\n",
    "    - name: \"accel\"\n",
    "      channels: 3\n",
    "      sequence_length: 128\n",
    "    - name: \"gyro\"\n",
    "      channels: 3\n",
    "      sequence_length: 128\n",
    "\n",
    "# 模型架构配置\n",
    "architecture:\n",
    "  dropout_rate: 0.3\n",
    "  # 定义专家模型，可以有多个，但目前只使用第一个\n",
    "  experts:\n",
    "    expert1:\n",
    "      type: \"HART\"\n",
    "      params:\n",
    "        projection_dim: 192\n",
    "        frame_length: 16\n",
    "        time_step: 16\n",
    "        filter_attention_head: 4\n",
    "        conv_kernels: [3, 7, 15, 31, 31, 31]\n",
    "        token_based: false\n",
    "\n",
    "# 训练过程配置\n",
    "training:\n",
    "  epochs: 20  # 为了快速演示，减少epoch数量\n",
    "  batch_size: 128\n",
    "  learning_rate: 0.001\n",
    "  optimizer: \"adamw\"  # 'adam', 'adamw', 'sgd'\n",
    "  weight_decay: 0.0001\n",
    "  scheduler: \"cosine\" # 'cosine', 'step', null\n",
    "  label_smoothing: 0.1\n",
    "  gradient_clip_norm: 1.0\n",
    "\n",
    "# 可视化配置\n",
    "visualization:\n",
    "  plot_learning_curves: true\n",
    "  plot_confusion_matrix: true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af482da3",
   "metadata": {},
   "source": [
    "## 步骤 5: 启动训练\n",
    "现在，一切准备就绪。运行下面的单元格来实例化 `ConfigurableTrainer` 并启动由 `config.yaml` 文件定义的完整训练流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d3fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # 指定配置文件路径\n",
    "        config_file = 'config.yaml'\n",
    "        \n",
    "        # 检查配置文件是否存在\n",
    "        if not os.path.exists(config_file):\n",
    "            print(f\"⚠ 配置文件 {config_file} 不存在，将使用传统硬编码模式\")\n",
    "            trainer = ConfigurableTrainer(config_path=None)\n",
    "        else:\n",
    "            print(f\"✓ 使用配置文件: {config_file}\")\n",
    "            trainer = ConfigurableTrainer(config_path=config_file)\n",
    "        \n",
    "        # 运行训练\n",
    "        trainer.run()\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n用户中断训练\")\n",
    "    except Exception as e:\n",
    "        print(f\"训练失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HAR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
