{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MazeruHAR åŠ¨æ€è®­ç»ƒå¼•æ“\n",
    "\n",
    "æ¬¢è¿ä½¿ç”¨ MazeruHAR çš„é…ç½®é©±åŠ¨è®­ç»ƒå¼•æ“ã€‚æ­¤ Notebook æ—¨åœ¨æä¾›ä¸€ä¸ªçµæ´»ã€å¯é‡ç°ä¸”æ˜“äºä½¿ç”¨çš„è®­ç»ƒæµç¨‹ã€‚\n",
    "\n",
    "**æ ¸å¿ƒç†å¿µ:** **ä»£ç åªå†™ä¸€æ¬¡ï¼Œå®éªŒé…ç½®ä¸‡åƒã€‚**\n",
    "\n",
    "æ‚¨åªéœ€è¦æ‰§è¡Œä»¥ä¸‹ä¸¤ä¸ªç®€å•æ­¥éª¤å³å¯å¼€å§‹è®­ç»ƒï¼š\n",
    "\n",
    "1.  **é…ç½®å®éªŒ**: ä¿®æ”¹ä½äº `config/` ç›®å½•ä¸‹çš„ `.yaml` é…ç½®æ–‡ä»¶ã€‚æ‚¨å¯ä»¥å¤åˆ¶ `config/default_configs/shl_config.yaml` å¹¶æ ¹æ®æ‚¨çš„éœ€æ±‚è¿›è¡Œè°ƒæ•´ï¼Œæ¯”å¦‚æ›´æ¢æ•°æ®é›†ã€æ¨¡å‹æ¶æ„æˆ–è¶…å‚æ•°ã€‚\n",
    "2.  **è¿è¡Œ Notebook**: åœ¨ä¸‹é¢çš„ **â€œå®éªŒé…ç½®â€**å•å…ƒæ ¼ä¸­è®¾ç½®å¥½é…ç½®æ–‡ä»¶çš„è·¯å¾„ï¼Œç„¶åä»å¤´åˆ°å°¾è¿è¡Œæ­¤ Notebook å³å¯ã€‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 1: ç¯å¢ƒè®¾ç½®ä¸åº“å¯¼å…¥\n",
    "\n",
    "æ­¤å•å…ƒæ ¼è´Ÿè´£å¯¼å…¥æ‰€æœ‰å¿…éœ€çš„åº“å¹¶è®¾ç½®åˆå§‹ç¯å¢ƒã€‚å®ƒæ•´åˆäº†é¡¹ç›®æ‰€éœ€çš„æ‰€æœ‰ä¾èµ–é¡¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ‡å‡†åº“å¯¼å…¥\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import random\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR, ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- é¡¹ç›®å†…éƒ¨æ¨¡å—å¯¼å…¥ ---\n",
    "print(\"æ‰€æœ‰æ¨¡å—å·²å‡†å¤‡å°±ç»ªã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 2: å®éªŒé…ç½®\n",
    "\n",
    "**è¿™æ˜¯æ‚¨å”¯ä¸€éœ€è¦ä¿®æ”¹çš„å•å…ƒæ ¼ã€‚**\n",
    "\n",
    "è¯·å°† `CONFIG_PATH`å˜é‡è®¾ç½®ä¸ºæ‚¨æƒ³è¦ä½¿ç”¨çš„é…ç½®æ–‡ä»¶çš„è·¯å¾„ã€‚æ‰€æœ‰å®éªŒå‚æ•°éƒ½å°†ä»æ­¤æ–‡ä»¶åŠ è½½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== æ ¸å¿ƒé…ç½® ==================\n",
    "# ğŸ”¥ åªéœ€ä¿®æ”¹æ­¤å¤„çš„é…ç½®æ–‡ä»¶è·¯å¾„å³å¯å¼€å§‹æ–°çš„å®éªŒ\n",
    "CONFIG_PATH = 'config/default_configs/shl_config.yaml'  # é»˜è®¤ä½¿ç”¨SHLé…ç½®æ–‡ä»¶\n",
    "# ==============================================\n",
    "\n",
    "# æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "if not os.path.exists(CONFIG_PATH):\n",
    "    print(f\"âŒ é”™è¯¯: é…ç½®æ–‡ä»¶ '{CONFIG_PATH}' æœªæ‰¾åˆ°!\")\n",
    "    print(\"è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ï¼Œæˆ–åˆ›å»ºä¸€ä¸ªæ–°çš„é…ç½®æ–‡ä»¶ã€‚\")\n",
    "else:\n",
    "    print(f\"âœ“ å°†ä½¿ç”¨é…ç½®æ–‡ä»¶: '{CONFIG_PATH}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 3: æ ¸å¿ƒè®­ç»ƒå™¨ç±»\n",
    "\n",
    "ä¸‹é¢çš„ `ConfigurableTrainer` ç±»æ˜¯æ•´ä¸ªè®­ç»ƒæµç¨‹çš„æ ¸å¿ƒã€‚å®ƒå°è£…äº†ä»é…ç½®åŠ è½½ã€ç¯å¢ƒè®¾ç½®ã€æ•°æ®å¤„ç†ã€æ¨¡å‹åˆ›å»ºã€è®­ç»ƒã€è¯„ä¼°åˆ°ç»“æœå¯è§†åŒ–çš„æ‰€æœ‰é€»è¾‘ã€‚æ‚¨æ— éœ€ä¿®æ”¹æ­¤ç±»ä¸­çš„ä»»ä½•ä»£ç ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurableTrainer:\n",
    "    def __init__(self, config_path: str):\n",
    "        \"\"\"åˆå§‹åŒ–è®­ç»ƒå™¨\"\"\"\n",
    "        self.config_path = config_path\n",
    "        self.config = self._load_config(config_path)\n",
    "        self.params = self._extract_params()\n",
    "        self.history = {\n",
    "            'train_loss': [], 'train_accuracy': [],\n",
    "            'val_loss': [], 'val_accuracy': [], 'val_f1': []\n",
    "        }\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.criterion = None\n",
    "        self.device = None\n",
    "        self.logger = None\n",
    "\n",
    "    def _load_config(self, config_path: str):\n",
    "        \"\"\"åŠ è½½é…ç½®æ–‡ä»¶\"\"\"\n",
    "        import yaml\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            return yaml.safe_load(f)\n",
    "\n",
    "    def _extract_params(self):\n",
    "        \"\"\"æå–å‚æ•°\"\"\"\n",
    "        params = {}\n",
    "        # æ•°æ®é›†å‚æ•°\n",
    "        dataset_config = self.config.get('dataset', {})\n",
    "        params.update({\n",
    "            'name': dataset_config.get('name', 'SHL'),\n",
    "            'path': dataset_config.get('path', './datasets/datasetStandardized/SHL_Multimodal/'),\n",
    "            'window_size': dataset_config.get('preprocessing', {}).get('window_size', 128),\n",
    "            'step_size': dataset_config.get('preprocessing', {}).get('step_size', 64),\n",
    "            'sample_rate': dataset_config.get('preprocessing', {}).get('sample_rate', 100),\n",
    "            'modalities': dataset_config.get('modalities', {}),\n",
    "            'activity_labels': list(dataset_config.get('activity_labels', {}).values())\n",
    "        })\n",
    "        \n",
    "        # è®­ç»ƒå‚æ•°\n",
    "        training_config = self.config.get('training', {})\n",
    "        params.update({\n",
    "            'epochs': training_config.get('epochs', 100),\n",
    "            'batch_size': training_config.get('batch_size', 64),  # å¢åŠ æ‰¹å¤§å°\n",
    "            'learning_rate': training_config.get('learning_rate', 0.0005),  # é™ä½å­¦ä¹ ç‡\n",
    "            'weight_decay': training_config.get('weight_decay', 0.01),  # å¢åŠ æ­£åˆ™åŒ–\n",
    "            'optimizer_name': training_config.get('optimizer', 'AdamW'),\n",
    "            'scheduler_name': training_config.get('scheduler', 'step'),\n",
    "            'label_smoothing': training_config.get('label_smoothing', 0.05),  # é™ä½æ ‡ç­¾å¹³æ»‘\n",
    "            'gradient_clip_norm': training_config.get('gradient_clip_norm', 1.0),\n",
    "            'early_stopping_patience': training_config.get('early_stopping', {}).get('patience', 25)\n",
    "        })\n",
    "        \n",
    "        # ç¯å¢ƒå‚æ•°\n",
    "        env_config = self.config.get('environment', {})\n",
    "        params.update({\n",
    "            'seed': env_config.get('seed', 42),\n",
    "            'device': env_config.get('device', 'auto'),\n",
    "            'num_workers': env_config.get('num_workers', 4)\n",
    "        })\n",
    "        \n",
    "        # è¾“å‡ºé…ç½®\n",
    "        params.update({\n",
    "            'output_dir': f\"./results/{params['name']}_fixed\",\n",
    "            'save_checkpoints': True,\n",
    "            'verbose': True\n",
    "        })\n",
    "        \n",
    "        return params\n",
    "\n",
    "    def setup_environment(self):\n",
    "        \"\"\"è®¾ç½®ç¯å¢ƒå’Œæ—¥å¿—è®°å½•\"\"\"\n",
    "        import random\n",
    "        \n",
    "        # è®¾ç½®éšæœºç§å­\n",
    "        seed = self.params.get('seed', 42)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "        # è®¾ç½®è®¾å¤‡\n",
    "        device_pref = self.params.get('device', 'auto')\n",
    "        if device_pref == 'auto':\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device_pref)\n",
    "        \n",
    "        # åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "        self.output_dir = Path(self.params.get('output_dir', './results/shl_fixed'))\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # è®¾ç½®æ—¥å¿—\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO, \n",
    "            format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(self.output_dir / 'training.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger()\n",
    "        self.logger.info(f\"ç¯å¢ƒè®¾ç½®å®Œæˆã€‚è®¾å¤‡: {self.device}, è¾“å‡ºç›®å½•: {self.output_dir}\")\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"ä¿®å¤æ•°æ®åŠ è½½é€»è¾‘\"\"\"\n",
    "        self.logger.info(f\"åŠ è½½SHLæ•°æ®é›†: {self.params['name']}\")\n",
    "        \n",
    "        # å¼•å…¥æ•°æ®å±‚æ¨¡å—\n",
    "        from data_layer import SHLDataParser, UniversalHARDataset\n",
    "        \n",
    "        # é…ç½®æ•°æ®è§£æå™¨\n",
    "        parser_config = {\n",
    "            'name': 'SHL',\n",
    "            'data_path': self.params.get('path', './datasets/datasetStandardized/SHL_Multimodal/'),\n",
    "            'window_size': self.params.get('window_size', 128),\n",
    "            'step_size': self.params.get('step_size', 64),\n",
    "            'sample_rate': self.params.get('sample_rate', 100),\n",
    "            'normalize_per_sample': True,  # å¯ç”¨æ ·æœ¬çº§æ ‡å‡†åŒ–\n",
    "            'modalities': {\n",
    "                'imu': {'enabled': True, 'channels': 6},\n",
    "                'pressure': {'enabled': True, 'channels': 1}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # åˆ›å»ºæ•°æ®è§£æå™¨å’Œæ•°æ®é›†\n",
    "        parser = SHLDataParser(parser_config)\n",
    "        train_dataset = UniversalHARDataset(parser, split='train')\n",
    "        dev_dataset = UniversalHARDataset(parser, split='val')\n",
    "        test_dataset = UniversalHARDataset(parser, split='test')\n",
    "        \n",
    "        self.logger.info(f\"æ•°æ®é›†å¤§å° - è®­ç»ƒ: {len(train_dataset)}, éªŒè¯: {len(dev_dataset)}, æµ‹è¯•: {len(test_dataset)}\")\n",
    "        \n",
    "        # åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.params['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=self.params['num_workers'],\n",
    "            pin_memory=True,\n",
    "            drop_last=True  # ç¡®ä¿æ‰¹å¤§å°ä¸€è‡´\n",
    "        )\n",
    "        \n",
    "        dev_loader = DataLoader(\n",
    "            dev_dataset, \n",
    "            batch_size=self.params['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=self.params['num_workers'],\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=self.params['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=self.params['num_workers'],\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # æ”¶é›†è®­ç»ƒæ ‡ç­¾ç”¨äºè®¡ç®—ç±»åˆ«æƒé‡\n",
    "        self.logger.info(\"æ”¶é›†è®­ç»ƒæ ‡ç­¾...\")\n",
    "        train_labels = []\n",
    "        for _, labels in train_loader:\n",
    "            train_labels.extend(labels.numpy())\n",
    "        train_labels = np.array(train_labels)\n",
    "        \n",
    "        # æ‰“å°ç±»åˆ«åˆ†å¸ƒ\n",
    "        unique_labels, counts = np.unique(train_labels, return_counts=True)\n",
    "        self.logger.info(f\"ç±»åˆ«åˆ†å¸ƒ: {dict(zip(unique_labels, counts))}\")\n",
    "        \n",
    "        return train_loader, dev_loader, test_loader, train_labels\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"æ„å»ºåŠ¨æ€æ¨¡å‹\"\"\"\n",
    "        self.logger.info(\"æ„å»ºåŠ¨æ€HARæ¨¡å‹...\")\n",
    "        \n",
    "        from model_layer.dynamic_har_model import DynamicHarModel\n",
    "        \n",
    "        # ä¿®å¤æ¨¡å‹é…ç½®\n",
    "        model_config = self.config.copy()\n",
    "        # ç¡®ä¿architectureé…ç½®æ­£ç¡®\n",
    "        if 'architecture' in model_config:\n",
    "            arch_config = model_config['architecture']\n",
    "            # ä¿®æ­£ä¸“å®¶é…ç½®ä¸­çš„ç±»å‹åç§°\n",
    "            if 'experts' in arch_config:\n",
    "                for expert_name, expert_config in arch_config['experts'].items():\n",
    "                    if 'type' in expert_config:\n",
    "                        # ç¡®ä¿ç±»å‹åç§°å°å†™ä¸”æ­£ç¡®\n",
    "                        expert_type = expert_config['type'].lower()\n",
    "                        if expert_type in ['transformerexpert', 'transformer_expert']:\n",
    "                            expert_config['type'] = 'transformer'\n",
    "                        elif expert_type in ['rnnexpert', 'rnn_expert']:\n",
    "                            expert_config['type'] = 'rnn'\n",
    "                        elif expert_type in ['cnnexpert', 'cnn_expert']:\n",
    "                            expert_config['type'] = 'cnn'\n",
    "        \n",
    "        self.model = DynamicHarModel(model_config)\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # æ‰“å°æ¨¡å‹ä¿¡æ¯\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        self.logger.info(f\"æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}\")\n",
    "        self.logger.info(f\"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\")\n",
    "\n",
    "    def setup_training_components(self, train_labels: np.ndarray):\n",
    "        \"\"\"è®¾ç½®è®­ç»ƒç»„ä»¶ï¼ˆä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°ç­‰ï¼‰\"\"\"\n",
    "        self.logger.info(\"è®¾ç½®è®­ç»ƒç»„ä»¶...\")\n",
    "        \n",
    "        # è®¡ç®—ç±»åˆ«æƒé‡ä»¥å¤„ç†æ•°æ®ä¸å¹³è¡¡\n",
    "        classes = np.unique(train_labels)\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced', \n",
    "            classes=classes, \n",
    "            y=train_labels\n",
    "        )\n",
    "        class_weights_tensor = torch.FloatTensor(class_weights).to(self.device)\n",
    "        self.logger.info(f\"ç±»åˆ«æƒé‡: {dict(zip(classes, class_weights))}\")\n",
    "        \n",
    "        # æŸå¤±å‡½æ•°\n",
    "        self.criterion = nn.CrossEntropyLoss(\n",
    "            weight=class_weights_tensor,\n",
    "            label_smoothing=self.params['label_smoothing']\n",
    "        )\n",
    "        \n",
    "        # ä¼˜åŒ–å™¨\n",
    "        optimizer_name = self.params['optimizer_name'].lower()\n",
    "        if optimizer_name == 'adamw':\n",
    "            self.optimizer = optim.AdamW(\n",
    "                self.model.parameters(),\n",
    "                lr=self.params['learning_rate'],\n",
    "                weight_decay=self.params['weight_decay'],\n",
    "                betas=(0.9, 0.999),\n",
    "                eps=1e-8\n",
    "            )\n",
    "        elif optimizer_name == 'adam':\n",
    "            self.optimizer = optim.Adam(\n",
    "                self.model.parameters(),\n",
    "                lr=self.params['learning_rate'],\n",
    "                weight_decay=self.params['weight_decay']\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {optimizer_name}\")\n",
    "        \n",
    "        # å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "        scheduler_name = self.params['scheduler_name']\n",
    "        if scheduler_name == 'step':\n",
    "            self.scheduler = optim.lr_scheduler.StepLR(\n",
    "                self.optimizer, \n",
    "                step_size=50, \n",
    "                gamma=0.5\n",
    "            )\n",
    "        elif scheduler_name == 'cosine':\n",
    "            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                self.optimizer, \n",
    "                T_max=self.params['epochs']\n",
    "            )\n",
    "        elif scheduler_name == 'plateau':\n",
    "            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, \n",
    "                mode='min', \n",
    "                patience=10,\n",
    "                factor=0.5\n",
    "            )\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "        \n",
    "        self.logger.info(f\"è®­ç»ƒç»„ä»¶è®¾ç½®å®Œæˆ: ä¼˜åŒ–å™¨={optimizer_name}, è°ƒåº¦å™¨={scheduler_name}\")\n",
    "\n",
    "    def train(self, train_loader, dev_loader):\n",
    "        \"\"\"ä¿®å¤è®­ç»ƒå¾ªç¯\"\"\"\n",
    "        self.logger.info(\"å¼€å§‹è®­ç»ƒ...\")\n",
    "        \n",
    "        best_val_f1 = 0.0\n",
    "        patience_counter = 0\n",
    "        patience = self.params['early_stopping_patience']\n",
    "        \n",
    "        for epoch in range(self.params['epochs']):\n",
    "            # è®­ç»ƒé˜¶æ®µ\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for batch_idx, (data_dict, targets) in enumerate(train_loader):\n",
    "                # å°†æ•°æ®ç§»åˆ°è®¾å¤‡\n",
    "                data_dict = {k: v.to(self.device) for k, v in data_dict.items()}\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                # å‰å‘ä¼ æ’­\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                try:\n",
    "                    outputs = self.model(data_dict)\n",
    "                    loss = self.criterion(outputs, targets)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"å‰å‘ä¼ æ’­é”™è¯¯: {e}\")\n",
    "                    # æ‰“å°è°ƒè¯•ä¿¡æ¯\n",
    "                    for k, v in data_dict.items():\n",
    "                        self.logger.error(f\"è¾“å…¥ {k} å½¢çŠ¶: {v.shape}\")\n",
    "                    raise e\n",
    "                \n",
    "                # åå‘ä¼ æ’­\n",
    "                loss.backward()\n",
    "                \n",
    "                # æ¢¯åº¦è£å‰ª\n",
    "                if self.params['gradient_clip_norm'] > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(),\n",
    "                        self.params['gradient_clip_norm']\n",
    "                    )\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # ç»Ÿè®¡\n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                train_total += targets.size(0)\n",
    "                train_correct += (predicted == targets).sum().item()\n",
    "                \n",
    "                # æ¯100ä¸ªbatchæ‰“å°ä¸€æ¬¡è¿›åº¦\n",
    "                if batch_idx % 100 == 0:\n",
    "                    self.logger.info(f\"Epoch {epoch+1}/{self.params['epochs']}, \"\n",
    "                                      f\"Batch {batch_idx}/{len(train_loader)}, \"\n",
    "                                      f\"Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # æ›´æ–°å­¦ä¹ ç‡\n",
    "            if self.scheduler and not isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            # éªŒè¯é˜¶æ®µ\n",
    "            val_loss, val_f1, val_acc = self.evaluate(dev_loader, is_test=False)\n",
    "            train_acc = train_correct / train_total\n",
    "            train_loss_avg = train_loss / len(train_loader)\n",
    "            \n",
    "            # å¦‚æœä½¿ç”¨ReduceLROnPlateauï¼Œåœ¨è¿™é‡Œæ›´æ–°\n",
    "            if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                self.scheduler.step(val_loss)\n",
    "            \n",
    "            # è®°å½•å†å²\n",
    "            self.history['train_loss'].append(train_loss_avg)\n",
    "            self.history['train_accuracy'].append(train_acc)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_accuracy'].append(val_acc)\n",
    "            self.history['val_f1'].append(val_f1)\n",
    "            \n",
    "            # æ‰“å°è®­ç»ƒä¿¡æ¯\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            self.logger.info(\n",
    "                f\"Epoch {epoch+1}/{self.params['epochs']} | \"\n",
    "                f\"Train Loss: {train_loss_avg:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "                f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f} | \"\n",
    "                f\"LR: {current_lr:.6f}\"\n",
    "            )\n",
    "            \n",
    "            # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                patience_counter = 0\n",
    "                if self.params.get('save_checkpoints', True):\n",
    "                    torch.save(self.model.state_dict(), self.output_dir / 'best_model.pth')\n",
    "                    self.logger.info(f\"æ–°æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼ŒéªŒè¯F1åˆ†æ•°: {best_val_f1:.4f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # æ—©åœæ£€æŸ¥\n",
    "            if patience_counter >= patience:\n",
    "                self.logger.info(f\"æ—©åœè§¦å‘! æœ€ä½³éªŒè¯F1: {best_val_f1:.4f}\")\n",
    "                break\n",
    "        \n",
    "        self.logger.info(f\"è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯F1: {best_val_f1:.4f}\")\n",
    "\n",
    "    def evaluate(self, data_loader, is_test=False):\n",
    "        \"\"\"è¯„ä¼°æ–¹æ³• - æ”¯æŒå¤šæ¨¡æ€æ•°æ®\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data_dict, targets in data_loader:\n",
    "                data_dict = {k: v.to(self.device) for k, v in data_dict.items()}\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                try:\n",
    "                    outputs = self.model(data_dict)\n",
    "                    loss = self.criterion(outputs, targets)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"è¯„ä¼°æ—¶å‰å‘ä¼ æ’­é”™è¯¯: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        accuracy = accuracy_score(all_targets, all_preds)\n",
    "        f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "        \n",
    "        if is_test:\n",
    "            # æ‰“å°è¯¦ç»†çš„æµ‹è¯•ç»“æœ\n",
    "            from sklearn.metrics import classification_report, confusion_matrix\n",
    "            self.logger.info(\"æµ‹è¯•é›†è¯¦ç»†ç»“æœ:\")\n",
    "            self.logger.info(f\"å‡†ç¡®ç‡: {accuracy:.4f}\")\n",
    "            self.logger.info(f\"F1åˆ†æ•°: {f1:.4f}\")\n",
    "            \n",
    "            # åˆ†ç±»æŠ¥å‘Š\n",
    "            target_names = self.params.get('activity_labels', [f'Class_{i}' for i in range(8)])\n",
    "            report = classification_report(all_targets, all_preds, target_names=target_names)\n",
    "            self.logger.info(f\"åˆ†ç±»æŠ¥å‘Š:\\n{report}\")\n",
    "            \n",
    "            # æ··æ·†çŸ©é˜µ\n",
    "            cm = confusion_matrix(all_targets, all_preds)\n",
    "            self.logger.info(f\"æ··æ·†çŸ©é˜µ:\\n{cm}\")\n",
    "        \n",
    "        return avg_loss, f1, accuracy\n",
    "\n",
    "    def plot_learning_curves(self):\n",
    "        \"\"\"ç»˜åˆ¶å­¦ä¹ æ›²çº¿\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # æŸå¤±æ›²çº¿\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(self.history['train_loss'], label='Train Loss')\n",
    "        plt.plot(self.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Loss vs. Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # å‡†ç¡®ç‡æ›²çº¿\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(self.history['train_accuracy'], label='Train Accuracy')\n",
    "        plt.plot(self.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Accuracy vs. Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # F1åˆ†æ•°æ›²çº¿\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(self.history['val_f1'], label='Validation F1', color='orange')\n",
    "        plt.title('F1 Score vs. Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_path = self.output_dir / 'learning_curves.png'\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        self.logger.info(f\"å­¦ä¹ æ›²çº¿å·²ä¿å­˜è‡³ {save_path}\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred, labels):\n",
    "        \"\"\"ç»˜åˆ¶æ··æ·†çŸ©é˜µ\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=labels, yticklabels=labels)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        save_path = self.output_dir / 'confusion_matrix.png'\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        self.logger.info(f\"æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³ {save_path}\")\n",
    "        plt.show()\n",
    "\n",
    "    def validate_shl_config(self):\n",
    "        \"\"\"éªŒè¯SHLé…ç½®æ–‡ä»¶çš„å®Œæ•´æ€§\"\"\"\n",
    "        required_sections = ['dataset', 'architecture', 'training']\n",
    "        \n",
    "        for section in required_sections:\n",
    "            if section not in self.config:\n",
    "                raise ValueError(f\"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„ '{section}' éƒ¨åˆ†\")\n",
    "        \n",
    "        # éªŒè¯æ•°æ®é›†é…ç½®\n",
    "        dataset_config = self.config['dataset']\n",
    "        if dataset_config['name'] != 'SHL':\n",
    "            raise ValueError(\"é…ç½®æ–‡ä»¶ä¸æ˜¯ä¸ºSHLæ•°æ®é›†è®¾è®¡çš„\")\n",
    "        \n",
    "        # éªŒè¯æ¨¡æ€é…ç½®\n",
    "        modalities = dataset_config.get('modalities', {})\n",
    "        required_modalities = ['imu', 'pressure']\n",
    "        for modality in required_modalities:\n",
    "            if modality not in modalities or not modalities[modality].get('enabled', False):\n",
    "                raise ValueError(f\"SHLé…ç½®å¿…é¡»å¯ç”¨ '{modality}' æ¨¡æ€\")\n",
    "        \n",
    "        # éªŒè¯ä¸“å®¶é…ç½®\n",
    "        experts = self.config['architecture']['experts']\n",
    "        expected_experts = ['imu_expert', 'pressure_expert']\n",
    "        for expert in expected_experts:\n",
    "            if expert not in experts:\n",
    "                raise ValueError(f\"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„ä¸“å®¶æ¨¡å‹: {expert}\")\n",
    "        \n",
    "        self.logger.info(\"âœ“ SHLé…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"å®Œæ•´çš„è®­ç»ƒæµç¨‹ - SHLå¤šæ¨¡æ€ç‰ˆæœ¬\"\"\"\n",
    "        try:\n",
    "            self.setup_environment()\n",
    "            \n",
    "            # å¼ºåˆ¶éªŒè¯é…ç½®\n",
    "            self.validate_shl_config()\n",
    "            \n",
    "            self.logger.info(\"å¼€å§‹SHLå¤šæ¨¡æ€è®­ç»ƒå®éªŒ\")\n",
    "            self.logger.info(f\"ä½¿ç”¨é…ç½®: {self.config_path}\")\n",
    "            \n",
    "            # åŠ è½½æ•°æ®\n",
    "            train_loader, dev_loader, test_loader, train_labels = self.load_data()\n",
    "            \n",
    "            # æ„å»ºæ¨¡å‹\n",
    "            self.build_model()\n",
    "            \n",
    "            # è®¾ç½®è®­ç»ƒç»„ä»¶\n",
    "            self.setup_training_components(train_labels)\n",
    "            \n",
    "            # è®­ç»ƒ\n",
    "            self.train(train_loader, dev_loader)\n",
    "            \n",
    "            # ç»˜åˆ¶å­¦ä¹ æ›²çº¿\n",
    "            self.plot_learning_curves()\n",
    "            \n",
    "            # æœ€ç»ˆæµ‹è¯•\n",
    "            self.logger.info(\"è¿›è¡Œæœ€ç»ˆæµ‹è¯•...\")\n",
    "            \n",
    "            # åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "            best_model_path = self.output_dir / 'best_model.pth'\n",
    "            if best_model_path.exists():\n",
    "                self.model.load_state_dict(torch.load(best_model_path))\n",
    "                self.logger.info(\"å·²åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•\")\n",
    "            \n",
    "            test_loss, test_f1, test_acc = self.evaluate(test_loader, is_test=True)\n",
    "            self.logger.info(f\"æœ€ç»ˆæµ‹è¯•ç»“æœ - Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, F1: {test_f1:.4f}\")\n",
    "            \n",
    "            self.logger.info(\"SHLå¤šæ¨¡æ€è®­ç»ƒå®éªŒå®Œæˆ!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}\")\n",
    "            import traceback\n",
    "            self.logger.error(f\"é”™è¯¯å †æ ˆ:\\n{traceback.format_exc()}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 4: æ‰§è¡Œè®­ç»ƒæµç¨‹\n",
    "\n",
    "æœ€åï¼Œæˆ‘ä»¬å®ä¾‹åŒ– `ConfigurableTrainer` ç±»å¹¶è°ƒç”¨å…¶ `run` æ–¹æ³•æ¥å¯åŠ¨æ•´ä¸ªè®­ç»ƒå’Œè¯„ä¼°æµç¨‹ã€‚æ‰€æœ‰æ“ä½œéƒ½å°†ç”±ä¹‹å‰åŠ è½½çš„é…ç½®é©±åŠ¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸»æ‰§è¡Œä»£ç \n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # ä½¿ç”¨ä¿®å¤åçš„SHLé…ç½®æ–‡ä»¶\n",
    "        config_file = 'config/default_configs/shl_config.yaml'\n",
    "        \n",
    "        if not os.path.exists(config_file):\n",
    "            print(f\"âŒ SHLé…ç½®æ–‡ä»¶ {config_file} ä¸å­˜åœ¨!\")\n",
    "            print(\"è¯·å…ˆåˆ›å»ºSHLæ•°æ®é›†çš„é…ç½®æ–‡ä»¶\")\n",
    "        else:\n",
    "            print(f\"âœ“ ä½¿ç”¨SHLé…ç½®æ–‡ä»¶: {config_file}\")\n",
    "            trainer = ConfigurableTrainer(config_path=config_file)\n",
    "            trainer.run()\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nç”¨æˆ·ä¸­æ–­è®­ç»ƒ\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nSHLè®­ç»ƒæµç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HAR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
