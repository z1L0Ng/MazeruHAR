{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "config-driven-header",
   "metadata": {},
   "source": [
    "# 配置驱动的HAR训练流程 - 任务2.3完整实现\n",
    "\n",
    "这个notebook实现了完全由配置文件驱动的训练执行引擎。\n",
    "通过修改配置文件即可启动不同的实验，无需修改代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844a56b8",
   "metadata": {},
   "source": [
    "## 步骤 1: 导入所有必需的库\n",
    "首先，我们导入所有需要的标准库、第三方库和项目内部模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准库导入\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "# 第三方库导入\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 项目内部导入 (占位符) ---\n",
    "# 在这个独立的notebook中，我们将必要的辅助函数和模型直接包含进来\n",
    "#真实的模块导入\n",
    "try:\n",
    "    from config.config_loader import ConfigLoader\n",
    "    from config.config_bridge import ConfigBridge\n",
    "except ImportError:\n",
    "    print(\"警告: 配置模块未找到，将只使用传统模式\")\n",
    "    ConfigLoader = None\n",
    "    ConfigBridge = None\n",
    "\n",
    "\n",
    "print(\"所有模块导入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b2e1f8",
   "metadata": {},
   "source": [
    "## 步骤 2: 定义辅助工具、模型和配置桥接器\n",
    "为了使此Notebook可以独立运行，我们将原本在 `utils_torch.py`、`model_cbranchformer.py` 和 `config_bridge.py` 中的关键代码直接定义在这里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e89d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 从 utils_torch.py 移入的关键代码 ===\n",
    "class HARDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "def load_dataset_pytorch(dataset_name, client_count, data_config, random_seed, base_path):\n",
    "    # 这是一个简化的数据加载器，用于演示目的\n",
    "    # 它会生成随机数据来模拟真实的数据集加载\n",
    "    print(f\"正在生成 {dataset_name} 的模拟数据...\")\n",
    "    \n",
    "    # 根据数据集名称定义参数\n",
    "    if dataset_name.upper() == 'UCI':\n",
    "        train_samples, test_samples = 7352, 2947\n",
    "        seq_len, channels = 128, 6\n",
    "    else: # 默认或其它\n",
    "        train_samples, test_samples = 10000, 2000\n",
    "        seq_len, channels = 100, 9\n",
    "    \n",
    "    # 创建模拟数据\n",
    "    train_data = torch.randn(train_samples, seq_len, channels)\n",
    "    train_label = torch.randint(0, 6, (train_samples,))\n",
    "    test_data = torch.randn(test_samples, seq_len, channels)\n",
    "    test_label = torch.randint(0, 6, (test_samples,))\n",
    "\n",
    "    class MockDataset:\n",
    "        def __init__(self):\n",
    "            self.central_train_data = train_data\n",
    "            self.central_train_label = train_label\n",
    "            self.central_test_data = test_data\n",
    "            self.central_test_label = test_label\n",
    "            self.central_dev_data = None # 让主程序自己分割验证集\n",
    "            self.central_dev_label = None\n",
    "\n",
    "    return MockDataset()\n",
    "\n",
    "def return_client_by_dataset(dataset_name):\n",
    "    # 模拟函数\n",
    "    return 1 # 集中式训练\n",
    "\n",
    "utils = type('utils', (), {'HARDataset': HARDataset, 'load_dataset_pytorch': load_dataset_pytorch, 'return_client_by_dataset': return_client_by_dataset})\n",
    "\n",
    "# === 从 model_cbranchformer.py 移入的关键代码 (简化版) ===\n",
    "class cbranchformer_har_base(nn.Module):\n",
    "    def __init__(self, input_shape, activity_count, **kwargs):\n",
    "        super().__init__()\n",
    "        # 这是一个非常简化的模型结构，用于演示\n",
    "        self.flatten = nn.Flatten()\n",
    "        in_features = input_shape[0] * input_shape[1]\n",
    "        self.fc1 = nn.Linear(in_features, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(kwargs.get('dropout_rate', 0.5))\n",
    "        self.fc2 = nn.Linear(128, activity_count)\n",
    "        print(f\"创建了一个简化的 cbranchformer_har_base 模型\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class MobileHART_XS(nn.Module):\n",
    "    def __init__(self, input_shape, activity_count, **kwargs):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        in_features = input_shape[0] * input_shape[1]\n",
    "        self.fc1 = nn.Linear(in_features, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, activity_count)\n",
    "        print(f\"创建了一个简化的 MobileHART_XS 模型\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = type('model', (), {'cbranchformer_har_base': cbranchformer_har_base, 'MobileHART_XS': MobileHART_XS})\n",
    "\n",
    "# === 从 config_bridge.py 移入的关键代码 ===\n",
    "class ConfigBridge:\n",
    "    def __init__(self, config_path: str):\n",
    "        \"\"\"改进的ConfigBridge初始化\"\"\"\n",
    "        try:\n",
    "            with open(config_path, 'r', encoding='utf-8') as f:\n",
    "                self.raw_config = yaml.safe_load(f)\n",
    "            self.config = self._to_dot_notation(self.raw_config)\n",
    "            self.use_new_config = True\n",
    "            print(f\"✓ 配置文件加载成功: {config_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"✗ 配置文件不存在: {config_path}\")\n",
    "            raise\n",
    "        except yaml.YAMLError as e:\n",
    "            print(f\"✗ YAML解析错误: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"✗ 配置加载失败: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _to_dot_notation(self, data):\n",
    "        \"\"\"将字典转换为点表示法对象，修复values()方法缺失问题\"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            # 创建一个自定义类，支持字典操作\n",
    "            class DotDict:\n",
    "                def __init__(self, data_dict):\n",
    "                    self._dict = data_dict\n",
    "                    for k, v in data_dict.items():\n",
    "                        setattr(self, k, self._to_dot_notation(v))\n",
    "                \n",
    "                def values(self):\n",
    "                    return [getattr(self, k) for k in self._dict.keys()]\n",
    "                \n",
    "                def keys(self):\n",
    "                    return self._dict.keys()\n",
    "                \n",
    "                def items(self):\n",
    "                    return [(k, getattr(self, k)) for k in self._dict.keys()]\n",
    "                \n",
    "                def get(self, key, default=None):\n",
    "                    return getattr(self, key, default)\n",
    "                \n",
    "                def _to_dot_notation(self, data):\n",
    "                    if isinstance(data, dict):\n",
    "                        return DotDict(data)\n",
    "                    elif isinstance(data, list):\n",
    "                        return [self._to_dot_notation(i) for i in data]\n",
    "                    else:\n",
    "                        return data\n",
    "            \n",
    "            return DotDict(data)\n",
    "        elif isinstance(data, list):\n",
    "            return [self._to_dot_notation(i) for i in data]\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "    def get_dataset_config(self) -> Dict[str, Any]:\n",
    "        return self.raw_config.get('dataset', {})\n",
    "\n",
    "    def get_training_config(self) -> Dict[str, Any]:\n",
    "        return self.raw_config.get('training', {})\n",
    "        \n",
    "    def get_visualization_config(self) -> Dict[str, Any]:\n",
    "        return self.raw_config.get('visualization', {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07368b",
   "metadata": {},
   "source": [
    "## 步骤 3: 定义核心训练器类 (已修复并整合)\n",
    "这里是修复和整合后的 `ConfigurableTrainer` 类。所有方法都被正确地定义在类中，解决了 `NameError` 并改善了代码结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rebuilt-trainer-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurableTrainer:\n",
    "    \"\"\"配置驱动的训练器类 - 完整实现任务2.3\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: Optional[str] = None):\n",
    "        \"\"\"初始化训练器\"\"\"\n",
    "        self.config_path = config_path\n",
    "        self.use_config = config_path is not None and ConfigBridge is not None\n",
    "        \n",
    "        if self.use_config:\n",
    "            try:\n",
    "                self.config_bridge = ConfigBridge(config_path)\n",
    "                self.config = self.config_bridge.config\n",
    "                self.use_config = self.config_bridge.use_new_config\n",
    "            except Exception as e:\n",
    "                print(f\"配置文件加载失败: {e}\")\n",
    "                print(\"回退到传统硬编码模式\")\n",
    "                self.use_config = False\n",
    "                self.config_bridge = None\n",
    "                self.config = None\n",
    "        else:\n",
    "            self.config_bridge = None\n",
    "            self.config = None\n",
    "        \n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.criterion = None\n",
    "        self.device = None\n",
    "        self.output_dir = None\n",
    "        self.logger = None\n",
    "        self.gradient_clip_norm = 0\n",
    "        \n",
    "        self.history = {\n",
    "            'train_loss': [], 'train_accuracy': [],\n",
    "            'val_loss': [], 'val_accuracy': []\n",
    "        }\n",
    "        \n",
    "        print(f\"训练器初始化完成，使用配置模式: {'新配置系统' if self.use_config else '传统硬编码'}\")\n",
    "\n",
    "    def setup_logging(self, output_dir: Path, verbose: bool = True):\n",
    "        \"\"\"设置日志系统\"\"\"\n",
    "        log_level = logging.INFO if verbose else logging.WARNING\n",
    "        log_dir = output_dir / 'logs'\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for handler in logging.root.handlers[:]:\n",
    "            logging.root.removeHandler(handler)\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=log_level,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_dir / 'training.log'),\n",
    "                logging.StreamHandler(sys.stdout)\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.info(\"日志系统初始化完成\")\n",
    "\n",
    "    def get_parameters(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取训练参数，支持配置文件和硬编码两种模式\"\"\"\n",
    "        if self.use_config:\n",
    "            return self._get_config_parameters()\n",
    "        else:\n",
    "            return self._get_hardcoded_parameters()\n",
    "\n",
    "    def _get_config_parameters(self) -> Dict[str, Any]:\n",
    "        \"\"\"从配置文件获取参数\"\"\"\n",
    "        try:\n",
    "            dataset_config = self.config_bridge.get_dataset_config()\n",
    "            training_config = self.config_bridge.get_training_config()\n",
    "            visualization_config = self.config_bridge.get_visualization_config()\n",
    "            \n",
    "            if not dataset_config or not training_config:\n",
    "                raise ValueError(\"配置文件不完整\")\n",
    "            \n",
    "            total_channels = sum(mod['channels'] for mod in dataset_config['modalities'])\n",
    "            \n",
    "            experts_config = self.config.architecture.experts\n",
    "            main_expert = list(experts_config.values())[0]\n",
    "            architecture_type = main_expert.type.upper()\n",
    "            \n",
    "            return {\n",
    "                'experiment_name': self.config.name,\n",
    "                'description': getattr(self.config, 'description', ''),\n",
    "                'dataset_name': dataset_config['name'],\n",
    "                'data_config': 'BALANCED',\n",
    "                'activity_labels': dataset_config['activity_labels'],\n",
    "                'activity_count': len(dataset_config['activity_labels']),\n",
    "                'architecture': architecture_type,\n",
    "                'segment_size': dataset_config['modalities'][0]['sequence_length'],\n",
    "                'num_input_channels': total_channels,\n",
    "                'input_shape': (dataset_config['modalities'][0]['sequence_length'], total_channels),\n",
    "                'batch_size': training_config['batch_size'],\n",
    "                'learning_rate': training_config['learning_rate'],\n",
    "                'local_epoch': training_config['epochs'],\n",
    "                'dropout_rate': self.config.architecture.dropout_rate,\n",
    "                'weight_decay': training_config.get('weight_decay', 1e-4),\n",
    "                'gradient_clip_norm': training_config.get('gradient_clip_norm', 1.0),\n",
    "                'label_smoothing': training_config.get('label_smoothing', 0.1),\n",
    "                'optimizer_name': training_config.get('optimizer', 'adam'),\n",
    "                'scheduler_name': training_config.get('scheduler', 'cosine'),\n",
    "                'early_stopping_patience': training_config.get('early_stopping_patience', 10),\n",
    "                'projection_dim': getattr(main_expert.params, 'projection_dim', 192),\n",
    "                'frame_length': getattr(main_expert.params, 'frame_length', 16),\n",
    "                'time_step': getattr(main_expert.params, 'time_step', 16),\n",
    "                'filter_attention_head': getattr(main_expert.params, 'filter_attention_head', 4),\n",
    "                'conv_kernels': getattr(main_expert.params, 'conv_kernels', [3, 7, 15, 31, 31, 31]),\n",
    "                'token_based': getattr(main_expert.params, 'token_based', False),\n",
    "                'random_seed': self.config.seed,\n",
    "                'device': self.config.device,\n",
    "                'output_dir': self.config.output_dir,\n",
    "                'save_checkpoints': self.config.save_checkpoints,\n",
    "                'verbose': self.config.verbose,\n",
    "                'show_train_verbose': 1 if self.config.verbose else 0,\n",
    "                'plot_learning_curves': visualization_config.get('plot_learning_curves', True),\n",
    "                'plot_confusion_matrix': visualization_config.get('plot_confusion_matrix', True),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            error_msg = f\"配置参数获取失败: {e}\"\n",
    "            if self.logger: self.logger.error(error_msg)\n",
    "            else: print(f\"✗ {error_msg}\")\n",
    "            raise\n",
    "\n",
    "    def _get_hardcoded_parameters(self) -> Dict[str, Any]:\n",
    "        \"\"\"硬编码参数（向后兼容）\"\"\"\n",
    "        return {\n",
    "            'experiment_name': 'Hardcoded_Experiment',\n",
    "            'description': '使用硬编码参数的实验',\n",
    "            'dataset_name': 'UCI', 'data_config': 'BALANCED',\n",
    "            'activity_labels': ['Walk', 'Upstair', 'Downstair', 'Sit', 'Stand', 'Lay'],\n",
    "            'activity_count': 6, 'architecture': 'HART',\n",
    "            'segment_size': 128, 'num_input_channels': 6,\n",
    "            'input_shape': (128, 6), 'batch_size': 256,\n",
    "            'learning_rate': 5e-3, 'local_epoch': 50,\n",
    "            'dropout_rate': 0.3, 'weight_decay': 1e-4,\n",
    "            'gradient_clip_norm': 1.0, 'label_smoothing': 0.1,\n",
    "            'optimizer_name': 'adam', 'scheduler_name': 'cosine',\n",
    "            'early_stopping_patience': 10,\n",
    "            'projection_dim': 192, 'frame_length': 16, 'time_step': 16,\n",
    "            'filter_attention_head': 4, 'conv_kernels': [3, 7, 15, 31, 31, 31],\n",
    "            'token_based': False, 'random_seed': 42, 'device': 'auto',\n",
    "            'output_dir': './results/hardcoded_experiment',\n",
    "            'save_checkpoints': True, 'verbose': True, 'show_train_verbose': 1,\n",
    "            'plot_learning_curves': True, 'plot_confusion_matrix': True,\n",
    "        }\n",
    "\n",
    "    def setup_environment(self, params: Dict[str, Any]) -> None:\n",
    "        \"\"\"设置训练环境\"\"\"\n",
    "        try:\n",
    "            seed = params['random_seed']\n",
    "            random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "            \n",
    "            device_config = params['device']\n",
    "            if device_config == 'auto':\n",
    "                if torch.cuda.is_available(): self.device = torch.device('cuda')\n",
    "                elif torch.backends.mps.is_available(): self.device = torch.device('mps')\n",
    "                else: self.device = torch.device('cpu')\n",
    "            else: self.device = torch.device(device_config)\n",
    "            print(f\"使用设备: {self.device}\")\n",
    "            \n",
    "            self.output_dir = Path(params['output_dir'])\n",
    "            self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            self.setup_logging(self.output_dir, params['verbose'])\n",
    "            self.gradient_clip_norm = params.get('gradient_clip_norm', 0)\n",
    "            \n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.deterministic = False\n",
    "            self.logger.info(\"环境设置完成\")\n",
    "        except Exception as e:\n",
    "            print(f\"环境设置失败: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_data(self, params: Dict[str, Any]) -> Tuple[torch.utils.data.DataLoader, ...]:\n",
    "        \"\"\"加载数据集 (修复多进程问题)\"\"\"\n",
    "        self.logger.info(f\"开始加载数据集: {params['dataset_name']}\")\n",
    "        try:\n",
    "            client_count = utils.return_client_by_dataset(params['dataset_name'])\n",
    "            loaded_dataset = utils.load_dataset_pytorch(\n",
    "                params['dataset_name'], client_count, params['data_config'], \n",
    "                params['random_seed'], './datasets/'\n",
    "            )\n",
    "            \n",
    "            train_data, train_label = loaded_dataset.central_train_data, loaded_dataset.central_train_label\n",
    "            test_data, test_label = loaded_dataset.central_test_data, loaded_dataset.central_test_label\n",
    "            \n",
    "            if hasattr(loaded_dataset, 'central_dev_data') and loaded_dataset.central_dev_data is not None:\n",
    "                dev_data, dev_label = loaded_dataset.central_dev_data, loaded_dataset.central_dev_label\n",
    "            else:\n",
    "                train_np, dev_data, train_label_np, dev_label = train_test_split(\n",
    "                    train_data.numpy(), train_label.numpy(), test_size=0.125, \n",
    "                    random_state=params['random_seed'], stratify=train_label.numpy()\n",
    "                )\n",
    "                train_data, train_label = torch.FloatTensor(train_np), torch.LongTensor(train_label_np)\n",
    "                dev_data, dev_label = torch.FloatTensor(dev_data), torch.LongTensor(dev_label)\n",
    "            \n",
    "            train_dataset = utils.HARDataset(train_data, train_label)\n",
    "            dev_dataset = utils.HARDataset(dev_data, dev_label)\n",
    "            test_dataset = utils.HARDataset(test_data, test_label)\n",
    "            \n",
    "            common_loader_params = {'batch_size': params['batch_size'], 'num_workers': 0, 'pin_memory': False}\n",
    "            train_loader = DataLoader(train_dataset, shuffle=True, **common_loader_params)\n",
    "            dev_loader = DataLoader(dev_dataset, shuffle=False, **common_loader_params)\n",
    "            test_loader = DataLoader(test_dataset, shuffle=False, **common_loader_params)\n",
    "            \n",
    "            self.logger.info(f\"数据加载完成: 训练={len(train_dataset)}, 验证={len(dev_dataset)}, 测试={len(test_dataset)}\")\n",
    "            self.logger.info(\"注意: DataLoader使用单进程模式(num_workers=0)以保证兼容性\")\n",
    "            \n",
    "            return train_loader, dev_loader, test_loader, train_label.cpu().numpy()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"数据加载失败: {e}\"); raise\n",
    "\n",
    "    def create_model(self, params: Dict[str, Any]) -> nn.Module:\n",
    "        \"\"\"创建模型\"\"\"\n",
    "        self.logger.info(f\"创建模型: {params['architecture']}, 输入形状={params['input_shape']}, 类别数={params['activity_count']}\")\n",
    "        try:\n",
    "            model_map = {\n",
    "                \"HART\": model.cbranchformer_har_base,\n",
    "                \"MOBILEHART_XS\": model.MobileHART_XS\n",
    "            }\n",
    "            model_class = model_map.get(params['architecture'], model.MobileHART_XS)\n",
    "            self.model = model_class(\n",
    "                input_shape=params['input_shape'], activity_count=params['activity_count'], **params\n",
    "            ).to(self.device)\n",
    "            \n",
    "            total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "            self.logger.info(f\"模型创建完成，总参数数量: {total_params:,}\")\n",
    "            return self.model\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"模型创建失败: {e}\"); raise\n",
    "\n",
    "    def setup_training(self, params: Dict[str, Any], train_labels: np.ndarray) -> None:\n",
    "        \"\"\"设置训练组件\"\"\"\n",
    "        try:\n",
    "            weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "            class_weights = torch.FloatTensor(weights).to(self.device)\n",
    "            self.criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=params['label_smoothing'])\n",
    "            \n",
    "            optimizer_name = params['optimizer_name'].lower()\n",
    "            opt_class = {'adam': optim.Adam, 'adamw': optim.AdamW, 'sgd': optim.SGD}.get(optimizer_name)\n",
    "            if not opt_class: raise ValueError(f\"不支持的优化器: {optimizer_name}\")\n",
    "            self.optimizer = opt_class(self.model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n",
    "            \n",
    "            scheduler_name = params['scheduler_name']\n",
    "            if scheduler_name == 'cosine': self.scheduler = CosineAnnealingLR(self.optimizer, T_max=params['local_epoch'])\n",
    "            elif scheduler_name == 'step': self.scheduler = StepLR(self.optimizer, step_size=30, gamma=0.1)\n",
    "            elif scheduler_name == 'plateau': self.scheduler = ReduceLROnPlateau(self.optimizer, 'min', patience=5)\n",
    "            else: self.scheduler = None\n",
    "            \n",
    "            self.logger.info(f\"训练组件设置完成: 优化器={optimizer_name}, 调度器={scheduler_name}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"训练组件设置失败: {e}\"); raise\n",
    "\n",
    "    def train_epoch(self, train_loader: DataLoader) -> Tuple[float, float]:\n",
    "        \"\"\"训练一个epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "        for data, targets in train_loader:\n",
    "            data, targets = data.to(self.device), targets.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(data)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            if self.gradient_clip_norm > 0: torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip_norm)\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def validate_epoch(self, val_loader: DataLoader) -> Tuple[float, float]:\n",
    "        \"\"\"验证一个epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data, targets in val_loader:\n",
    "                data, targets = data.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(data)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"执行完整的训练和评估流程\"\"\"\n",
    "        try:\n",
    "            params = self.get_parameters()\n",
    "            self.setup_environment(params)\n",
    "            self.logger.info(f\"--- 实验开始: {params['experiment_name']} ---\")\n",
    "            self.logger.info(f\"参数:\\n{json.dumps(params, indent=2, ensure_ascii=False)}\")\n",
    "\n",
    "            train_loader, dev_loader, test_loader, train_labels = self.load_data(params)\n",
    "            self.create_model(params)\n",
    "            self.setup_training(params, train_labels)\n",
    "\n",
    "            self.logger.info(\"--- 开始训练 ---\")\n",
    "            best_val_accuracy, patience_counter = 0.0, 0\n",
    "            early_stopping = params.get('early_stopping_patience', 10)\n",
    "\n",
    "            for epoch in range(params['local_epoch']):\n",
    "                self.logger.info(f\"\\nEpoch {epoch+1}/{params['local_epoch']}\")\n",
    "                train_loss, train_acc = self.train_epoch(train_loader)\n",
    "                val_loss, val_acc = self.validate_epoch(dev_loader)\n",
    "                self.logger.info(f\"训练 - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "                self.logger.info(f\"验证 - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "                self.history['train_loss'].append(train_loss); self.history['train_accuracy'].append(train_acc)\n",
    "                self.history['val_loss'].append(val_loss); self.history['val_accuracy'].append(val_acc)\n",
    "                \n",
    "                if self.scheduler:\n",
    "                    if isinstance(self.scheduler, ReduceLROnPlateau): self.scheduler.step(val_loss)\n",
    "                    else: self.scheduler.step()\n",
    "                \n",
    "                if val_acc > best_val_accuracy:\n",
    "                    best_val_accuracy, patience_counter = val_acc, 0\n",
    "                    if params['save_checkpoints']:\n",
    "                        torch.save(self.model.state_dict(), self.output_dir / 'best_model.pth')\n",
    "                        self.logger.info(f\"保存最佳模型 (验证准确率: {best_val_accuracy:.2f}%)\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= early_stopping:\n",
    "                    self.logger.info(f\"早停触发 (patience: {early_stopping})\"); break\n",
    "\n",
    "            self.logger.info(f\"训练完成！最佳验证准确率: {best_val_accuracy:.2f}%\")\n",
    "            if params.get('plot_learning_curves', True): self.plot_learning_curves()\n",
    "            \n",
    "            self.logger.info(\"--- 开始测试评估 ---\")\n",
    "            best_model_path = self.output_dir / 'best_model.pth'\n",
    "            if best_model_path.exists():\n",
    "                self.model.load_state_dict(torch.load(best_model_path))\n",
    "            self.evaluate_model(test_loader, params)\n",
    "            self.logger.info(\"--- 实验完成 ---\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"训练过程中发生错误: {e}\"\n",
    "            if self.logger: self.logger.error(error_msg, exc_info=True)\n",
    "            else: print(f\"✗ {error_msg}\"); import traceback; traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def evaluate_model(self, test_loader: DataLoader, params: Dict[str, Any]) -> None:\n",
    "        \"\"\"评估模型\"\"\"\n",
    "        self.model.eval()\n",
    "        test_loss, correct, total = 0.0, 0, 0\n",
    "        all_preds, all_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for data, targets in test_loader:\n",
    "                data, targets = data.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(data)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "                all_preds.extend(predicted.cpu().numpy()); all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        test_acc = 100. * correct / total\n",
    "        f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "        self.logger.info(f\"测试结果: Accuracy={test_acc:.4f}, F1-Score={f1:.4f}\")\n",
    "        if params['plot_confusion_matrix']:\n",
    "            self.plot_confusion_matrix(all_targets, all_preds, params['activity_labels'])\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred, labels):\n",
    "        \"\"\"绘制混淆矩阵\"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "        plt.title('Confusion Matrix'); plt.ylabel('True Label'); plt.xlabel('Predicted Label')\n",
    "        cm_path = self.output_dir / 'confusion_matrix.png'\n",
    "        plt.savefig(cm_path)\n",
    "        self.logger.info(f\"混淆矩阵已保存至 {cm_path}\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_learning_curves(self):\n",
    "        \"\"\"绘制学习曲线\"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history['train_loss'], label='Train Loss')\n",
    "        plt.plot(self.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Loss Curves'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.history['train_accuracy'], label='Train Accuracy')\n",
    "        plt.plot(self.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Accuracy Curves'); plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()\n",
    "        plt.tight_layout()\n",
    "        curves_path = self.output_dir / 'learning_curves.png'\n",
    "        plt.savefig(curves_path)\n",
    "        self.logger.info(f\"学习曲线已保存至 {curves_path}\")\n",
    "        plt.show()\n",
    "\n",
    "print(\"✓ ConfigurableTrainer 类已修复并完整加载\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5e4508",
   "metadata": {},
   "source": [
    "## 步骤 4: 创建配置文件\n",
    "这是本脚本的核心驱动力。下面的单元格将创建一个名为 `config.yaml` 的文件。你可以直接修改这个单元格中的内容来改变实验参数，而无需触碰上面的类定义代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f513fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "\n",
    "# 实验元数据\n",
    "name: \"UCI_HART_Baseline_Experiment\"\n",
    "description: \"使用HART模型在UCI-HAR数据集上进行的基线实验\"\n",
    "seed: 42\n",
    "device: \"auto\"  # 'auto', 'cpu', 'cuda'\n",
    "output_dir: \"./results/uci_hart_baseline\"\n",
    "save_checkpoints: true\n",
    "verbose: true\n",
    "\n",
    "# 数据集配置\n",
    "dataset:\n",
    "  name: \"UCI\"\n",
    "  activity_labels: ['Walk', 'Upstair', 'Downstair', 'Sit', 'Stand', 'Lay']\n",
    "  modalities:\n",
    "    - name: \"accel\"\n",
    "      channels: 3\n",
    "      sequence_length: 128\n",
    "    - name: \"gyro\"\n",
    "      channels: 3\n",
    "      sequence_length: 128\n",
    "\n",
    "# 模型架构配置\n",
    "architecture:\n",
    "  dropout_rate: 0.3\n",
    "  # 定义专家模型，可以有多个，但目前只使用第一个\n",
    "  experts:\n",
    "    expert1:\n",
    "      type: \"HART\"\n",
    "      params:\n",
    "        projection_dim: 192\n",
    "        frame_length: 16\n",
    "        time_step: 16\n",
    "        filter_attention_head: 4\n",
    "        conv_kernels: [3, 7, 15, 31, 31, 31]\n",
    "        token_based: false\n",
    "\n",
    "# 训练过程配置\n",
    "training:\n",
    "  epochs: 20  # 为了快速演示，减少epoch数量\n",
    "  batch_size: 128\n",
    "  learning_rate: 0.001\n",
    "  optimizer: \"adamw\"  # 'adam', 'adamw', 'sgd'\n",
    "  weight_decay: 0.0001\n",
    "  scheduler: \"cosine\" # 'cosine', 'step', null\n",
    "  label_smoothing: 0.1\n",
    "  gradient_clip_norm: 1.0\n",
    "  early_stopping_patience: 10\n",
    "\n",
    "# 可视化配置\n",
    "visualization:\n",
    "  plot_learning_curves: true\n",
    "  plot_confusion_matrix: true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af482da3",
   "metadata": {},
   "source": [
    "## 步骤 5: 启动训练\n",
    "现在，一切准备就绪。运行下面的单元格来实例化 `ConfigurableTrainer` 并启动由 `config.yaml` 文件定义的完整训练流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d3fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        config_file = 'config.yaml'\n",
    "        \n",
    "        if not os.path.exists(config_file):\n",
    "            print(f\"⚠ 配置文件 {config_file} 不存在，将使用传统硬编码模式\")\n",
    "            trainer = ConfigurableTrainer(config_path=None)\n",
    "        else:\n",
    "            print(f\"✓ 使用配置文件: {config_file}\")\n",
    "            trainer = ConfigurableTrainer(config_path=config_file)\n",
    "        \n",
    "        trainer.run()\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n用户中断训练\")\n",
    "    except Exception as e:\n",
    "        # 错误已在run方法中记录，这里仅为捕获最终异常\n",
    "        print(f\"\\n训练流程因未捕获的异常而终止。请检查日志。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HAR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
