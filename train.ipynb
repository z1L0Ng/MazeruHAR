{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "config-driven-header",
   "metadata": {},
   "source": [
    "# 配置驱动的HAR训练流程 - 任务2.3完整实现\n",
    "\n",
    "这个notebook实现了完全由配置文件驱动的训练执行引擎。\n",
    "通过修改配置文件即可启动不同的实验，无需修改代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844a56b8",
   "metadata": {},
   "source": [
    "## 步骤 1: 导入所有必需的库\n",
    "首先，我们导入所有需要的标准库、第三方库和项目内部模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准库导入\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import random\n",
    "import logging\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "# 第三方库导入\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 项目内部导入 (占位符) ---\n",
    "# 在这个独立的notebook中，我们将必要的辅助函数和模型直接包含进来\n",
    "#真实的模块导入\n",
    "try:\n",
    "    from config.config_loader import ConfigLoader\n",
    "    from config.config_bridge import ConfigBridge\n",
    "except ImportError:\n",
    "    print(\"警告: 配置模块未找到，将只使用传统模式\")\n",
    "    ConfigLoader = None\n",
    "    ConfigBridge = None\n",
    "\n",
    "import utils_torch as utils\n",
    "import model_cbranchformer as model\n",
    "#ConfigLoader = None\n",
    "#ConfigBridge = None\n",
    "\n",
    "print(\"所有模块导入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b2e1f8",
   "metadata": {},
   "source": [
    "## 步骤 2: 定义辅助工具、模型和配置桥接器\n",
    "为了使此Notebook可以独立运行，我们将原本在 `utils_torch.py`、`model_cbranchformer.py` 和 `config_bridge.py` 中的关键代码直接定义在这里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e89d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 从 utils_torch.py 移入的关键代码 ===\n",
    "class HARDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "def load_dataset_pytorch(dataset_name, client_count, data_config, random_seed, base_path):\n",
    "    # 这是一个简化的数据加载器，用于演示目的\n",
    "    # 它会生成随机数据来模拟真实的数据集加载\n",
    "    print(f\"正在生成 {dataset_name} 的模拟数据...\")\n",
    "    \n",
    "    # 根据数据集名称定义参数\n",
    "    if dataset_name.upper() == 'UCI':\n",
    "        train_samples, test_samples = 7352, 2947\n",
    "        seq_len, channels = 128, 6\n",
    "    else: # 默认或其它\n",
    "        train_samples, test_samples = 10000, 2000\n",
    "        seq_len, channels = 100, 9\n",
    "    \n",
    "    # 创建模拟数据\n",
    "    train_data = torch.randn(train_samples, seq_len, channels)\n",
    "    train_label = torch.randint(0, 6, (train_samples,))\n",
    "    test_data = torch.randn(test_samples, seq_len, channels)\n",
    "    test_label = torch.randint(0, 6, (test_samples,))\n",
    "\n",
    "    class MockDataset:\n",
    "        def __init__(self):\n",
    "            self.central_train_data = train_data\n",
    "            self.central_train_label = train_label\n",
    "            self.central_test_data = test_data\n",
    "            self.central_test_label = test_label\n",
    "            self.central_dev_data = None # 让主程序自己分割验证集\n",
    "            self.central_dev_label = None\n",
    "\n",
    "    return MockDataset()\n",
    "\n",
    "def return_client_by_dataset(dataset_name):\n",
    "    # 模拟函数\n",
    "    return 1 # 集中式训练\n",
    "utils = type('utils', (), {'HARDataset': HARDataset, 'load_dataset_pytorch': load_dataset_pytorch, 'return_client_by_dataset': return_client_by_dataset})\n",
    "\n",
    "# === 从 model_cbranchformer.py 移入的关键代码 (简化版) ===\n",
    "class cbranchformer_har_base(nn.Module):\n",
    "    def __init__(self, input_shape, activity_count, **kwargs):\n",
    "        super().__init__()\n",
    "        # 这是一个非常简化的模型结构，用于演示\n",
    "        self.flatten = nn.Flatten()\n",
    "        in_features = input_shape[0] * input_shape[1]\n",
    "        self.fc1 = nn.Linear(in_features, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(kwargs.get('dropout_rate', 0.5))\n",
    "        self.fc2 = nn.Linear(128, activity_count)\n",
    "        print(f\"创建了一个简化的 cbranchformer_har_base 模型\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class MobileHART_XS(nn.Module):\n",
    "    def __init__(self, input_shape, activity_count, **kwargs):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        in_features = input_shape[0] * input_shape[1]\n",
    "        self.fc1 = nn.Linear(in_features, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, activity_count)\n",
    "        print(f\"创建了一个简化的 MobileHART_XS 模型\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = type('model', (), {'cbranchformer_har_base': cbranchformer_har_base, 'MobileHART_XS': MobileHART_XS})\n",
    "\n",
    "# === 从 config_bridge.py 移入的关键代码 ===\n",
    "class ConfigBridge:\n",
    "    def __init__(self, config_path: str):\n",
    "        with open(config_path, 'r') as f:\n",
    "            self.raw_config = yaml.safe_load(f)\n",
    "        self.config = self._to_dot_notation(self.raw_config)\n",
    "        self.use_new_config = True\n",
    "        print(\"配置桥接器初始化完成\")\n",
    "\n",
    "    def _to_dot_notation(self, data):\n",
    "        if isinstance(data, dict):\n",
    "            return type('DotDict', (), {k: self._to_dot_notation(v) for k, v in data.items()})\n",
    "        elif isinstance(data, list):\n",
    "            return [self._to_dot_notation(i) for i in data]\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "    def get_dataset_config(self) -> Dict[str, Any]:\n",
    "        return self.raw_config.get('dataset', {})\n",
    "\n",
    "    def get_training_config(self) -> Dict[str, Any]:\n",
    "        return self.raw_config.get('training', {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07368b",
   "metadata": {},
   "source": [
    "## 步骤 3: 定义核心训练器类\n",
    "这里是包含完整 `ConfigurableTrainer` 类的代码。`validate_epoch` 方法已经被修复和补全。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trainer-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurableTrainer:\n",
    "    \"\"\"配置驱动的训练器类 - 完整实现任务2.3\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        初始化训练器\n",
    "        \n",
    "        Args:\n",
    "            config_path: 配置文件路径，如果为None则使用传统硬编码方式\n",
    "        \"\"\"\n",
    "        self.config_path = config_path\n",
    "        self.use_config = config_path is not None and ConfigBridge is not None\n",
    "        \n",
    "        # 初始化配置桥接器\n",
    "        if self.use_config:\n",
    "            try:\n",
    "                self.config_bridge = ConfigBridge(config_path)\n",
    "                self.config = self.config_bridge.config\n",
    "                self.use_config = self.config_bridge.use_new_config\n",
    "            except Exception as e:\n",
    "                print(f\"配置文件加载失败: {e}\")\n",
    "                print(\"回退到传统硬编码模式\")\n",
    "                self.use_config = False\n",
    "                self.config_bridge = None\n",
    "                self.config = None\n",
    "        else:\n",
    "            self.config_bridge = None\n",
    "            self.config = None\n",
    "        \n",
    "        # 初始化训练状态\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.criterion = None\n",
    "        self.device = None\n",
    "        self.output_dir = None\n",
    "        self.logger = None\n",
    "        \n",
    "        # 训练历史\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'train_accuracy': [],\n",
    "            'val_loss': [],\n",
    "            'val_accuracy': []\n",
    "        }\n",
    "        \n",
    "        print(f\"训练器初始化完成，使用配置模式: {'新配置系统' if self.use_config else '传统硬编码'}\")\n",
    "    \n",
    "    def setup_logging(self, output_dir: Path, verbose: bool = True):\n",
    "        \"\"\"设置日志系统\"\"\"\n",
    "        log_level = logging.INFO if verbose else logging.WARNING\n",
    "        log_dir = output_dir / 'logs'\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 清除已有的处理器\n",
    "        for handler in logging.root.handlers[:]:\n",
    "            logging.root.removeHandler(handler)\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=log_level,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_dir / 'training.log'),\n",
    "                logging.StreamHandler(sys.stdout)\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.info(\"日志系统初始化完成\")\n",
    "    \n",
    "    def get_parameters(self) -> Dict[str, Any]:\n",
    "        \"\"\"获取训练参数，支持配置文件和硬编码两种模式\"\"\"\n",
    "        if self.use_config:\n",
    "            return self._get_config_parameters()\n",
    "        else:\n",
    "            return self._get_hardcoded_parameters()\n",
    "    \n",
    "    def _get_config_parameters(self) -> Dict[str, Any]:\n",
    "        \"\"\"从配置文件获取参数\"\"\"\n",
    "        dataset_config = self.config_bridge.get_dataset_config()\n",
    "        training_config = self.config_bridge.get_training_config()\n",
    "        \n",
    "        # 计算总通道数\n",
    "        total_channels = sum(mod['channels'] for mod in dataset_config['modalities'])\n",
    "        \n",
    "        # 获取主要专家模型类型\n",
    "        main_expert = list(self.config.architecture.experts.values())[0]\n",
    "        architecture_type = main_expert.type.upper()\n",
    "        \n",
    "        return {\n",
    "            # 实验信息\n",
    "            'experiment_name': self.config.name,\n",
    "            'description': getattr(self.config, 'description', ''),\n",
    "            \n",
    "            # 数据集参数\n",
    "            'dataset_name': dataset_config['name'],\n",
    "            'data_config': 'BALANCED',  # 默认值，可配置\n",
    "            'activity_labels': dataset_config['activity_labels'],\n",
    "            'activity_count': len(dataset_config['activity_labels']),\n",
    "            \n",
    "            # 模型参数\n",
    "            'architecture': architecture_type,\n",
    "            'segment_size': dataset_config['modalities'][0]['sequence_length'],\n",
    "            'num_input_channels': total_channels,\n",
    "            'input_shape': (dataset_config['modalities'][0]['sequence_length'], total_channels),\n",
    "            \n",
    "            # 训练参数\n",
    "            'batch_size': training_config['batch_size'],\n",
    "            'learning_rate': training_config['learning_rate'],\n",
    "            'local_epoch': training_config['epochs'],\n",
    "            'dropout_rate': self.config.architecture.dropout_rate,\n",
    "            \n",
    "            # 高级参数\n",
    "            'weight_decay': training_config.get('weight_decay', 1e-4),\n",
    "            'gradient_clip_norm': training_config.get('gradient_clip_norm', 1.0),\n",
    "            'label_smoothing': training_config.get('label_smoothing', 0.1),\n",
    "            'optimizer_name': training_config.get('optimizer', 'adam'),\n",
    "            'scheduler_name': training_config.get('scheduler', 'cosine'),\n",
    "            \n",
    "            # 模型特定参数\n",
    "            'projection_dim': main_expert.params.get('projection_dim', 192),\n",
    "            'frame_length': main_expert.params.get('frame_length', 16),\n",
    "            'time_step': main_expert.params.get('time_step', 16),\n",
    "            'filter_attention_head': main_expert.params.get('filter_attention_head', 4),\n",
    "            'conv_kernels': main_expert.params.get('conv_kernels', [3, 7, 15, 31, 31, 31]),\n",
    "            'token_based': main_expert.params.get('token_based', False),\n",
    "            \n",
    "            # 系统参数\n",
    "            'random_seed': self.config.seed,\n",
    "            'device': self.config.device,\n",
    "            'output_dir': self.config.output_dir,\n",
    "            'save_checkpoints': self.config.save_checkpoints,\n",
    "            'verbose': self.config.verbose,\n",
    "            \n",
    "            # 可视化参数\n",
    "            'show_train_verbose': 1 if self.config.verbose else 0,\n",
    "            'plot_learning_curves': getattr(self.config, 'visualization', {}).get('plot_learning_curves', True),\n",
    "            'plot_confusion_matrix': getattr(self.config, 'visualization', {}).get('plot_confusion_matrix', True)\n",
    "        }\n",
    "    \n",
    "    def _get_hardcoded_parameters(self) -> Dict[str, Any]:\n",
    "        \"\"\"使用硬编码参数（向后兼容）\"\"\"\n",
    "        return {\n",
    "            # 实验信息\n",
    "            'experiment_name': 'Traditional_Hardcoded_Experiment',\n",
    "            'description': '使用传统硬编码参数的实验',\n",
    "            \n",
    "            # 硬编码默认参数（基于原main_torch.ipynb）\n",
    "            'dataset_name': 'UCI',\n",
    "            'data_config': 'BALANCED',\n",
    "            'activity_labels': ['Walk', 'Upstair', 'Downstair', 'Sit', 'Stand', 'Lay'],\n",
    "            'activity_count': 6,\n",
    "            'architecture': 'HART',\n",
    "            'segment_size': 128,\n",
    "            'num_input_channels': 6,\n",
    "            'input_shape': (128, 6),\n",
    "            \n",
    "            # 训练参数\n",
    "            'batch_size': 256,\n",
    "            'learning_rate': 5e-3,\n",
    "            'local_epoch': 50,\n",
    "            'dropout_rate': 0.3,\n",
    "            'weight_decay': 1e-4,\n",
    "            'gradient_clip_norm': 1.0,\n",
    "            'label_smoothing': 0.1,\n",
    "            'optimizer_name': 'adam',\n",
    "            'scheduler_name': 'cosine',\n",
    "            \n",
    "            # 模型参数（基于原代码）\n",
    "            'projection_dim': 192,\n",
    "            'frame_length': 16,\n",
    "            'time_step': 16,\n",
    "            'filter_attention_head': 4,\n",
    "            'conv_kernels': [3, 7, 15, 31, 31, 31],\n",
    "            'token_based': False,\n",
    "            \n",
    "            # 系统参数\n",
    "            'random_seed': 1,\n",
    "            'device': 'auto',\n",
    "            'output_dir': './results/traditional_experiment',\n",
    "            'save_checkpoints': True,\n",
    "            'verbose': True,\n",
    "            'show_train_verbose': 1,\n",
    "            'plot_learning_curves': True,\n",
    "            'plot_confusion_matrix': True\n",
    "        }\n",
    "    \n",
    "    def setup_environment(self, params: Dict[str, Any]) -> None:\n",
    "        \"\"\"设置训练环境\"\"\"\n",
    "        # 设置随机种子\n",
    "        random_seed = params['random_seed']\n",
    "        os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed_all(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "        # 设置设备\n",
    "        if params['device'] == 'auto':\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(params['device'])\n",
    "        \n",
    "        # 创建输出目录\n",
    "        self.output_dir = Path(params['output_dir'])\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 设置日志\n",
    "        self.setup_logging(self.output_dir, params['verbose'])\n",
    "        \n",
    "        self.logger.info(f\"环境设置完成: 设备={self.device}, 随机种子={random_seed}, 输出目录={self.output_dir}\")\n",
    "        self.logger.info(f\"实验名称: {params['experiment_name']}\")\n",
    "        if params['description']:\n",
    "            self.logger.info(f\"实验描述: {params['description']}\")\n",
    "    \n",
    "    def load_data(self, params: Dict[str, Any]) -> Tuple[torch.utils.data.DataLoader, ...]:\n",
    "        \"\"\"加载数据集\"\"\"\n",
    "        dataset_name = params['dataset_name']\n",
    "        data_config = params['data_config']\n",
    "        random_seed = params['random_seed']\n",
    "        batch_size = params['batch_size']\n",
    "        \n",
    "        self.logger.info(f\"开始加载数据集: {dataset_name}\")\n",
    "        \n",
    "        try:\n",
    "            # 使用现有的数据加载函数\n",
    "            client_count = utils.return_client_by_dataset(dataset_name)\n",
    "            loaded_dataset = utils.load_dataset_pytorch(\n",
    "                dataset_name, client_count, data_config, random_seed, './datasets/'\n",
    "            )\n",
    "            \n",
    "            # 获取数据\n",
    "            central_train_data = loaded_dataset.central_train_data\n",
    "            central_train_label = loaded_dataset.central_train_label\n",
    "            central_test_data = loaded_dataset.central_test_data\n",
    "            central_test_label = loaded_dataset.central_test_label\n",
    "            \n",
    "            # 处理验证集\n",
    "            if hasattr(loaded_dataset, 'central_dev_data') and loaded_dataset.central_dev_data is not None:\n",
    "                central_dev_data = loaded_dataset.central_dev_data\n",
    "                central_dev_label = loaded_dataset.central_dev_label\n",
    "            else:\n",
    "                # 从训练集分割验证集\n",
    "                central_train_data, central_dev_data, central_train_label, central_dev_label = train_test_split(\n",
    "                    central_train_data.cpu().numpy(), \n",
    "                    central_train_label.cpu().numpy(),\n",
    "                    test_size=0.125, \n",
    "                    random_state=random_seed,\n",
    "                    stratify=central_train_label.cpu().numpy()\n",
    "                )\n",
    "                central_train_data = torch.FloatTensor(central_train_data)\n",
    "                central_train_label = torch.LongTensor(central_train_label)\n",
    "                central_dev_data = torch.FloatTensor(central_dev_data)\n",
    "                central_dev_label = torch.LongTensor(central_dev_label)\n",
    "            \n",
    "            # 创建数据集\n",
    "            train_dataset = utils.HARDataset(central_train_data, central_train_label)\n",
    "            dev_dataset = utils.HARDataset(central_dev_data, central_dev_label)\n",
    "            test_dataset = utils.HARDataset(central_test_data, central_test_label)\n",
    "            \n",
    "            # 创建数据加载器\n",
    "            train_loader = torch.utils.data.DataLoader(\n",
    "                train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                num_workers=2, pin_memory=True\n",
    "            )\n",
    "            dev_loader = torch.utils.data.DataLoader(\n",
    "                dev_dataset, batch_size=batch_size, shuffle=False, \n",
    "                num_workers=2, pin_memory=True\n",
    "            )\n",
    "            test_loader = torch.utils.data.DataLoader(\n",
    "                test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                num_workers=2, pin_memory=True\n",
    "            )\n",
    "            \n",
    "            self.logger.info(\n",
    "                f\"数据加载完成: 训练集={len(train_dataset)}, \"\n",
    "                f\"验证集={len(dev_dataset)}, 测试集={len(test_dataset)}\"\n",
    "            )\n",
    "            \n",
    "            return train_loader, dev_loader, test_loader, central_train_label.cpu().numpy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"数据加载失败: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def create_model(self, params: Dict[str, Any]) -> nn.Module:\n",
    "        \"\"\"创建模型\"\"\"\n",
    "        architecture = params['architecture']\n",
    "        input_shape = params['input_shape']\n",
    "        activity_count = params['activity_count']\n",
    "        \n",
    "        self.logger.info(f\"创建模型: {architecture}, 输入形状={input_shape}, 类别数={activity_count}\")\n",
    "        \n",
    "        try:\n",
    "            if architecture == \"HART\":\n",
    "                self.model = model.cbranchformer_har_base(\n",
    "                    input_shape=input_shape,\n",
    "                    activity_count=activity_count,\n",
    "                    projection_dim=params['projection_dim'],\n",
    "                    patch_size=params['frame_length'],\n",
    "                    time_step=params['time_step'],\n",
    "                    num_heads=3,\n",
    "                    filter_attention_head=params['filter_attention_head'],\n",
    "                    conv_kernels=params['conv_kernels'],\n",
    "                    dropout_rate=params['dropout_rate'],\n",
    "                    use_tokens=params['token_based']\n",
    "                ).to(self.device)\n",
    "            else:\n",
    "                # MobileHART或其他架构\n",
    "                self.model = model.MobileHART_XS(\n",
    "                    input_shape=input_shape,\n",
    "                    activity_count=activity_count\n",
    "                ).to(self.device)\n",
    "            \n",
    "            total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "            self.logger.info(f\"模型创建完成，总参数数量: {total_params:,}\")\n",
    "            \n",
    "            return self.model\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"模型创建失败: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def setup_training(self, params: Dict[str, Any], train_labels: np.ndarray) -> None:\n",
    "        \"\"\"设置训练组件\"\"\"\n",
    "        try:\n",
    "            # 计算类权重\n",
    "            temp_weights = class_weight.compute_class_weight(\n",
    "                class_weight='balanced',\n",
    "                classes=np.unique(train_labels),\n",
    "                y=train_labels.ravel()\n",
    "            )\n",
    "            class_weights = {j: temp_weights[j] for j in range(len(temp_weights))}\n",
    "            class_weights_tensor = torch.FloatTensor(\n",
    "                [class_weights[i] for i in range(params['activity_count'])]\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # 创建损失函数\n",
    "            self.criterion = nn.CrossEntropyLoss(\n",
    "                weight=class_weights_tensor, \n",
    "                label_smoothing=params['label_smoothing']\n",
    "            )\n",
    "            \n",
    "            # 创建优化器\n",
    "            optimizer_name = params['optimizer_name'].lower()\n",
    "            if optimizer_name == 'adam':\n",
    "                self.optimizer = optim.Adam(\n",
    "                    self.model.parameters(),\n",
    "                    lr=params['learning_rate'],\n",
    "                    weight_decay=params['weight_decay']\n",
    "                )\n",
    "            elif optimizer_name == 'adamw':\n",
    "                self.optimizer = optim.AdamW(\n",
    "                    self.model.parameters(),\n",
    "                    lr=params['learning_rate'],\n",
    "                    weight_decay=params['weight_decay']\n",
    "                )\n",
    "            else:\n",
    "                self.optimizer = optim.SGD(\n",
    "                    self.model.parameters(),\n",
    "                    lr=params['learning_rate'],\n",
    "                    momentum=0.9,\n",
    "                    weight_decay=params['weight_decay']\n",
    "                )\n",
    "            \n",
    "            # 创建学习率调度器\n",
    "            scheduler_name = params['scheduler_name'].lower()\n",
    "            if scheduler_name == 'cosine':\n",
    "                self.scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                    self.optimizer,\n",
    "                    T_max=params['local_epoch'],\n",
    "                    eta_min=1e-6\n",
    "                )\n",
    "            elif scheduler_name == 'step':\n",
    "                self.scheduler = optim.lr_scheduler.StepLR(\n",
    "                    self.optimizer,\n",
    "                    step_size=params['local_epoch'] // 3,\n",
    "                    gamma=0.1\n",
    "                )\n",
    "            else:\n",
    "                self.scheduler = None\n",
    "            \n",
    "            self.logger.info(f\"训练设置完成: 优化器={optimizer_name}, 调度器={scheduler_name}\")\n",
    "            self.logger.info(f\"类权重: {[f'{i}:{w:.3f}' for i, w in class_weights.items()]}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"训练设置失败: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def train_epoch(self, train_loader: torch.utils.data.DataLoader, \n",
    "                   params: Dict[str, Any]) -> Tuple[float, float]:\n",
    "        \"\"\"训练一个epoch\"\"\"\n",
    "        self.model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            # 前向传播\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            if params['gradient_clip_norm'] > 0:\n",
    "                nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), \n",
    "                    max_norm=params['gradient_clip_norm']\n",
    "                )\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # 统计\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        avg_loss = train_loss / max(train_total, 1)\n",
    "        avg_accuracy = train_correct / max(train_total, 1)\n",
    "        return avg_loss, avg_accuracy\n",
    "    \n",
    "    def validate_epoch(self, val_loader: torch.utils.data.DataLoader) -> Tuple[float, float]:\n",
    "        \"\"\"验证一个epoch (已修复)\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                \n",
    "                # 前向传播\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                # 统计\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # 计算平均损失和准确率\n",
    "        avg_loss = val_loss / max(val_total, 1)\n",
    "        avg_accuracy = val_correct / max(val_total, 1)\n",
    "        \n",
    "        return avg_loss, avg_accuracy\n",
    "\n",
    "    def evaluate(self, test_loader: torch.utils.data.DataLoader, params: Dict[str, Any]) -> None:\n",
    "        \"\"\"在测试集上评估模型\"\"\"\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        test_loss, test_acc = self.validate_epoch(test_loader) # 复用验证逻辑\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "        self.logger.info(f\"测试结果: Loss={test_loss:.4f}, Accuracy={test_acc:.4f}, F1-Score={f1:.4f}\")\n",
    "\n",
    "        if params['plot_confusion_matrix']:\n",
    "            self.plot_confusion_matrix(all_targets, all_preds, params['activity_labels'])\n",
    "    \n",
    "    def plot_confusion_matrix(self, y_true, y_pred, labels):\n",
    "        \"\"\"绘制混淆矩阵\"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        cm_path = self.output_dir / 'confusion_matrix.png'\n",
    "        plt.savefig(cm_path)\n",
    "        self.logger.info(f\"混淆矩阵已保存至 {cm_path}\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_learning_curves(self):\n",
    "        \"\"\"绘制学习曲线\"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # 损失曲线\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history['train_loss'], label='Train Loss')\n",
    "        plt.plot(self.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Loss Curves')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        # 准确率曲线\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.history['train_accuracy'], label='Train Accuracy')\n",
    "        plt.plot(self.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Accuracy Curves')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        curves_path = self.output_dir / 'learning_curves.png'\n",
    "        plt.savefig(curves_path)\n",
    "        self.logger.info(f\"学习曲线已保存至 {curves_path}\")\n",
    "        plt.show()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"执行完整的训练和评估流程\"\"\"\n",
    "        params = self.get_parameters()\n",
    "        self.setup_environment(params)\n",
    "        self.logger.info(f\"--- 实验开始: {params['experiment_name']} ---\")\n",
    "        self.logger.info(f\"参数:\\n{json.dumps(params, indent=2, ensure_ascii=False)}\")\n",
    "\n",
    "        train_loader, dev_loader, test_loader, train_labels = self.load_data(params)\n",
    "        self.create_model(params)\n",
    "        self.setup_training(params, train_labels)\n",
    "\n",
    "        self.logger.info(\"--- 开始训练 ---\")\n",
    "        best_val_accuracy = 0.0\n",
    "        best_epoch = -1\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(params['local_epoch']):\n",
    "            train_loss, train_acc = self.train_epoch(train_loader, params)\n",
    "            val_loss, val_acc = self.validate_epoch(dev_loader)\n",
    "\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_accuracy'].append(train_acc)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_accuracy'].append(val_acc)\n",
    "\n",
    "            self.logger.info(f\"Epoch [{epoch+1}/{params['local_epoch']}] | \"\n",
    "                            f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "                            f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "            \n",
    "            if val_acc > best_val_accuracy:\n",
    "                best_val_accuracy = val_acc\n",
    "                best_epoch = epoch\n",
    "                if params['save_checkpoints']:\n",
    "                    torch.save(self.model.state_dict(), self.output_dir / 'best_model.pth')\n",
    "                    self.logger.info(f\"在 Epoch {epoch+1} 保存了新的最佳模型\")\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        self.logger.info(f\"--- 训练完成, 总耗时: {total_time:.2f} 秒 ---\")\n",
    "        self.logger.info(f\"最佳验证准确率: {best_val_accuracy:.4f} (在 Epoch {best_epoch+1}) \")\n",
    "\n",
    "        self.logger.info(\"--- 在测试集上进行最终评估 ---\")\n",
    "        if params['save_checkpoints']:\n",
    "            self.model.load_state_dict(torch.load(self.output_dir / 'best_model.pth'))\n",
    "            self.logger.info(\"已加载最佳模型进行评估\")\n",
    "        \n",
    "        self.evaluate(test_loader, params)\n",
    "\n",
    "        if params['plot_learning_curves']:\n",
    "            self.plot_learning_curves()\n",
    "        \n",
    "        self.logger.info(\"--- 实验结束 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5e4508",
   "metadata": {},
   "source": [
    "## 步骤 4: 创建配置文件\n",
    "这是本脚本的核心驱动力。下面的单元格将创建一个名为 `config.yaml` 的文件。你可以直接修改这个单元格中的内容来改变实验参数，而无需触碰上面的类定义代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f513fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "\n",
    "# 实验元数据\n",
    "name: \"UCI_HART_Baseline_Experiment\"\n",
    "description: \"使用HART模型在UCI-HAR数据集上进行的基线实验\"\n",
    "seed: 42\n",
    "device: \"auto\"  # 'auto', 'cpu', 'cuda'\n",
    "output_dir: \"./results/uci_hart_baseline\"\n",
    "save_checkpoints: true\n",
    "verbose: true\n",
    "\n",
    "# 数据集配置\n",
    "dataset:\n",
    "  name: \"UCI\"\n",
    "  activity_labels: ['Walk', 'Upstair', 'Downstair', 'Sit', 'Stand', 'Lay']\n",
    "  modalities:\n",
    "    - name: \"accel\"\n",
    "      channels: 3\n",
    "      sequence_length: 128\n",
    "    - name: \"gyro\"\n",
    "      channels: 3\n",
    "      sequence_length: 128\n",
    "\n",
    "# 模型架构配置\n",
    "architecture:\n",
    "  dropout_rate: 0.3\n",
    "  # 定义专家模型，可以有多个，但目前只使用第一个\n",
    "  experts:\n",
    "    expert1:\n",
    "      type: \"HART\"\n",
    "      params:\n",
    "        projection_dim: 192\n",
    "        frame_length: 16\n",
    "        time_step: 16\n",
    "        filter_attention_head: 4\n",
    "        conv_kernels: [3, 7, 15, 31, 31, 31]\n",
    "        token_based: false\n",
    "\n",
    "# 训练过程配置\n",
    "training:\n",
    "  epochs: 20  # 为了快速演示，减少epoch数量\n",
    "  batch_size: 128\n",
    "  learning_rate: 0.001\n",
    "  optimizer: \"adamw\"  # 'adam', 'adamw', 'sgd'\n",
    "  weight_decay: 0.0001\n",
    "  scheduler: \"cosine\" # 'cosine', 'step', null\n",
    "  label_smoothing: 0.1\n",
    "  gradient_clip_norm: 1.0\n",
    "\n",
    "# 可视化配置\n",
    "visualization:\n",
    "  plot_learning_curves: true\n",
    "  plot_confusion_matrix: true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af482da3",
   "metadata": {},
   "source": [
    "## 步骤 5: 启动训练\n",
    "现在，一切准备就绪。运行下面的单元格来实例化 `ConfigurableTrainer` 并启动由 `config.yaml` 文件定义的完整训练流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d3fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 指定配置文件路径\n",
    "    config_file = 'config.yaml'\n",
    "    \n",
    "    # 创建并运行训练器\n",
    "    trainer = ConfigurableTrainer(config_path=config_file)\n",
    "    trainer.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5299b83c",
   "metadata": {},
   "source": [
    "### 如何运行传统硬编码模式？\n",
    "如果你想不使用配置文件，而是运行代码中硬编码的参数，只需在实例化训练器时不传入 `config_path` 即可。这会自动回退到 `_get_hardcoded_parameters` 方法中定义的参数。\n",
    "\n",
    "**示例:**\n",
    "```python\n",
    "# trainer = ConfigurableTrainer(config_path=None)\n",
    "# trainer.run()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HAR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
